{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e807feda-b240-48ae-badc-bd57fc01f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "from training.trainer import create_train_state, gradient_descent, reset_model, forward\n",
    "from training.utils import cross_entropy, mse, MLP, CNN, load_mnist\n",
    "\n",
    "from meta_opt import MetaOpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ed64-63a1-46de-8106-28529a15467e",
   "metadata": {},
   "source": [
    "### Todo\n",
    "- add wall clock time to `stats`\n",
    "- add MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2f18ab8-5fcc-42ee-b5e2-6e7812e927d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax.numpy as jnp\n",
    "# m = jnp.zeros((4,))\n",
    "# tstate, train_ds, test_ds, rng, args = get_problem(0, optax.sgd(0.1))\n",
    "# p = tstate.params\n",
    "\n",
    "# def d_fn(_p):\n",
    "#     n = _p.ndim\n",
    "#     s = [1,] * (n + 1)\n",
    "#     s[0] = 4\n",
    "#     return jnp.tile(_p, s)\n",
    "\n",
    "# d = jax.tree_map(d_fn, p)\n",
    "\n",
    "# control = jax.tree_map(lambda s: (m[:, *[None for _ in range(s.ndim - 1)]] * s).sum(axis=0), d)\n",
    "# print(control['dense 0']['kernel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1                      \n",
    "NUM_ITERS = 8000\n",
    "EVAL_EVERY = 100\n",
    "BATCH_SIZE = 2048\n",
    "RESET_EVERY = 4000\n",
    "PRINT_EVERY = 1000\n",
    "\n",
    "def set_seed(seed):\n",
    "    if seed is None: \n",
    "        seed = np.random.randint()\n",
    "        print('seed set to {}'.format(seed))\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    return rng, seed\n",
    "\n",
    "# MNIST\n",
    "def get_problem(seed, optimizer):\n",
    "    rng, seed = set_seed(seed)\n",
    "\n",
    "    # get dataset\n",
    "    train_ds, test_ds, loss_fn, input_dims = load_mnist(NUM_ITERS, BATCH_SIZE)\n",
    "    \n",
    "    # define model, optimizer, and train state\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "    model = MLP([28 * 28, 100, 100, 10])\n",
    "    tstate = create_train_state(init_rng, model, input_dims, optimizer, loss_fn)\n",
    "    del init_rng\n",
    "\n",
    "    args = {'seed': seed,\n",
    "            'model': str(model),\n",
    "            'dataset': 'MNIST',\n",
    "            'num_iters': NUM_ITERS,\n",
    "            'eval_every': EVAL_EVERY,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'reset_every': RESET_EVERY,\n",
    "            'print_every': PRINT_EVERY}\n",
    "\n",
    "    return tstate, train_ds, test_ds, rng, args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68611b25-ff6c-498a-bac1-30873eac9b16",
   "metadata": {},
   "source": [
    "# Standard Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35553ddc-66dd-454e-9608-8820ba402b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_standard_opt(seed, optimizer):\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, optimizer)\n",
    "    \n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = deepcopy(tstate.opt_state.hyperparams)\n",
    "    args['optimizer_args']['name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "    \n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=len(train_ds))):\n",
    "        t += 1\n",
    "    \n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
    "        \n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0: s['eval_loss'] = sum(forward(tstate, batch) for batch in test_ds.as_numpy_iterator()) / len(test_ds)\n",
    "        stats[t] = s\n",
    "    \n",
    "        # print if we gotta\n",
    "        if t % PRINT_EVERY == 0 and t > 0:\n",
    "            idxs = [stats[i] for i in range(t - PRINT_EVERY, t) if i in stats]\n",
    "            avg_train_loss = np.mean([s['loss'] for s in idxs if 'loss' in s])\n",
    "            avg_eval_loss = np.mean([s['eval_loss'] for s in idxs if 'eval_loss' in s])\n",
    "            print(f'iters {t - PRINT_EVERY} - {t}')\n",
    "            print(f'\\tavg train loss: {avg_train_loss}')\n",
    "            print(f'\\tavg eval loss: {avg_eval_loss}')\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f",
   "metadata": {},
   "source": [
    "# Meta-Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907792cd-7420-4e61-a458-372e2dac9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_opt(seed, m_method: str, meta_lr: float, H: int, HH: int, initial_lr: int):\n",
    "    optimizer = optax.sgd(learning_rate=initial_lr)\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, optimizer)\n",
    "    \n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = {'name': 'meta',\n",
    "                              'initial_lr': initial_lr,\n",
    "                              'm_method': m_method,\n",
    "                              'meta_lr': meta_lr,\n",
    "                              'H': H,\n",
    "                              'HH': HH\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    meta_opt = MetaOpt(tstate, H=H, HH=HH, meta_lr=meta_lr, delta=1e-5, m_method=m_method)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=len(train_ds))):\n",
    "        t += 1\n",
    "    \n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            meta_opt = meta_opt.episode_reset()\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
    "        tstate = meta_opt.meta_step(tstate, grads, batch)\n",
    "        \n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0: s['eval_loss'] = sum(forward(tstate, batch) for batch in test_ds.as_numpy_iterator()) / len(test_ds)\n",
    "        if m_method == 'scalar': s['M'] = meta_opt.cstate.M.reshape(-1)\n",
    "        stats[t] = s\n",
    "\n",
    "        # print if we gotta\n",
    "        if t % PRINT_EVERY == 0 and t > 0:\n",
    "            idxs = [stats[i] for i in range(t - PRINT_EVERY, t) if i in stats]\n",
    "            avg_train_loss = np.mean([s['loss'] for s in idxs if 'loss' in s])\n",
    "            avg_eval_loss = np.mean([s['eval_loss'] for s in idxs if 'eval_loss' in s])\n",
    "            print(f'iters {t - PRINT_EVERY} - {t}')\n",
    "            print(f'\\tavg train loss: {avg_train_loss}')\n",
    "            print(f'\\tavg eval loss: {avg_eval_loss}')\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada709b3-489a-44e8-bba2-cfd9221f0988",
   "metadata": {},
   "source": [
    "# Hypergradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hgd(seed, initial_lr: float, hypergrad_lr: float):\n",
    "\n",
    "    optimizer = optax.inject_hyperparams(optax.sgd)(learning_rate=initial_lr)\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = {'name': 'hgd',\n",
    "                              'initial_lr': initial_lr,\n",
    "                              'hypergrad_lr': hypergrad_lr,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "    \n",
    "    prev_grads = None\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=len(train_ds))):\n",
    "        t += 1\n",
    "    \n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
    "        if prev_grads is not None: \n",
    "            hypergrad = -sum([(g1 * g2).sum() for g1, g2 in zip(jax.tree_util.tree_leaves(grads), jax.tree_util.tree_leaves(prev_grads))])\n",
    "            tstate.opt_state.hyperparams['learning_rate'] -= hypergrad_lr * hypergrad\n",
    "        prev_grads = grads\n",
    "        \n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['loss'] = loss\n",
    "        s['lr'] = tstate.opt_state.hyperparams['learning_rate'].item()\n",
    "        if t % EVAL_EVERY == 0: s['eval_loss'] = sum(forward(tstate, batch) for batch in test_ds.as_numpy_iterator()) / len(test_ds)\n",
    "        stats[t] = s\n",
    "    \n",
    "        # print if we gotta\n",
    "        if t % PRINT_EVERY == 0 and t > 0:\n",
    "            idxs = [stats[i] for i in range(t - PRINT_EVERY, t) if i in stats]\n",
    "            avg_train_loss = np.mean([s['loss'] for s in idxs if 'loss' in s])\n",
    "            avg_eval_loss = np.mean([s['eval_loss'] for s in idxs if 'eval_loss' in s])\n",
    "            print(f'iters {t - PRINT_EVERY} - {t}')\n",
    "            print(f'\\tavg train loss: {avg_train_loss}')\n",
    "            print(f'\\tavg eval loss: {avg_eval_loss}')\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125998da-b7ed-4d07-af09-d2020a813e0a",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934ab689-7c95-4bf5-af31-86d4895d496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently available stats:\n"
     ]
    }
   ],
   "source": [
    "print('currently available stats:')\n",
    "for k in list(globals().keys()):\n",
    "    if 'stats' in k: print('\\t', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ab476-5e1b-48db-b914-0395115a741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▊                                                                     | 201/8000 [00:11<09:02, 14.36it/s, loss=0.173]"
     ]
    }
   ],
   "source": [
    "# sgd_stats = train_standard_opt(SEED, optax.inject_hyperparams(optax.sgd)(learning_rate=0.2)) \n",
    "# adam_stats = train_standard_opt(SEED, optax.inject_hyperparams(optax.adam)(learning_rate=0.001)) \n",
    "scalar_mo_stats = train_meta_opt(SEED, 'scalar', meta_lr=0.015, H=8, HH=3, initial_lr=0.2)\n",
    "# diagonal_mo_stats = train_meta_opt(SEED, 'diagonal', meta_lr=1., H=4, HH=2, initial_lr=0.2)\n",
    "# hgd_stats = train_hgd(SEED, initial_lr=0.2, hypergrad_lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703c2448-249d-421f-a584-f38e6cbcca7a",
   "metadata": {},
   "source": [
    "# Load/Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da954d-20fe-4818-9a01-bab3858a7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'scalar_mo_stats'\n",
    "\n",
    "# assert name in globals()\n",
    "# with open(f'./data/{name}.pkl', 'wb') as f:\n",
    "#     pkl.dump(globals()[name], f)\n",
    "#     print(f'dumped {name} to {f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975d811-0d96-47fb-8ac4-1529c56e04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd_stats = pkl.load(open('./data/sgd_stats.pkl', 'rb'))\n",
    "# adam_stats = pkl.load(open('./data/adam_stats.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bfe43-a5b7-4855-8e6c-294a129080a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "things_to_plot = [\n",
    "                    'loss', \n",
    "                    'eval_loss', \n",
    "                    'lr',\n",
    "                    ]\n",
    "experiments_to_plot = [\n",
    "                       'sgd', \n",
    "                       'adam', \n",
    "                       # 'hgd', \n",
    "                       # 'diagonal_mo', \n",
    "                       'scalar_mo',\n",
    "                      ]\n",
    "\n",
    "fig, ax = plt.subplots(len(things_to_plot), 1, figsize=(10, 24))\n",
    "for e in experiments_to_plot:\n",
    "    try: s = globals()[e + '_stats']\n",
    "    except: continue\n",
    "    ts = [int(k) for k in s.keys() if k != 'args']\n",
    "    for _ax, p in zip(ax, things_to_plot):\n",
    "        _ts, _vals = [], []\n",
    "        for t in ts:\n",
    "            if p in s[t]:\n",
    "                _ts.append(t)\n",
    "                _vals.append(s[t][p])\n",
    "        if len(_ts) == 0:\n",
    "            print(f'{e} has no statistic \\\"{p}\\\"')\n",
    "            continue\n",
    "        _ax.set_title(p)\n",
    "        _ax.plot(_ts, _vals, label=e)\n",
    "        _ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
