{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "79cdc2ec-af42-4061-a6f5-3287d7d9ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import struct\n",
    "import tensorflow as tf\n",
    "\n",
    "from controllers._base import ControllerState\n",
    "\n",
    "from training.trainer import TrainState, reset_model, create_train_state, forward, forward_and_backward, apply_gradients\n",
    "from training.hgd import HGDState, hypergrad_step\n",
    "from training.utils import cross_entropy, mse, load_mnist, MLP, CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6efa1f40-84cf-4a99-83ab-e4809c0f56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, optimizer, and train state\n",
    "rng = jax.random.PRNGKey(1)\n",
    "model = MLP([28 * 28, 100, 100, 10])\n",
    "optimizer = optax.inject_hyperparams(optax.sgd)(learning_rate=0.01, momentum=0.)\n",
    "init_rng, rng = jax.random.split(rng)\n",
    "tstate = create_train_state(init_rng, model, [28, 28], optimizer, None)\n",
    "del init_rng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9610c23a-0713-4e94-a734-4f9540f7cbe3",
   "metadata": {},
   "source": [
    "## Define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75b2d0b0-ec89-45b2-a349-1872b4983f20",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "JAX only supports number and bool dtypes, got dtype <class 'jax._src.tree_util.Partial'> in array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ma\u001b[39m(z):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPartial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mat[i]\u001b[38;5;241m.\u001b[39mset(jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mPartial(a))\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2097\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2094\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly implemented for order=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2096\u001b[0m \u001b[38;5;66;03m# check if the given dtype is compatible with JAX\u001b[39;00m\n\u001b[0;32m-> 2097\u001b[0m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_user_dtype_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;66;03m# Here we make a judgment call: we only return a weakly-typed array when the\u001b[39;00m\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;66;03m# input object itself is weakly typed. That ensures asarray(x) is a no-op\u001b[39;00m\n\u001b[1;32m   2101\u001b[0m \u001b[38;5;66;03m# whenever x is weak, but avoids introducing weak types with something like\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;66;03m# array([1, 2, 3])\u001b[39;00m\n\u001b[1;32m   2103\u001b[0m weak_type \u001b[38;5;241m=\u001b[39m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mis_weakly_typed(\u001b[38;5;28mobject\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/jax/_src/dtypes.py:685\u001b[0m, in \u001b[0;36mcheck_user_dtype_supported\u001b[0;34m(dtype, fun_name)\u001b[0m\n\u001b[1;32m    683\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX only supports number and bool dtypes, got dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fun_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 685\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m np_dtype \u001b[38;5;241m!=\u001b[39m canonicalize_dtype(dtype):\n\u001b[1;32m    687\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplicitly requested dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not available, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be truncated to dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. To enable more dtypes, set the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax_enable_x64 configuration option or the JAX_ENABLE_X64 shell \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    690\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    691\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/google/jax#current-gotchas for more.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: JAX only supports number and bool dtypes, got dtype <class 'jax._src.tree_util.Partial'> in array"
     ]
    }
   ],
   "source": [
    "def a(z):\n",
    "    return z ** 2\n",
    "\n",
    "for i in range(5):\n",
    "    x = x.at[i].set(jax.tree_util.Partial(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da05a7-ff49-4c3f-98ab-959ead6d2826",
   "metadata": {},
   "source": [
    "## Define GPC Controller for Meta-Opt\n",
    "\n",
    "- \"state\" means parameters at current iteration\n",
    "- \"cost_fn\" means function to optimize at current iteration, can be MSE on current batch or something\n",
    "- \"disturbance\" means gradient at previous iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2d37911-b45c-46eb-81e7-12af058e6e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 58 (1056905210.py, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[104], line 65\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_control(cstate: MetaOptGPCState):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 58\n"
     ]
    }
   ],
   "source": [
    "class MetaOptGPCState(ControllerState):\n",
    "    M: jnp.ndarray  # disturbance-feedback control matrices\n",
    "    state_history: jnp.ndarray  # state history, for hallucinations\n",
    "    cost_fn_history: deque  # history of cost functions, for hallucinations\n",
    "    disturbance_history: jnp.ndarray  # disturbance history\n",
    "    \n",
    "    state_dim: int = struct.field(pytree_node=False)\n",
    "    control_dim: int = struct.field(pytree_node=False)\n",
    "    H: int = struct.field(pytree_node=False)  # history of the controller, how many past disturbances to use for control\n",
    "    HH: int = struct.field(pytree_node=False)  # history of the system, how many hallucination steps to take\n",
    "    t: int  # time counter (for decaying learning rate)\n",
    "    lr: float\n",
    "    decay_lr: bool = struct.field(pytree_node=False)\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, \n",
    "               state_dim: int,\n",
    "               control_dim: int,\n",
    "               m_method: str,\n",
    "               H: int,\n",
    "               HH: int,\n",
    "               lr: float = 0.008,\n",
    "               decay_lr: bool = True,):\n",
    "        \n",
    "        if m_method == 'scalar': M = jnp.zeros((H, 1))\n",
    "        elif m_method == 'diagonal': M = jnp.zeros((H, state_dim))\n",
    "        elif m_method == 'full': M = jnp.zeros((H, control_dim, state_dim))\n",
    "        else: raise NotImplementedError(m_method)\n",
    "        \n",
    "        state_history = jnp.zeros((HH, state_dim))  # past HH states ordered increasing in time\n",
    "        disturbance_history = jnp.zeros((H + HH, state_dim))  # Past H + HH noises ordered increasing in time\n",
    "        cost_fn_history = deque([], maxlen=HH)  # past HH cost functions ordered increasing in time\n",
    "        tx = optax.inject_hyperparams(optax.sgd)(learning_rate=lr)  # M optimizer\n",
    "        opt_state = tx.init(M)\n",
    "        \n",
    "        return cls(M=M, state_history=state_history, disturbance_history=disturbance_history, cost_fn_history=cost_fn_history,\n",
    "                   state_dim=state_dim, control_dim=control_dim, H=H, HH=HH, t=0,\n",
    "                   lr=lr, decay_lr=decay_lr, tx=tx, opt_state=opt_state)\n",
    "\n",
    "@jax.jit\n",
    "def _compute_control(M, disturbances):\n",
    "    if len(M.shape) == 3: control = jnp.tensordot(M, disturbances, axes=([0, 2], [0, 1]))\n",
    "    else: control = (M * disturbances).sum(axis=0)\n",
    "    return control\n",
    "\n",
    "@jax.jit\n",
    "def _compute_loss(cstate: MetaOptGPCState, \n",
    "                  intial_state,\n",
    "                  step_fn,  # takes in [tstate, cost_fn] and outputs [new_tstate,]. most likely via gradient descent\n",
    "                  cost_fn,  # cost function to evaluate hallucinated final state with\n",
    "                 ):\n",
    "    \"\"\"FINSIH\"\"\"\n",
    "    def _evolve(tstate, h):\n",
    "        return cstate.A @ state + _compute_control(cstate.M, jax.lax.dynamic_slice_in_dim(cstate.disturbance_history, h, cstate.H)), None\n",
    "    final_tstate, _ = jax.lax.scan(_evolve, intial_tstate, jnp.arange(cstate.HH - 1))\n",
    "    return quad_loss(final_state, _action(final_state, cstate.HH - 1))\n",
    "\n",
    "def update(cstate: MetaOptGPCState,\n",
    "           params,\n",
    "           disturbance,\n",
    "           cost_fn,\n",
    "          ):\n",
    "    \n",
    "    updates, new_opt_state = cstate.tx.update(grads, cstate.opt_state, cstate.M)\n",
    "    M = optax.apply_updates(cstate.M, updates[0])\n",
    "\n",
    "    \n",
    "\n",
    "    return cstate.replace(M=M, opt_state=new_opt_state, disturbance_history=disturbance_history, t=cstate.t+1)   \n",
    "    \n",
    "\n",
    "def get_control(cstate: MetaOptGPCState):\n",
    "    return _compute_control(cstate.M, jax.lax.dynamic_slice_in_dim(cstate.disturbance_history, -cstate.H, cstate.H))\n",
    "\n",
    "def reset(cstate: MetaOptGPCState):\n",
    "    state_history = deque([], maxlen=cstate.HH)  # past HH states ordered increasing in time\n",
    "    disturbance_history = jnp.zeros((cstate.H + cstate.HH, cstate.state_dim))  # Past H + HH noises ordered increasing in time\n",
    "    cost_fn_history = deque([], maxlen=cstate.HH)  # past HH cost functions ordered increasing in time\n",
    "    return cstate.replace(state_history=state_history, disturbance_history=disturbance_history, cost_fn_history=cost_fn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088911d1-219e-4411-8d70-e5e9fc58fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## what fns will we need\n",
    "\n",
    "def gd_step(tstate, batch):  # take gd update step w current opt on batch\n",
    "    return tstate\n",
    "\n",
    "def gpc_cost(cstate, ...):  # used to take derivative w.r.t. M's in order to update gpc controller. should HALLUCINATE!!!\n",
    "    return cost\n",
    "\n",
    "def update(cstate, params, disturbance, cost_fn):\n",
    "    # append params\n",
    "    # compute update using cstate's disturbance and cost fn histories\n",
    "    # append disturbance, cost fn, and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f31be-2854-46bd-aee9-44bfebf8a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how meta opt train loop should go\n",
    "for step in range(tsteps):\n",
    "    tstate = gd_step(tstate, batch)  # maybe make this to take input arbitrary functions, not just batches\n",
    "    tstate = (1-delta) * tstate + get_control(...)  # use disturbances up to and including `step-1`, but NOT the gradient we just calculated above\n",
    "    gpc = update_gpc(...)  # hallucainate. use cost fns during history for GD steps, but compute final state loss with cost from 2 lines ago\n",
    "    # append grad to disturbances!\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
