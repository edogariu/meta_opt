{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4fc9de-e02f-4ca4-a807-3b42951c6cc7",
   "metadata": {},
   "source": [
    "This notebook is for experiments probing things other than performance, such as checking conditions and assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782d902f-6c49-4270-a7d2-ca44be9503f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for use in google colab!!\n",
    "# !git clone https://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@github.com/edogariu/meta-opt\n",
    "# !pip install ./meta-opt\n",
    "# !pip install tensorflow-text ml_collections clu sentencepiece  # for WMT\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DIR_PREFIX = \"drive/My Drive/meta-opt\"\n",
    "\n",
    "# # for extra one-time setup in colab\n",
    "# !git clone https://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@github.com/edogariu/meta-opt\n",
    "# !mkdir meta-opt/data\n",
    "# !mkdir meta-opt/datasets\n",
    "# !cp -r \"meta-opt\" \"drive/My Drive/\"\n",
    "# !pip install kora -q  # library from https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab to help get ID\n",
    "# from kora.xattr import get_id\n",
    "# fid = get_id(f\"{dir_prefix}meta_opt.ipynb\")\n",
    "# print(\"https://colab.research.google.com/drive/\"+fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f07326-a164-4e44-8a3f-13aa6770c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from itertools import accumulate\n",
    "from collections import defaultdict, deque\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from meta_opt.controllers.utils import append\n",
    "from meta_opt.nn.trainer import create_train_state, train_step, reset_model, eval\n",
    "from meta_opt.problems import mnist, cifar10, wmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4467e-f9fd-4654-aa50-76f63e3af756",
   "metadata": {},
   "source": [
    "# Background code \n",
    "Code to reproducibly train the model and collect stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59a4f7e5-6b3c-4338-88c0-e1231c0c130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed is None: \n",
    "        seed = np.random.randint()\n",
    "        print('seed set to {}'.format(seed))\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    return rng, seed\n",
    "\n",
    "\n",
    "def run_trial(seed, problem_name, optimizer, experiment_step_fn):\n",
    "    \"\"\"\n",
    "    `experiment_step_fn` is a function mapping `tstate, batch, carry -> tstate, (loss, grads, stats, carry)` for each timestep\n",
    "\n",
    "    to run regular experiment with no funny business or extra logging, one would set `experiment_step_fn` to be `gradient_descent` but also returning an empty dict\n",
    "    \"\"\"\n",
    "    \n",
    "    rng, seed = set_seed(seed)\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "\n",
    "    cfg = {'num_iters': NUM_ITERS, 'batch_size': BATCH_SIZE, 'eval_every': EVAL_EVERY, 'reset_every': RESET_EVERY, 'num_eval_iters': -1}\n",
    "\n",
    "    # get dataset and model\n",
    "    if 'MNIST' in problem_name:\n",
    "        train_ds, test_ds, example_input, loss_fn, metric_fns = mnist.load_mnist(cfg, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
    "        # model = mnist.MLP([28 * 28, 100, 100, 10])\n",
    "        model = mnist.MLP([28 * 28, 1, 10])\n",
    "    elif 'CIFAR' in problem_name:\n",
    "        train_ds, test_ds, example_input, loss_fn, metric_fns = cifar10.load_cifar10(cfg, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
    "        model = cifar10.VGG(stages=((32, 32), (64, 64), (128, 128)), layer_dims=[128, 10], drop_last_activation=True, dropout=0.1)\n",
    "    elif 'WMT' in problem_name:\n",
    "        train_ds, test_ds, example_input, loss_fn, metric_fns = wmt.load_wmt(cfg, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
    "        model = wmt.make_transformer(num_heads=8, num_layers=6, emb_dim=256, qkv_dim=256, mlp_dim=1024)\n",
    "    else:\n",
    "        raise NotImplementedError(problem_name)\n",
    "    \n",
    "    tstate = create_train_state(init_rng, model, example_input, optimizer, loss_fn, metric_fns=metric_fns)\n",
    "    del init_rng\n",
    "\n",
    "    args = {'seed': seed,\n",
    "            'model': str(model),\n",
    "            'params': sum(x.size for x in jax.tree_util.tree_leaves(tstate.params)),\n",
    "            'dataset': problem_name,\n",
    "            'num_iters': NUM_ITERS,\n",
    "            'eval_every': EVAL_EVERY,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'reset_every': RESET_EVERY}\n",
    "    print(args['params'], 'params in the model')\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = deepcopy(tstate.opt_state.hyperparams)\n",
    "    args['optimizer_args']['name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    carry = None\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "    \n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        # --------------------------------------------------------------------------------\n",
    "        tstate, (loss, grads, s, carry) = experiment_step_fn(tstate, batch, carry)  # this is what changes between different experiments\n",
    "        # --------------------------------------------------------------------------------\n",
    "        print(s['sequential_stability'])\n",
    "        # update all the stats\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0: \n",
    "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
    "            n = 0\n",
    "            for batch in test_ds.as_numpy_iterator():\n",
    "                loss, acc = eval(tstate, batch)\n",
    "                s['eval_loss'] += loss\n",
    "                s['eval_acc'] += acc\n",
    "                n += 1\n",
    "            s['eval_loss'] /= n\n",
    "            s['eval_acc'] /= n\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "        stats[t] = s\n",
    "    \n",
    "    return dict(stats)\n",
    "\n",
    "def run_experiment(seeds, problem_name, optimizer, experiment_step_fn, fname=f'{DIR_PREFIX}/data/{NAME}_sequentialstability_raw.pkl'):\n",
    "    results = []\n",
    "    \n",
    "    for s in seeds:\n",
    "        results.append(run_trial(s, NAME, optax.inject_hyperparams(optax.sgd)(0.1), sequential_stability))\n",
    "    \n",
    "        if len(results) > 0:\n",
    "            pkl.dump(results, open(fname, 'wb'))\n",
    "            print(f'Saved checkpoint for seed #{s}')\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1a678-5b6a-4ee6-a2d9-b57fc2a3c2b8",
   "metadata": {},
   "source": [
    "# Sequential Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9433b-0947-41d5-81c0-99b690e19681",
   "metadata": {},
   "source": [
    "A time-varying linear dynamical system with dynamics $A_1, \\ldots, A_T$ is $(\\kappa, \\gamma)$-sequentially stable if  for all intervals $I = [r, s]\\subseteq [T]$,\n",
    "$$\n",
    "\\left \\|\\prod_{t=s}^{r} A_t\\right \\| \\le \\kappa^2 (1-\\gamma)^{|I|}$$\n",
    "We check if the LTV in meta-opt is indeed sequentially stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa377ad-8191-4a87-bde2-19562ffd8e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 16  # for computational reasons, we will only compute with lengths up to this value\n",
    "DELTA = 0.001  # (1-delta) decay factor for state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac2716-6f83-44e9-aa26-963fc53bc7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805 params in the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                        | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2415, 2415) (16,) Traced<ShapedArray(float32[16])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def forward_and_backward_with_hessian(tstate, batch):\n",
    "    if tstate.rng is not None:\n",
    "        next_key, dropout_key = jax.random.split(tstate.rng)\n",
    "        tstate = tstate.replace(rng=next_key)\n",
    "    else: dropout_key = None\n",
    "    def loss_fn(params):\n",
    "        yhat = tstate.apply_fn({'params': params}, batch['x'], train=True, rngs={'dropout': dropout_key})\n",
    "        loss = tstate.loss_fn(yhat, batch['y'])\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(tstate.params)\n",
    "\n",
    "    p, td = jax.tree_util.tree_flatten(tstate.params)\n",
    "    def loss_fn_from_flat(params_flat):  # for hessian computation\n",
    "        q = []\n",
    "        n = 0\n",
    "        for v in p:\n",
    "            d = np.prod(v.shape)\n",
    "            q.append(params_flat[n: n + d].reshape(v.shape))\n",
    "            n += d\n",
    "        params = jax.tree_util.tree_unflatten(td, q)\n",
    "        return loss_fn(params)\n",
    "        \n",
    "    hessians = jax.hessian(loss_fn_from_flat)(jnp.concatenate([_p.reshape(-1) for _p in p], axis=0))\n",
    "    return tstate, (loss, grads, hessians)\n",
    "\n",
    "@jax.jit\n",
    "def sequential_stability(tstate, batch, carry):\n",
    "    # the vanilla stuff, but also computing hessian\n",
    "    stats = {}\n",
    "    tstate, (loss, grads, hessians) = forward_and_backward_with_hessian(tstate, batch)\n",
    "    tstate = tstate.apply_gradients(grads=grads)\n",
    "\n",
    "    # use hessian to compute transition matrix and append to the buffer. note that this is using batch averages\n",
    "    def f(H, eta, delta, carry):\n",
    "        I = jnp.eye(H.shape[0])\n",
    "        A = jnp.block([[(1 - delta) * I, 0 * I, -eta * I], [I, 0 * I, 0 * I], [H, -H, 0 * I]])  # transition matrix for this step\n",
    "        carry = A @ append(carry, jnp.eye(A.shape[0]))  # append an entry of 1 to the right, then left multiply each entry by A. this dynamically handles the cumprod\n",
    "        spectral_norms = jnp.linalg.norm(carry, axis=(1, 2), ord=2)\n",
    "        return carry, spectral_norms\n",
    "    \n",
    "    H = hessians # + 2 * beta * jnp.eye(hessians.shape[0])  # TODO CHECK THIS!!!\n",
    "    if carry is None: carry = jnp.zeros((MAX_LEN, H.shape[0] * 3, H.shape[0] * 3))\n",
    "    carry, spectral_norms = f(H, tstate.opt_state.hyperparams['learning_rate'], DELTA, carry)\n",
    "    print(carry.shape, spectral_norms.shape, spectral_norms)\n",
    "    stats['sequential_stability'] = spectral_norms\n",
    "\n",
    "    return tstate, (loss, grads, stats, carry)\n",
    "\n",
    "results = run_experiment(SEEDS, NAME, optax.inject_hyperparams(optax.sgd)(0.1), sequential_stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65966847-6875-40bb-875e-a07b6636139f",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c157174-896a-4882-a162-00d0aa916b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29397d8f-77c9-4896-902b-50890e384cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
