{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239db45b-400e-4e3e-88ee-d67fa7f32643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n"
     ]
    }
   ],
   "source": [
    "# Note: must set this env variable before jax is imported\n",
    "import os\n",
    "os.environ['XLA_FLAGS'] = \"--xla_force_host_platform_device_count=8\"  # hack to pretend i have many devices :)\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import functools\n",
    "\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6a3eba-e6bc-4333-aa3a-ecd2062d559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a \"dataset\"\n",
    "batch_size, input_dim = 64, 500\n",
    "num_devices = len(jax.devices())\n",
    "assert num_devices == 8 and batch_size % num_devices == 0\n",
    "\n",
    "def batch(gt):  # generate random batch of inputs and g.t. outputs\n",
    "    xs = jnp.array(np.random.randn(batch_size, input_dim)) / 10\n",
    "    ys = (xs * gt).sum(axis=-1)\n",
    "    return (xs, ys)\n",
    "\n",
    "def forward(_params, _x, _y):\n",
    "    yhat = (_params * _x).sum(axis=-1)\n",
    "    loss = ((yhat - _y) ** 2).sum()\n",
    "    return loss\n",
    "\n",
    "@functools.partial(jax.pmap, axis_name='devices', in_axes=(0, 0, 0))\n",
    "def forward_pmapped(_params, _x, _y):\n",
    "    yhat = (_params * _x).sum(axis=1)\n",
    "    loss = ((yhat - _y) ** 2).sum()\n",
    "    return jax.lax.psum(loss, axis_name='devices')\n",
    "\n",
    "def train_step(_params, _eta, _x, _y):\n",
    "    def loss_fn(p): return forward(p, _x, _y)\n",
    "    grads = jax.grad(loss_fn)(_params)\n",
    "    return _params - _eta * grads\n",
    "\n",
    "@functools.partial(jax.pmap, axis_name='devices', in_axes=(0, None, 0, 0))\n",
    "def train_step_pmapped(_params, _eta, _x, _y):\n",
    "    def loss_fn(p): return forward(p, _x, _y)\n",
    "    grads = jax.grad(loss_fn)(_params)\n",
    "    grads = jax.lax.psum(grads, axis_name='devices')\n",
    "    return _params - _eta * grads\n",
    "\n",
    "\n",
    "def meta_step(_params, _eta, _x, _y, do_pmap: bool = False):\n",
    "    HH = 2\n",
    "    meta_eta = 1e-1\n",
    "    def _meta_loss(_eta, _params, _x, _y):\n",
    "        loss = None\n",
    "        if not do_pmap:  # do regular meta learning\n",
    "            for _ in range(HH):\n",
    "              _params = train_step(_params, _eta, _x, _y)  # rollout `HH` steps\n",
    "            loss = forward(_params, _x, _y)\n",
    "            return loss, _params\n",
    "\n",
    "        else:  # do pmapped meta learning\n",
    "            _x_reshaped = _x.reshape(num_devices, -1, input_dim)\n",
    "            _y_reshaped = _y.reshape(num_devices, -1)\n",
    "            for _ in range(HH):\n",
    "              _params = train_step_pmapped(_params, _eta, _x_reshaped, _y_reshaped)\n",
    "            loss = forward_pmapped(_params, _x_reshaped, _y_reshaped)\n",
    "            return loss.at[0].get(), _params\n",
    "\n",
    "\n",
    "    if do_pmap:\n",
    "        _x_reshaped = _x.reshape(num_devices, -1, input_dim)\n",
    "        _y_reshaped = _y.reshape(num_devices, -1)\n",
    "        (loss, _params), meta_grad = jax.value_and_grad(_meta_loss, has_aux=True)(_eta, _params, _x_reshaped, _y_reshaped)\n",
    "        return _params, _eta - meta_eta * meta_grad, loss, meta_grad\n",
    "    else:\n",
    "        (loss, _params), meta_grad = jax.value_and_grad(_meta_loss, has_aux=True)(_eta, _params, _x, _y)\n",
    "        return _params, _eta - meta_eta * meta_grad, loss, meta_grad\n",
    "\n",
    "\n",
    "# \"train\"\n",
    "def trial(seed, use_pmap, T):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # define the model and loss\n",
    "    gt = jnp.array(np.random.randn(input_dim)) / 10  # ground truth model params\n",
    "    params = jnp.zeros((input_dim,))  # params of the model\n",
    "    if use_pmap:\n",
    "      params = flax.jax_utils.replicate(params)\n",
    "    eta = 0.1  # adaptive lr\n",
    "\n",
    "    losses = []\n",
    "    lrs = []\n",
    "    meta_grads = []\n",
    "    for _ in range(T):\n",
    "        x, y = batch(gt)\n",
    "        params, eta, loss, meta_grad = meta_step(params, eta, x, y, use_pmap)\n",
    "        losses.append(loss)\n",
    "        lrs.append(eta)\n",
    "        meta_grads.append(meta_grad)\n",
    "    return np.array(losses), np.array(lrs), np.array(meta_grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06c508-19fc-4dc1-9db9-e44c9547ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.arange(5)\n",
    "T = 100\n",
    "\n",
    "# run repeated experiments\n",
    "experiment_losses, experiment_lrs, experiment_meta_grads = [], [], []\n",
    "pmap_experiment_losses, pmap_experiment_lrs, pmap_experiment_meta_grads = [], [], []\n",
    "\n",
    "for s in seeds:\n",
    "    losses, lrs, meta_grads = trial(s, use_pmap=False, T=T)\n",
    "    pmap_losses, pmap_lrs, pmap_meta_grads = trial(s, use_pmap=True, T=T)\n",
    "    experiment_losses.append(losses); experiment_lrs.append(lrs); experiment_meta_grads.append(meta_grads)\n",
    "    pmap_experiment_losses.append(pmap_losses); pmap_experiment_lrs.append(pmap_lrs); pmap_experiment_meta_grads.append(pmap_meta_grads)\n",
    "experiment_losses, experiment_lrs, experiment_meta_grads, pmap_experiment_losses, pmap_experiment_lrs, pmap_experiment_meta_grads = map(np.array, [experiment_losses, experiment_lrs, experiment_meta_grads, pmap_experiment_losses, pmap_experiment_lrs, pmap_experiment_meta_grads])\n",
    "\n",
    "# plot\n",
    "def plot(arr, label, ax):\n",
    "    assert arr.shape == (len(seeds), T)\n",
    "    ts = range(T); avgs = np.mean(arr, 0); stds = np.std(arr, 0)\n",
    "    ax.plot(ts, avgs, label=label)\n",
    "    ax.fill_between(ts, avgs - 1.96 * stds, avgs + 1.96 * stds, alpha=0.2)\n",
    "    ax.legend()\n",
    "    pass\n",
    "\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "plot(experiment_losses, 'regular', ax[0])\n",
    "plot(pmap_experiment_losses, 'pmapped', ax[0])\n",
    "plot(experiment_lrs, 'regular', ax[1])\n",
    "plot(pmap_experiment_lrs, 'pmapped', ax[1])\n",
    "plot(experiment_meta_grads, 'regular', ax[2])\n",
    "plot(pmap_experiment_meta_grads, 'pmapped', ax[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
