{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
        "outputId": "61d99c82-819c-46b3-9944-f61f04462b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'meta-opt'...\n",
            "remote: Enumerating objects: 474, done.\u001b[K\n",
            "remote: Counting objects: 100% (67/67), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 474 (delta 41), reused 5 (delta 5), pack-reused 407\u001b[K\n",
            "Receiving objects: 100% (474/474), 57.50 MiB | 10.20 MiB/s, done.\n",
            "Resolving deltas: 100% (269/269), done.\n",
            "Processing ./meta-opt\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from meta-opt==1.0) (0.42.0)\n",
            "Building wheels for collected packages: meta-opt\n",
            "  Building wheel for meta-opt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for meta-opt: filename=meta_opt-1.0-py3-none-any.whl size=35179 sha256=d7ac6ed36d49db6c6d92dc4606bdd8582b2b90e2afdb4db70c284a894f5cd3a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/7f/cc/02e1e9308a7a6a9cf424bab756bd66019f03234ddb8b8ec006\n",
            "Successfully built meta-opt\n",
            "Installing collected packages: meta-opt\n",
            "Successfully installed meta-opt-1.0\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clu\n",
            "  Downloading clu-0.0.10-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.15.0)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from clu) (1.6.0)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from clu) (0.7.5)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from clu) (0.4.23)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from clu) (0.4.23+cuda12.cudnn89)\n",
            "Collecting numpy==1.23.1 (from clu)\n",
            "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clu) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from clu) (4.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from clu) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow<2.16,>=2.15.0 (from tensorflow-text)\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clu\n",
            "  Downloading clu-0.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from clu) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->clu) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->clu) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->clu) (3.17.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->clu) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax->clu) (0.1.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->clu) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->clu) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax->clu) (13.7.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->clu) (1.11.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.42.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->clu) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax->clu) (2.16.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.0.1)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax->clu) (0.1.7)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->clu) (1.5.8)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax->clu) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax->clu) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->clu) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.2.2)\n",
            "Building wheels for collected packages: ml_collections\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94505 sha256=2439b0e96f538fd8a7e154d1907e0ad486e1ea45eda4b09bab9919395e64ff0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml_collections\n",
            "Installing collected packages: sentencepiece, ml_collections, tensorflow-text, clu\n",
            "Successfully installed clu-0.0.9 ml_collections-0.1.1 sentencepiece-0.1.99 tensorflow-text-2.15.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# for use in google colab!!\n",
        "!git clone https://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@github.com/edogariu/meta-opt\n",
        "!pip install ./meta-opt\n",
        "!pip install tensorflow-text ml_collections clu sentencepiece  # for WMT\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DIR_PREFIX = \"drive/My Drive/meta-opt\"\n",
        "\n",
        "# # for extra one-time setup in colab\n",
        "# !git clone https://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@github.com/edogariu/meta-opt\n",
        "# !mkdir meta-opt/data\n",
        "# !mkdir meta-opt/datasets\n",
        "# !cp -r \"meta-opt\" \"drive/My Drive/\"\n",
        "# !pip install kora -q  # library from https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab to help get ID\n",
        "# from kora.xattr import get_id\n",
        "# fid = get_id(f\"{dir_prefix}meta_opt.ipynb\")\n",
        "# print(\"https://colab.research.google.com/drive/\"+fid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b898f1e-f002-435a-b1b4-65a21e05a8a7",
      "metadata": {
        "id": "7b898f1e-f002-435a-b1b4-65a21e05a8a7"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "from meta_opt.nn.trainer import create_train_state, gradient_descent, reset_model, eval\n",
        "from meta_opt.problems import mnist, cifar10, wmt\n",
        "\n",
        "from meta_opt.meta_opt import MetaOpt\n",
        "from meta_opt.gaps import MetaOptGAPS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6730ed64-63a1-46de-8106-28529a15467e",
      "metadata": {
        "id": "6730ed64-63a1-46de-8106-28529a15467e"
      },
      "source": [
        "### Todo\n",
        "- add caching to dataloaders\n",
        "- add MP, cosine, cyclical learning rates, hedging, AGD, DoWG, D-adaptation, adagrad & rmsprop\n",
        "- try other settings\n",
        "- check \"training instability\" literature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12",
      "metadata": {
        "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    if seed is None:\n",
        "        seed = np.random.randint()\n",
        "        print('seed set to {}'.format(seed))\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "    return rng, seed\n",
        "\n",
        "def get_problem(seed, name, optimizer):\n",
        "    rng, seed = set_seed(seed)\n",
        "    init_rng, rng = jax.random.split(rng)\n",
        "\n",
        "    # get dataset and model\n",
        "    if 'MNIST' in name:\n",
        "        train_ds, test_ds, example_input, loss_fn, acc_fn = mnist.load_mnist(NUM_ITERS, BATCH_SIZE, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
        "        model = mnist.MLP([28 * 28, 100, 100, 10])\n",
        "    elif 'CIFAR' in name:\n",
        "        train_ds, test_ds, example_input, loss_fn, acc_fn = cifar10.load_cifar10(NUM_ITERS, BATCH_SIZE, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
        "        model = cifar10.VGG(stages=((32, 32), (64, 64), (128, 128)), layer_dims=[128, 10], drop_last_activation=True, dropout=0.1)\n",
        "    elif 'WMT' in name:\n",
        "        train_ds, test_ds, example_input, loss_fn, acc_fn, tokenizer = wmt.load_wmt(NUM_ITERS, BATCH_SIZE, dataset_dir=f'{DIR_PREFIX}/datasets', num_eval_iters=NUM_EVAL_ITERS if 'NUM_EVAL_ITERS' in globals() else 256)\n",
        "        train_ds.cache()\n",
        "        model = wmt.make_transformer(num_heads=16, num_layers=6, emb_dim=1024, qkv_dim=1024, mlp_dim=4096)\n",
        "        # model = wmt.make_transformer(num_heads=4, num_layers=3, emb_dim=64, qkv_dim=64, mlp_dim=256)\n",
        "    else:\n",
        "        raise NotImplementedError(name)\n",
        "\n",
        "    tstate = create_train_state(init_rng, model, example_input, optimizer, loss_fn, acc_fn=acc_fn)\n",
        "    del init_rng\n",
        "\n",
        "    args = {'seed': seed,\n",
        "            'model': str(model),\n",
        "            'params': sum(x.size for x in jax.tree_util.tree_leaves(tstate.params)),\n",
        "            'dataset': name,\n",
        "            'num_iters': NUM_ITERS,\n",
        "            'eval_every': EVAL_EVERY,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'reset_every': RESET_EVERY,\n",
        "            'print_every': PRINT_EVERY}\n",
        "    if 'WMT' in name: args['tokenizer'] = tokenizer\n",
        "\n",
        "    return tstate, train_ds, test_ds, rng, args"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68611b25-ff6c-498a-bac1-30873eac9b16",
      "metadata": {
        "id": "68611b25-ff6c-498a-bac1-30873eac9b16"
      },
      "source": [
        "# Standard Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "35553ddc-66dd-454e-9608-8820ba402b03",
      "metadata": {
        "id": "35553ddc-66dd-454e-9608-8820ba402b03"
      },
      "outputs": [],
      "source": [
        "def train_standard_opt(seed, problem_name, optimizer):\n",
        "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
        "\n",
        "    stats = defaultdict(dict)\n",
        "    args['optimizer_args'] = deepcopy(tstate.opt_state.hyperparams)\n",
        "    args['optimizer_args']['name'] = 'standard'\n",
        "    stats['args'] = args\n",
        "\n",
        "    t0 = perf_counter()\n",
        "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
        "        t += 1\n",
        "\n",
        "        if t % RESET_EVERY == 0:\n",
        "            reset_rng, rng = jax.random.split(rng)\n",
        "            tstate = reset_model(reset_rng, tstate)\n",
        "            del reset_rng\n",
        "\n",
        "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
        "\n",
        "        # update all the stats\n",
        "        s = {}\n",
        "        s['timestamp'] = perf_counter() - t0\n",
        "        s['loss'] = loss\n",
        "        if t % EVAL_EVERY == 0:\n",
        "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
        "            n = 0\n",
        "            for batch in test_ds.as_numpy_iterator():\n",
        "                loss, acc = eval(tstate, batch)\n",
        "                s['eval_loss'] += loss\n",
        "                s['eval_acc'] += acc\n",
        "                n += 1\n",
        "            s['eval_loss'] /= n\n",
        "            s['eval_acc'] /= n\n",
        "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
        "        try:\n",
        "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
        "            s['bleu'] = wmt.bleu(tstate, test_ds, tokenizer=args['tokenizer'])   # calculate bleu score if WMT task\n",
        "            print(s['bleu'][1])\n",
        "        except Exception as e:\n",
        "          print('uh oh', BLEU_EVERY, e)\n",
        "        stats[t] = s\n",
        "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
        "\n",
        "    stats['model_params'] = deepcopy(tstate.params)\n",
        "    return dict(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f",
      "metadata": {
        "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f"
      },
      "source": [
        "# Meta-Opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "907792cd-7420-4e61-a458-372e2dac9128",
      "metadata": {
        "id": "907792cd-7420-4e61-a458-372e2dac9128"
      },
      "outputs": [],
      "source": [
        "def train_meta_opt(seed, problem_name: str, m_method: str, meta_lr: float, use_adam: bool, H: int, HH: int, initial_lr: int):\n",
        "    optimizer = optax.sgd(learning_rate=initial_lr)\n",
        "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
        "\n",
        "    stats = defaultdict(dict)\n",
        "    args['optimizer_args'] = {'name': 'meta',\n",
        "                              'initial_lr': initial_lr,\n",
        "                              'm_method': m_method,\n",
        "                              'meta_lr': meta_lr,\n",
        "                              'use_adam': use_adam,\n",
        "                              'H': H,\n",
        "                              'HH': HH\n",
        "                              }\n",
        "    stats['args'] = args\n",
        "    meta_opt = MetaOpt(tstate, H=H, HH=HH, meta_lr=meta_lr, delta=1e-5, m_method=m_method, use_adam=use_adam)\n",
        "\n",
        "    it = iter(train_ds.as_numpy_iterator())\n",
        "    t0 = perf_counter()\n",
        "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
        "        t += 1\n",
        "\n",
        "        if t % RESET_EVERY == 0:\n",
        "            reset_rng, rng = jax.random.split(rng)\n",
        "            tstate = reset_model(reset_rng, tstate)\n",
        "            meta_opt = meta_opt.episode_reset()\n",
        "            del reset_rng\n",
        "\n",
        "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
        "        tstate = meta_opt.meta_step(tstate, grads, batch)\n",
        "\n",
        "        # update all the stats\n",
        "        s = {}\n",
        "        s['timestamp'] = perf_counter() - t0\n",
        "        s['loss'] = loss\n",
        "        if t % EVAL_EVERY == 0:\n",
        "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
        "            n = 0\n",
        "            for batch in test_ds.as_numpy_iterator():\n",
        "                loss, acc = eval(tstate, batch)\n",
        "                s['eval_loss'] += loss\n",
        "                s['eval_acc'] += acc\n",
        "                n += 1\n",
        "            s['eval_loss'] /= n\n",
        "            s['eval_acc'] /= n\n",
        "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
        "        try:\n",
        "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
        "            s['bleu'] = wmt.bleu(tstate, test_ds, tokenizer=args['tokenizer'])   # calculate bleu score if WMT task\n",
        "            print(s['bleu'][1])\n",
        "        except Exception as e:\n",
        "          print('uh oh', BLEU_EVERY, e)\n",
        "        if m_method == 'scalar': s['M'] = meta_opt.cstate.M.reshape(-1)\n",
        "        else: s['M'] = jnp.stack([m.reshape((m.shape[0], -1)).mean(axis=-1) for m in jax.tree_util.tree_leaves(meta_opt.cstate.M)], axis=0).mean(axis=0)\n",
        "        stats[t] = s\n",
        "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), 'M': s['M'].sum()})\n",
        "\n",
        "    stats['model_params'] = deepcopy(tstate.params)\n",
        "    stats['controller_params'] = deepcopy(meta_opt.cstate.M)\n",
        "    return dict(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d10045c-53d9-41f3-b559-f7a0c2fe8bf6",
      "metadata": {
        "id": "4d10045c-53d9-41f3-b559-f7a0c2fe8bf6"
      },
      "source": [
        "# Gradient-based Adaptive Policy Selection (GAPS) Meta-Opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "c5867db6-fe41-4dbd-9934-1a52617cc94c",
      "metadata": {
        "id": "c5867db6-fe41-4dbd-9934-1a52617cc94c"
      },
      "outputs": [],
      "source": [
        "def train_gaps_meta_opt(seed, problem_name: str, m_method: str, meta_lr: float, use_adam: bool, H: int, B: int, initial_lr: int):\n",
        "    optimizer = optax.sgd(learning_rate=initial_lr)\n",
        "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
        "\n",
        "    stats = defaultdict(dict)\n",
        "    args['optimizer_args'] = {'name': 'gaps_meta',\n",
        "                              'initial_lr': initial_lr,\n",
        "                              'm_method': m_method,\n",
        "                              'meta_lr': meta_lr,\n",
        "                              'use_adam': use_adam,\n",
        "                              'H': H,\n",
        "                              'B': B\n",
        "                              }\n",
        "    stats['args'] = args\n",
        "\n",
        "    meta_opt = MetaOptGAPS(tstate, H=H, B=B, meta_lr=meta_lr, use_adam=use_adam, delta=1e-5, m_method=m_method)\n",
        "\n",
        "    t0 = perf_counter()\n",
        "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
        "        t += 1\n",
        "\n",
        "        if t % RESET_EVERY == 0:\n",
        "            reset_rng, rng = jax.random.split(rng)\n",
        "            tstate = reset_model(reset_rng, tstate)\n",
        "            meta_opt = meta_opt.episode_reset()\n",
        "            del reset_rng\n",
        "\n",
        "        # tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
        "        tstate, (loss, grads) = meta_opt.meta_step(tstate, batch)\n",
        "\n",
        "        # update all the stats\n",
        "        s = {}\n",
        "        s['timestamp'] = perf_counter() - t0\n",
        "        s['loss'] = loss\n",
        "        if t % EVAL_EVERY == 0:\n",
        "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
        "            n = 0\n",
        "            for batch in test_ds.as_numpy_iterator():\n",
        "                loss, acc = eval(tstate, batch)\n",
        "                s['eval_loss'] += loss\n",
        "                s['eval_acc'] += acc\n",
        "                n += 1\n",
        "            s['eval_loss'] /= n\n",
        "            s['eval_acc'] /= n\n",
        "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
        "        try:\n",
        "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
        "            s['bleu'] = wmt.bleu(tstate, test_ds, tokenizer=args['tokenizer'])   # calculate bleu score if WMT task\n",
        "            print(s['bleu'][1])\n",
        "        except Exception as e:\n",
        "          print('uh oh', BLEU_EVERY, e)\n",
        "        if m_method == 'scalar': s['M'] = meta_opt.cstate.M.reshape(-1)\n",
        "        else: s['M'] = jnp.stack([m.reshape((m.shape[0], -1)).mean(axis=-1) for m in jax.tree_util.tree_leaves(meta_opt.cstate.M)], axis=0).mean(axis=0)\n",
        "        stats[t] = s\n",
        "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), 'M': s['M'].sum()})\n",
        "\n",
        "    stats['model_params'] = deepcopy(tstate.params)\n",
        "    stats['controller_params'] = deepcopy(meta_opt.cstate.M)\n",
        "    return dict(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada709b3-489a-44e8-bba2-cfd9221f0988",
      "metadata": {
        "id": "ada709b3-489a-44e8-bba2-cfd9221f0988"
      },
      "source": [
        "# Hypergradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3",
      "metadata": {
        "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3"
      },
      "outputs": [],
      "source": [
        "def train_hgd(seed, problem_name: str, initial_lr: float, hypergrad_lr: float):\n",
        "\n",
        "    optimizer = optax.inject_hyperparams(optax.sgd)(learning_rate=initial_lr)\n",
        "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
        "\n",
        "    stats = defaultdict(dict)\n",
        "    args['optimizer_args'] = {'name': 'hgd',\n",
        "                              'initial_lr': initial_lr,\n",
        "                              'hypergrad_lr': hypergrad_lr,\n",
        "                              }\n",
        "    stats['args'] = args\n",
        "\n",
        "    prev_grads = None\n",
        "    t0 = perf_counter()\n",
        "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
        "        t += 1\n",
        "\n",
        "        if t % RESET_EVERY == 0:\n",
        "            reset_rng, rng = jax.random.split(rng)\n",
        "            tstate = reset_model(reset_rng, tstate)\n",
        "            del reset_rng\n",
        "\n",
        "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
        "        if prev_grads is not None:\n",
        "            hypergrad = -sum([(g1 * g2).sum() for g1, g2 in zip(jax.tree_util.tree_leaves(grads), jax.tree_util.tree_leaves(prev_grads))])\n",
        "            tstate.opt_state.hyperparams['learning_rate'] -= hypergrad_lr * hypergrad\n",
        "        prev_grads = grads\n",
        "\n",
        "        # update all the stats\n",
        "        s = {}\n",
        "        s['timestamp'] = perf_counter() - t0\n",
        "        s['loss'] = loss\n",
        "        s['lr'] = tstate.opt_state.hyperparams['learning_rate'].item()\n",
        "        if t % EVAL_EVERY == 0:\n",
        "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
        "            n = 0\n",
        "            for batch in test_ds.as_numpy_iterator():\n",
        "                loss, acc = eval(tstate, batch)\n",
        "                s['eval_loss'] += loss\n",
        "                s['eval_acc'] += acc\n",
        "                n += 1\n",
        "            s['eval_loss'] /= n\n",
        "            s['eval_acc'] /= n\n",
        "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
        "        try:\n",
        "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
        "            s['bleu'] = wmt.bleu(tstate, test_ds, tokenizer=args['tokenizer'])   # calculate bleu score if WMT task\n",
        "            print(s['bleu'][1])\n",
        "        except Exception as e:\n",
        "          print('uh oh', BLEU_EVERY, e)\n",
        "        stats[t] = s\n",
        "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
        "\n",
        "    stats['model_params'] = deepcopy(tstate.params)\n",
        "    return dict(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "125998da-b7ed-4d07-af09-d2020a813e0a",
      "metadata": {
        "id": "125998da-b7ed-4d07-af09-d2020a813e0a"
      },
      "source": [
        "# Run\n",
        "Select the hyperparameters and the seeds to use for each trial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
        "outputId": "d6fafbb3-b362-4f84-d304-aff7c908bc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset: WMT_1-13\n",
            "using gpu for jax\n",
            "saving data at `drive/My Drive/meta-opt/data/`\n"
          ]
        }
      ],
      "source": [
        "# hyperparams\n",
        "SEEDS = [48,]\n",
        "# SEEDS = [62, 23, 37]\n",
        "# SEEDS = [48, 62, 23, 37]\n",
        "NUM_ITERS = 24000\n",
        "NUM_EVAL_ITERS = 100\n",
        "EVAL_EVERY = 500\n",
        "BLEU_EVERY = 2000  # only for WMT\n",
        "BATCH_SIZE = 32\n",
        "RESET_EVERY = 24000\n",
        "PRINT_EVERY = int(1e10)\n",
        "\n",
        "NAME = 'WMT_1-13'\n",
        "if 'DIR_PREFIX' not in globals(): DIR_PREFIX = '.'  # use this directory if unspecified\n",
        "\n",
        "from jax.lib import xla_bridge\n",
        "print('dataset:', NAME)\n",
        "print('using', xla_bridge.get_backend().platform, 'for jax')\n",
        "print(f'saving data at `{DIR_PREFIX}/data/`')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _rsqrt_schedule(\n",
        "    init_value: float,\n",
        "    shift: int = 0,\n",
        "):\n",
        "  \"\"\"Applies a reverse square-root schedule.\n",
        "\n",
        "  The reverse square root schedule is simply `lr = init_value / sqrt(step)`.\n",
        "\n",
        "  Args:\n",
        "    init_value: Base learning rate (before applying the rsqrt schedule).\n",
        "    shift: How many steps the rsqrt should be shifted. Shifting the rsqrt\n",
        "      schedule makes it less steep in the beginning (close to 0).\n",
        "\n",
        "  Returns:\n",
        "    A schedule `count -> learning_rate`.\n",
        "  \"\"\"\n",
        "\n",
        "  def schedule(count):\n",
        "    return init_value * (count + shift) ** -0.5 * shift**0.5\n",
        "\n",
        "  return schedule\n",
        "\n",
        "\n",
        "def create_learning_rate_schedule(learning_rate: float, warmup_steps: int):\n",
        "  \"\"\"Creates a rsqrt schedule with linear warmup.\"\"\"\n",
        "  return optax.join_schedules(\n",
        "      [\n",
        "          optax.linear_schedule(\n",
        "              init_value=0,\n",
        "              end_value=learning_rate,\n",
        "              transition_steps=warmup_steps,\n",
        "          ),\n",
        "          _rsqrt_schedule(init_value=learning_rate, shift=warmup_steps),\n",
        "      ],\n",
        "      boundaries=[warmup_steps],\n",
        "  )\n",
        "rsqrt_schedule = jax.tree_util.Partial(create_learning_rate_schedule, warmup_steps=1000)"
      ],
      "metadata": {
        "id": "MJc8EF8FuFgZ"
      },
      "id": "MJc8EF8FuFgZ",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4b5b25-2f17-4c6a-afaa-88f904c1c98e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4b5b25-2f17-4c6a-afaa-88f904c1c98e",
        "outputId": "f088dddc-a71a-4a86-ae7e-58f81739500c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/24000 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# uncomment the ones to run\n",
        "results = defaultdict(list)\n",
        "# filename = f'{DIR_PREFIX}/data/{NAME}_raw.pkl'; results = pkl.load(open(filename, 'rb')); print(f'loaded checkpoint from {filename}, containing {list(results.keys())}')\n",
        "\n",
        "for s in SEEDS:\n",
        "    # # standard benchmarks\n",
        "    # results['sgd_0.1'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.sgd)(learning_rate=0.1)))  # (CIFAR, 0.1)\n",
        "    # # results['sgd_0.04'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.sgd)(learning_rate=0.04)))\n",
        "    # # results['sgd_0.4'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.sgd)(learning_rate=0.4)))\n",
        "    # results['momentum_0.01'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.sgd)(learning_rate=0.01, momentum=0.9)))  # (CIFAR, 0.01)\n",
        "    # # results['momentum_0.001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.sgd)(learning_rate=0.001, momentum=0.9)))\n",
        "    # results['adam_0.001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adam)(learning_rate=1e-3)))  # (CIFAR, 0.001)\n",
        "    results['rsqrt_0.001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adamw)(learning_rate=rsqrt_schedule(0.001), b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-5)))\n",
        "    # results['adamw_0.001_1e-2'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adamw)(learning_rate=1e-3, weight_decay=1e-2)))\n",
        "    # results['adamw_0.001_1e-3'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adamw)(learning_rate=1e-3, weight_decay=1e-3)))\n",
        "    # # results['adam_0.0001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adam)(learning_rate=1e-4)))\n",
        "    # results['rmsprop_0.001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.rmsprop)(learning_rate=0.001)))  # (CIFAR, 0.001)\n",
        "    # # results['hgd_0.1'].append(train_hgd(s, NAME, initial_lr=0.1, hypergrad_lr=1e-5))\n",
        "\n",
        "    # # scalar\n",
        "    # results['scalar_0.0004'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.0004, H=64, HH=2, initial_lr=0.1, use_adam=False))\n",
        "    # # results['scalar_0.00004'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.00004, H=64, HH=2, initial_lr=0.01, use_adam=False))\n",
        "    # results['scalar_adam_0.0001'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.0001, H=64, HH=2, initial_lr=0.01, use_adam=True))  # (CIFAR, <=0.0001)\n",
        "    # results['scalar_adam_0.00004'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.00004, H=64, HH=2, initial_lr=0.01, use_adam=True))\n",
        "    # results['scalar_adam_0.0001_0.0'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.0001, H=64, HH=2, initial_lr=0.0, use_adam=True))\n",
        "\n",
        "    # # diagonal\n",
        "    # # results['diagonal_1.0'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=1.0, H=36, HH=2, initial_lr=0.01, use_adam=False))\n",
        "    # # results['diagonal_adam_0.0002'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=0.0002, H=36, HH=2, initial_lr=0.01, use_adam=True))\n",
        "\n",
        "    # # # the caltech paper\n",
        "    # # results['meta_GAPS'].append(train_gaps_meta_opt(s, NAME, 'scalar', meta_lr=0.001, H=6, B=6, initial_lr=0.2, use_adam=False))\n",
        "\n",
        "    if len(results) > 0:\n",
        "        filename = f'{DIR_PREFIX}/data/{NAME}_raw.pkl'\n",
        "        with open(filename, 'wb') as f:\n",
        "            pkl.dump(results, f)\n",
        "            print(f'Saved checkpoint for seed #{s} to {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335f1b24-5846-45f3-b20b-ddcd89bf8fe5",
      "metadata": {
        "id": "335f1b24-5846-45f3-b20b-ddcd89bf8fe5"
      },
      "outputs": [],
      "source": [
        "# clean the stats\n",
        "to_del = []\n",
        "for k, v in results.items():\n",
        "    if len(v) == 0: to_del.append(k)\n",
        "for k in to_del: del results[k]\n",
        "\n",
        "aggregated = {}  # experiment name -> 'args' or timestamp -> stat key -> stat value\n",
        "# gather stats\n",
        "for k, v in results.items():  # for each experiment\n",
        "    aggregated[k] = {'args': []}\n",
        "\n",
        "    for n in range(len(v)):  # for each trial\n",
        "        aggregated[k]['args'].append(v[n]['args'])\n",
        "\n",
        "        for t in range(1, v[0]['args']['num_iters'] + 1):  # for each timestamp\n",
        "            for stat_key, value in v[n][t].items():  # for each stat recorded at that timestamp\n",
        "                if stat_key not in aggregated[k]: aggregated[k][stat_key] = {}\n",
        "                if t not in aggregated[k][stat_key]: aggregated[k][stat_key][t] = []\n",
        "                aggregated[k][stat_key][t].append(value)\n",
        "\n",
        "# aggregate stats\n",
        "ret = defaultdict(dict)  # stat key -> experiment name -> 't' or 'avg' or 'std' ->\n",
        "args = {}\n",
        "for k, v in aggregated.items():  # for experiment\n",
        "    for stat_key in v.keys():  # for stat\n",
        "        if stat_key in ['args', 'model_params', 'controller_params']:\n",
        "            args[k] = v[stat_key]\n",
        "            continue\n",
        "        if k not in ret[stat_key]: ret[stat_key][k] = {}\n",
        "        ret[stat_key][k]['t'] = list(v[stat_key].keys())\n",
        "        arr = np.array(list(v[stat_key].values()))\n",
        "        ret[stat_key][k]['avg'] = np.mean(arr, axis=1)\n",
        "        ret[stat_key][k]['std'] = np.std(arr, axis=1)\n",
        "\n",
        "with open(f'{DIR_PREFIX}/data/{NAME}_processed.pkl', 'wb') as f:\n",
        "    pkl.dump(ret, f)\n",
        "    print('Saved processed results')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf",
      "metadata": {
        "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kkj0hJUA5gkp",
      "metadata": {
        "id": "kkj0hJUA5gkp"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------\n",
        "# plot a particular set of experiments\n",
        "# ----------------------------------------\n",
        "keys_to_plot = [\n",
        "    'sgd_0.1',\n",
        "    'momentum_0.01',\n",
        "    'adam_0.001',\n",
        "    # 'adamw_0.001',\n",
        "    # 'adamw_0.001_1e-2',\n",
        "    # 'adamw_0.001_1e-3',\n",
        "    'rmsprop_0.001',\n",
        "    'scalar_adam_0.0001',\n",
        "    'scalar_adam_0.00004',\n",
        "    ]\n",
        "\n",
        "# ----------------------------------------\n",
        "# OR just plot em all\n",
        "# ----------------------------------------\n",
        "# keys_to_plot = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd",
      "metadata": {
        "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "fig, ax = plt.subplots(len(ret), 1, figsize=(10, 24))\n",
        "Ms = {}\n",
        "\n",
        "for i, stat_key in enumerate(ret.keys()):\n",
        "    ax[i].set_title(stat_key)\n",
        "    for experiment_name in ret[stat_key].keys():\n",
        "        if keys_to_plot is not None and experiment_name not in keys_to_plot: continue\n",
        "        ts, avgs, stds = ret[stat_key][experiment_name]['t'], ret[stat_key][experiment_name]['avg'], ret[stat_key][experiment_name]['std']\n",
        "        if avgs.ndim == 2:\n",
        "            Ms[experiment_name] = avgs\n",
        "            ax[i].plot(ts, avgs.sum(axis=-1), label=experiment_name)\n",
        "            stds = ((stds ** 2).sum(axis=-1)) ** 0.5\n",
        "            ax[i].fill_between(ts, avgs.sum(axis=-1) - 1.96 * stds, avgs.sum(axis=-1) + 1.96 * stds, alpha=0.2)\n",
        "            # for j in range(avgs.shape[1]):\n",
        "            #     ax[i].plot(ts, avgs[:, j], label=f'{experiment_name} {str(j)}')\n",
        "            #     ax[i].fill_between(ts, avgs[:, j] - 1.96 * stds[:, j], avgs[:, j] + 1.96 * stds[:, j], alpha=0.2)\n",
        "        else:\n",
        "            if stat_key in ['loss', 'grad_sq_norm']:\n",
        "                n = 20\n",
        "                kernel = np.array([1 / n,] * n)\n",
        "                avgs = np.convolve(avgs, kernel)[n // 2:n // 2 + avgs.shape[0]]\n",
        "                stds = np.convolve(stds ** 2, kernel ** 2)[n // 2:n // 2 + stds.shape[0]] ** 0.5\n",
        "            ax[i].plot(ts, avgs, label=experiment_name)\n",
        "            ax[i].fill_between(ts, avgs - 1.96 * stds, avgs + 1.96 * stds, alpha=0.2)\n",
        "    ax[i].legend()\n",
        "\n",
        "\n",
        "ax[1].set_ylim(-0.1, 2.5)\n",
        "# ax[2].set_ylim(-0.1, 0.7)\n",
        "ax[3].set_ylim(0.5, 0.9)\n",
        "# ax[4].set_ylim(-0.1, 40)\n",
        "# ax[5].set_ylim(-0.05, 0.05)\n",
        "# plt.savefig(f'{DIR_PREFIX}/figs/{NAME}.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTMpnF5ybSr2"
      },
      "source": [
        "# Animate"
      ],
      "id": "LTMpnF5ybSr2"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "for v in Ms.values(): assert v.shape == list(Ms.values())[0].shape\n",
        "\n",
        "downsample_factor = 100\n",
        "T, H = v.shape\n",
        "ymin, ymax = -0.012, 0.012\n",
        "\n",
        "fig = plt.figure()  # initializing a figure in which the graph will be plotted\n",
        "ax = plt.axes(xlim =(0, H), ylim=(ymin, ymax))  # marking the x-axis and y-axis\n",
        "ax.set_xlabel('number of steps in the past')\n",
        "ax.set_ylabel('M coefficient')\n",
        "\n",
        "# initializing a line variable\n",
        "ls = {}\n",
        "for k in Ms.keys():\n",
        "  ls[k], = ax.plot([], [], lw = 3, label=k)\n",
        "legend = ax.legend()\n",
        "\n",
        "# data which the line will contain (x, y)\n",
        "def init():\n",
        "  for l in ls.values(): l.set_data([], [])\n",
        "  return list(ls.values())\n",
        "\n",
        "def animate(i):\n",
        "    for k, M in Ms.items():\n",
        "      x, y = range(0, H), M[i * downsample_factor]\n",
        "      ls[k].set_data(x, y[::-1])\n",
        "      # line.set_label(i)\n",
        "    # legend.get_texts()[0].set_text(i * downsample_factor) #Update label each at frame\n",
        "    ax.set_title(f'timestep #{i * downsample_factor} of meta-opt on {}')\n",
        "    return list(ls.values())\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func = init,\n",
        "                     frames = T // downsample_factor, interval = downsample_factor, blit = True)\n",
        "plt.close()\n",
        "h = HTML(anim.to_html5_video())\n",
        "display(h)"
      ],
      "metadata": {
        "id": "DA5o-cJ1MpL5"
      },
      "id": "DA5o-cJ1MpL5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "meta-opt",
      "language": "python",
      "name": "meta-opt"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}