{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03381178-fb38-48e9-88a4-ee54e642b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "from typing import List, Tuple\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import struct\n",
    "import tensorflow as tf\n",
    "\n",
    "from controllers._base import ControllerState\n",
    "from controllers.utils import append\n",
    "\n",
    "from training.trainer import TrainState, reset_model, create_train_state, forward, forward_and_backward, apply_gradients\n",
    "from training.hgd import HGDState, hypergrad_step\n",
    "from training.utils import cross_entropy, mse, load_mnist, MLP, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a2343-561c-4272-b211-c92820c5b4e7",
   "metadata": {},
   "source": [
    "## Things we keep track of\n",
    "- states (params at current iterations)\n",
    "- disturbances (grads at previous iterations)\n",
    "- evolve functions (based on lrs and cost fns at current iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16ad318a-091b-4986-ae08-2163b8b37300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(params, lr, cost_fn):\n",
    "    \"\"\"Gradient descent. \"\"\"\n",
    "    cost, grad = jax.value_and_grad(cost_fn)(params)\n",
    "    new_params = jax.tree_map(lambda p, g: p - lr * g, params, grad)\n",
    "    return (new_params, grad, cost,)\n",
    "\n",
    "# @jax.jit\n",
    "def slice_pytree(pytree, start_idx, slice_size):\n",
    "    return jax.tree_map(lambda p: jax.lax.dynamic_slice_in_dim(p, start_idx, slice_size), pytree)\n",
    "# slice_pytree = jax.jit(slice_pytree, static_argnums=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7289cfae-f627-4851-8ad3-daadeb12c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaOptGPCState(ControllerState):\n",
    "    M: jnp.ndarray  # pytree of disturbance-feedback control matrices\n",
    "    \n",
    "    H: int = struct.field(pytree_node=False)  # history of the controller, how many past disturbances to use for control\n",
    "    HH: int = struct.field(pytree_node=False)  # history of the system, how many hallucination steps to take\n",
    "    lr: float\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, \n",
    "               params,\n",
    "               m_method: str,\n",
    "               H: int,\n",
    "               HH: int,\n",
    "               lr: float = 0.008,):\n",
    "\n",
    "        def make_m(p):\n",
    "            if m_method == 'scalar': shape = (1,) * p.ndim \n",
    "            elif m_method == 'diagonal': shape = p.shape\n",
    "            else: raise NotImplementedError(m_method)\n",
    "            return jnp.zeros((H, *shape))\n",
    "        \n",
    "        M = jax.tree_map(make_m, params)\n",
    "        tx = optax.sgd(learning_rate=lr)  # M optimizer\n",
    "        opt_state = tx.init(M)\n",
    "        \n",
    "        return cls(M=M,\n",
    "                   H=H, HH=HH, \n",
    "                   lr=lr, tx=tx, opt_state=opt_state)\n",
    "\n",
    "@jax.jit\n",
    "def compute_control(M, disturbances):\n",
    "    control = jax.tree_map(lambda m, d: (m * d).sum(axis=0), M, disturbances)\n",
    "    # control = (M * disturbances).sum(axis=0)\n",
    "    return control\n",
    "\n",
    "def _compute_loss(M, H, HH, initial_params, \n",
    "                  disturbances,  # past H + HH disturbances\n",
    "                  evolve_fns,  # past HH evolve functions, starting at the one that would have been used to evolve `initial_params`\n",
    "                  cost_fn):\n",
    "    params = initial_params\n",
    "    for h in range(HH):\n",
    "        params = jax.tree_map(lambda p, c: p + c, evolve_fns[h](params), compute_control(M, slice_pytree(disturbances, h, H)))\n",
    "    loss = cost_fn(params)\n",
    "    return loss\n",
    "\n",
    "_grad_fn = jax.grad(_compute_loss, (0,))\n",
    "\n",
    "# @jax.jit\n",
    "def update(cstate: MetaOptGPCState,\n",
    "           initial_params,  # params from HH steps ago\n",
    "           disturbances,  # past H + HH disturbances\n",
    "           evolve_fns,  # past HH evolve functions, starting at the one that would have been used to evolve `initial_params`\n",
    "           cost_fn\n",
    "          ):\n",
    "    \n",
    "    grads = _grad_fn(cstate.M, cstate.H, cstate.HH, initial_params, disturbances, evolve_fns, cost_fn)\n",
    "    updates, new_opt_state = cstate.tx.update(grads, cstate.opt_state, cstate.M)\n",
    "    M = optax.apply_updates(cstate.M, updates[0])\n",
    "    cstate = cstate.replace(M=M, opt_state=new_opt_state)   \n",
    "    return cstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91f28c76-cb2e-425c-9fe0-54a50c6a0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaOpt:\n",
    "    param_history: Tuple\n",
    "    grad_history: jnp.ndarray\n",
    "    evolve_fn_history: Tuple\n",
    "    cstate: MetaOptGPCState\n",
    "    delta: float\n",
    "    t: int\n",
    "\n",
    "    def __init__(self,\n",
    "                 initial_params,\n",
    "                 H: int, HH: int,\n",
    "                 meta_lr: float, delta: float,\n",
    "                 m_method: str):\n",
    "        self.param_history = (None,) * HH\n",
    "        self.grad_history = jax.tree_map(lambda p: jnp.zeros((H + HH, *p.shape)), initial_params)\n",
    "        self.evolve_fn_history = (None,) * HH\n",
    "        self.delta = delta\n",
    "        self.t = 0\n",
    "\n",
    "        assert m_method in ['scalar', 'diagonal']\n",
    "        self.cstate = MetaOptGPCState.create(initial_params, m_method, H, HH, lr=meta_lr)\n",
    "        pass\n",
    "\n",
    "    def meta_step(self, \n",
    "                  params,  # params after a step of gd\n",
    "                  grads,  # grads from the step of gd that resulted in `params`\n",
    "                  lr, cost_fn,  # lr and cost fn from step of gd that resulted in `params`\n",
    "                 ):        \n",
    "        if self.t >= self.cstate.H + self.cstate.HH:\n",
    "            control = compute_control(self.cstate.M, slice_pytree(self.grad_history, self.cstate.HH, self.cstate.H))  # use past H disturbances\n",
    "            params = jax.tree_map(lambda p, c: (1 - self.delta) * p + c, params, control)\n",
    "            self.cstate = update(self.cstate, self.param_history[0], self.grad_history, self.evolve_fn_history, jax.tree_util.Partial(cost_fn))\n",
    "\n",
    "        self.param_history = append(self.param_history, params)\n",
    "        self.grad_history = jax.tree_map(lambda h, g: append(h, g), self.grad_history, grads)\n",
    "\n",
    "        self.evolve_fn_history = append(self.evolve_fn_history, jax.tree_util.Partial(lambda p: gd(p, lr, cost_fn)[0]))\n",
    "        self.t += 1\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61959a46-aca6-4633-8284-4f038e8f654f",
   "metadata": {},
   "source": [
    "# Run Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a179a5e-3b25-45b1-a463-c46a404a49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(seed, dim, use_meta_opt):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    def f(x): return jnp.abs(x) ** 0.5 * jnp.sign(x)\n",
    "    # def f(x): return x\n",
    "\n",
    "    num_iters, batch_size = 500, 256\n",
    "    \n",
    "    inputs = jnp.array(np.random.randn(num_iters, batch_size, dim))\n",
    "    model = MLP([dim, dim, dim, dim])\n",
    "    tstate = create_train_state(jax.random.PRNGKey(seed), model, [dim,], optax.sgd(learning_rate=0., momentum=0.), None)\n",
    "    params = tstate.params\n",
    "    lr = 0.01\n",
    "\n",
    "    if use_meta_opt: metaopt = MetaOpt(params, H=3, HH=1, meta_lr=20., delta=0., m_method='diagonal')\n",
    "\n",
    "    losses = []\n",
    "    for i in tqdm.trange(num_iters):\n",
    "        x = inputs[i]\n",
    "        cost_fn = jax.tree_util.Partial(lambda p: ((tstate.apply_fn({'params': p}, x) - f(x)) ** 2).mean())\n",
    "        params, grad, loss = gd(params, lr, cost_fn)\n",
    "        losses.append(loss)\n",
    "        if use_meta_opt: params = metaopt.meta_step(params, grad, lr, cost_fn)\n",
    "    return losses            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64404183-fc05-4bfc-a51c-3da017d9d130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████████▍                                                                  | 146/500 [00:02<00:06, 52.40it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed, dim = 2, 500\n",
    "losses = run(seed, dim, False)\n",
    "# mo_losses = run(seed, dim, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676d340-6a7a-4572-b512-4a06081d1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses)), losses)\n",
    "plt.plot(range(len(mo_losses)), mo_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695058c-55f4-466a-9324-37e75d183865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
