{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d209b7-b77e-4151-9ec3-964460bedee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.profiler.start_trace(\"/tmp/tensorboard\")\n",
    "\n",
    "x = jax.numpy.ones((int(1e8),))\n",
    "x = (2 * x) ** 2\n",
    "x.block_until_ready()\n",
    "\n",
    "jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe29c5-bdcd-468c-9c4e-ffac17fc99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorboard-plugin-profile\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir /tmp/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
    "outputId": "5af8d008-1034-4d1d-bc78-58a0d6ffc6e6"
   },
   "outputs": [],
   "source": [
    "# handle the system stuff, colab stuff, etc\n",
    "import os\n",
    "try:\n",
    "    from google import colab  # for use in google colab!!\n",
    "    !git clone https://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@github.com/edogariu/meta-opt\n",
    "    !pip install -q ./meta-opt\n",
    "    !pip install -q dill\n",
    "    # !pip install -q jax[cuda12_pip]==0.4.20 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html  # for disabling prealloc, see https://github.com/google/jax/discussions/19014\n",
    "    # !pip install -q tensorflow-text ml_collections clu sentencepiece  # for WMT\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DIR = os.path.abspath(\"./drive/My Drive/meta-opt\")\n",
    "except: \n",
    "    DIR = os.path.abspath(\".\")\n",
    "assert os.path.isdir(DIR)\n",
    "\n",
    "# make sure we have the necessary folders\n",
    "for subdir in ['data', 'figs', 'datasets']: \n",
    "    temp = os.path.join(DIR, subdir)\n",
    "    if not os.path.isdir(temp): os.mkdir(temp)\n",
    "\n",
    "# # for the one-time colab setup\n",
    "# !git clone https://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@github.com/edogariu/meta-opt\n",
    "# !cp -r \"meta-opt\" \"drive/My Drive/\"\n",
    "# !pip install kora -q  # library from https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab to help get ID\n",
    "# from kora.xattr import get_id\n",
    "# fid = get_id(f\"{dir_prefix}meta_opt.ipynb\")\n",
    "# print(\"https://colab.research.google.com/drive/\"+fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b898f1e-f002-435a-b1b4-65a21e05a8a7",
   "metadata": {
    "id": "7b898f1e-f002-435a-b1b4-65a21e05a8a7"
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf; tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as jnn\n",
    "import optax\n",
    "\n",
    "from meta_opt.nn.trainer import create_train_state, reset_model, train_step, eval\n",
    "from meta_opt.problems import mnist, cifar10#, wmt  # WMT is a bit broken atm\n",
    "\n",
    "from meta_opt.meta_opt import MetaOpt\n",
    "from meta_opt.gaps import MetaOptGAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12",
   "metadata": {
    "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint()\n",
    "        print('seed set to {}'.format(seed))\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    return rng, seed\n",
    "\n",
    "def get_opt_hyperparams(opt_state):\n",
    "    \"\"\"\n",
    "    helper fn to serialize optax optimizer hyperparameters from the opt_state\n",
    "    \"\"\"\n",
    "    if isinstance(opt_state, Tuple): h = [deepcopy(o.hyperparams) for o in opt_state if hasattr(o, 'hyperparams')]\n",
    "    else: h = deepcopy(opt_state.hyperparams)\n",
    "    return h\n",
    "\n",
    "\n",
    "def get_problem(seed, name, optimizer):\n",
    "    rng, seed = set_seed(seed)\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "\n",
    "    # get dataset and model\n",
    "    if 'MNIST' in name:\n",
    "        train_ds, test_ds, example_input, loss_fn, metric_fns = mnist.load_mnist(NUM_ITERS, BATCH_SIZE, num_eval_iters=NUM_EVAL_ITERS, dataset_dir=os.path.join(DIR, 'datasets'))\n",
    "        model = mnist.MLP([28 * 28, 100, 100, 10])\n",
    "    elif 'CIFAR' in name:\n",
    "        train_ds, test_ds, example_input, loss_fn, metric_fns = cifar10.load_cifar10(NUM_ITERS, BATCH_SIZE, num_eval_iters=NUM_EVAL_ITERS, dataset_dir=os.path.join(DIR, 'datasets'))\n",
    "        model = cifar10.make_vgg16()\n",
    "    # elif 'WMT' in name:  # WMT is a bit broken atm\n",
    "    #     train_ds, test_ds, example_input, loss_fn, metric_fns, tokenizer = wmt.load_wmt(NUM_ITERS, BATCH_SIZE, dataset_dir=os.path.join(DIR, 'datasets'), num_eval_iters=NUM_EVAL_ITERS)\n",
    "    #     train_ds.cache()\n",
    "    #     model = wmt.make_transformer(num_heads=8, num_layers=6, emb_dim=512, qkv_dim=512, mlp_dim=2048)\n",
    "    #     # model = wmt.make_transformer(num_heads=4, num_layers=3, emb_dim=64, qkv_dim=64, mlp_dim=256)\n",
    "    #     raise NotImplementedError('gotta figure out how to keep the tokenizer around')\n",
    "    else:\n",
    "        raise NotImplementedError(name)\n",
    "\n",
    "    tstate = create_train_state(init_rng, model, example_input, optimizer, loss_fn, metric_fns=metric_fns)\n",
    "    del init_rng\n",
    "\n",
    "    args = {'seed': seed,\n",
    "            'model': str(model),\n",
    "            'params': sum(x.size for x in jax.tree_util.tree_leaves(tstate.params)),\n",
    "            'dataset': name,\n",
    "            'num_iters': NUM_ITERS,\n",
    "            'eval_every': EVAL_EVERY,\n",
    "            'num_eval_iters': NUM_EVAL_ITERS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'reset_every': RESET_EVERY,\n",
    "           }\n",
    "\n",
    "    return tstate, train_ds, test_ds, rng, args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68611b25-ff6c-498a-bac1-30873eac9b16",
   "metadata": {
    "id": "68611b25-ff6c-498a-bac1-30873eac9b16"
   },
   "source": [
    "# Standard Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35553ddc-66dd-454e-9608-8820ba402b03",
   "metadata": {
    "id": "35553ddc-66dd-454e-9608-8820ba402b03"
   },
   "outputs": [],
   "source": [
    "def train_standard_opt(seed, problem_name, optimizer):\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = get_opt_hyperparams(tstate.opt_state)\n",
    "    args['optimizer_name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = train_step(tstate, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "            \n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    stats['tstate'] = deepcopy(tstate)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f",
   "metadata": {
    "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f"
   },
   "source": [
    "# Meta-Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907792cd-7420-4e61-a458-372e2dac9128",
   "metadata": {
    "id": "907792cd-7420-4e61-a458-372e2dac9128"
   },
   "outputs": [],
   "source": [
    "def train_meta_opt(seed, problem_name: str, m_method: str, meta_optimizer, H: int, HH: int, initial_lr: int, ema_keys = [], grad_clip = 1e9):\n",
    "\n",
    "    optimizer = optax.chain(optax.add_decayed_weights(1e-5), optax.sgd(learning_rate=initial_lr))\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "    meta_opt = MetaOpt(tstate, H=H, HH=HH, m_method=m_method, meta_optimizer=meta_optimizer, ema_keys=ema_keys, grad_clip=grad_clip)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_name'] = 'meta'\n",
    "    args['optimizer_args'] = {'initial_lr': initial_lr,\n",
    "                              'm_method': m_method,\n",
    "                              'meta_optimizer_args': get_opt_hyperparams(meta_opt.cstate.opt_state),\n",
    "                              'H': H,\n",
    "                              'HH': HH,\n",
    "                              'ema_keys': ema_keys,\n",
    "                              'grad_clip': grad_clip,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            meta_opt = meta_opt.episode_reset()\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = train_step(tstate, batch)\n",
    "        tstate = meta_opt.meta_step(tstate, grads, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "\n",
    "        # log the value of the Ms\n",
    "        if m_method == 'scalar':\n",
    "            s['M'] = meta_opt.cstate.cparams['M'].reshape(-1)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = v\n",
    "        else:\n",
    "            s['M'] = jnp.stack([m.reshape((m.shape[0], -1)).mean(axis=-1) for m in jax.tree_util.tree_leaves(meta_opt.cstate.cparams['M'])], axis=0).mean(axis=0)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = jnp.stack([m.mean() for m in jax.tree_util.tree_leaves(v)], axis=0).mean(axis=0)\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), 'M': s['M'].sum()})\n",
    "\n",
    "    stats['tstate'] = deepcopy(tstate)\n",
    "    stats['cstate'] = deepcopy(meta_opt.cstate)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10045c-53d9-41f3-b559-f7a0c2fe8bf6",
   "metadata": {
    "id": "4d10045c-53d9-41f3-b559-f7a0c2fe8bf6"
   },
   "source": [
    "# Gradient-based Adaptive Policy Selection (GAPS) Meta-Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5867db6-fe41-4dbd-9934-1a52617cc94c",
   "metadata": {
    "id": "c5867db6-fe41-4dbd-9934-1a52617cc94c"
   },
   "outputs": [],
   "source": [
    "def train_gaps_meta_opt(seed, problem_name: str, m_method: str, meta_lr: float, use_adam: bool, H: int, B: int, initial_lr: int, grad_clip: float = 1.0):\n",
    "    optimizer = optax.sgd(learning_rate=initial_lr)\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "    meta_opt = MetaOptGAPS(tstate, H=H, B=B, meta_lr=meta_lr, use_adam=use_adam, delta=1e-5, m_method=m_method)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_name'] = 'gaps_meta'\n",
    "    args['optimizer_args'] = {'initial_lr': initial_lr,\n",
    "                              'm_method': m_method,\n",
    "                              'meta_lr': meta_lr,\n",
    "                              'use_adam': use_adam,\n",
    "                              'H': H,\n",
    "                              'B': B,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            meta_opt = meta_opt.episode_reset()\n",
    "            del reset_rng\n",
    "\n",
    "        # tstate, (loss, grads) = train_step(tstate, batch)  # gaps one does not use both lines\n",
    "        tstate, (loss, grads) = meta_opt.meta_step(tstate, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "\n",
    "        # log Ms\n",
    "        if m_method == 'scalar':\n",
    "            s['M'] = meta_opt.cstate.cparams['M'].reshape(-1)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = v\n",
    "        else:\n",
    "            s['M'] = jnp.stack([m.reshape((m.shape[0], -1)).mean(axis=-1) for m in jax.tree_util.tree_leaves(meta_opt.cstate.cparams['M'])], axis=0).mean(axis=0)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = jnp.stack([m.mean() for m in jax.tree_util.tree_leaves(v)], axis=0).mean(axis=0)\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), 'M': s['M'].sum()})\n",
    "\n",
    "    stats['tstate'] = deepcopy(tstate)\n",
    "    stats['cstate'] = deepcopy(meta_opt.cstate)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada709b3-489a-44e8-bba2-cfd9221f0988",
   "metadata": {
    "id": "ada709b3-489a-44e8-bba2-cfd9221f0988"
   },
   "source": [
    "# Hypergradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3",
   "metadata": {
    "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3"
   },
   "outputs": [],
   "source": [
    "def train_hgd(seed, problem_name: str, initial_lr: float, hypergrad_lr: float):\n",
    "\n",
    "    optimizer = optax.inject_hyperparams(optax.sgd)(learning_rate=initial_lr)\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_name'] = 'hgd'\n",
    "    args['optimizer_args'] = {'initial_lr': initial_lr,\n",
    "                              'hypergrad_lr': hypergrad_lr,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    prev_grads = None\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = train_step(tstate, batch)\n",
    "        if prev_grads is not None:\n",
    "            hypergrad = -sum([(g1 * g2).sum() for g1, g2 in zip(jax.tree_util.tree_leaves(grads), jax.tree_util.tree_leaves(prev_grads))])\n",
    "            tstate.opt_state.hyperparams['learning_rate'] -= hypergrad_lr * hypergrad\n",
    "        else: hypergrad = 0.\n",
    "        prev_grads = grads\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "        s['hypergrad'] = hypergrad\n",
    "        s['lr'] = float(tstate.opt_state.hyperparams['learning_rate'])\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    stats['tstate'] = deepcopy(tstate)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125998da-b7ed-4d07-af09-d2020a813e0a",
   "metadata": {
    "id": "125998da-b7ed-4d07-af09-d2020a813e0a"
   },
   "source": [
    "# Run\n",
    "Select the hyperparameters and the seeds to use for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
    "outputId": "ef881563-f7b8-483c-b35a-523921e95e13"
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "SEEDS = [4,]\n",
    "NUM_ITERS = 1000\n",
    "EVAL_EVERY = 50\n",
    "NUM_EVAL_ITERS = -1\n",
    "BATCH_SIZE = 128\n",
    "RESET_EVERY = 1000\n",
    "NAME = 'CIFAR_silly'\n",
    "\n",
    "print('using', jax.lib.xla_bridge.get_backend().platform, 'for jax')\n",
    "print(f'results will be stored at: {DIR}/data/{NAME}_*.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db68d9-62c2-4848-9d47-92c9aa56b251",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5db68d9-62c2-4848-9d47-92c9aa56b251",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "6c427465-7c82-45d4-b918-7f3d322f4ca4"
   },
   "outputs": [],
   "source": [
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "\n",
    "# uncomment the ones to run\n",
    "results = defaultdict(list)\n",
    "# filename = f'{DIR_PREFIX}/data/{NAME}_raw.pkl'; results = pkl.load(open(filename, 'rb')); print(f'loaded checkpoint from {filename}, containing {list(results.keys())}')\n",
    "\n",
    "for s in SEEDS:\n",
    "    # ours\n",
    "    meta_opt = optax.inject_hyperparams(optax.sgd)(learning_rate=1e-5)\n",
    "    adam_meta_opt = optax.inject_hyperparams(optax.adam)(learning_rate=1e-3, b1=0.9, b2=0.999)\n",
    "    # results['scalar'].append(train_meta_opt(s, NAME, 'scalar', H=64, HH=2, initial_lr=0.001, meta_optimizer=meta_opt))\n",
    "    # results['diagonal'].append(train_meta_opt(s, NAME, 'diagonal', H=32, HH=2, initial_lr=0.001, meta_optimizer=meta_opt))\n",
    "    # results['scalar_adam'].append(train_meta_opt(s, NAME, 'scalar', H=32, HH=2, initial_lr=0.1, meta_optimizer=adam_meta_opt))\n",
    "    # results['diagonal_adam'].append(train_meta_opt(s, NAME, 'diagonal', H=32, HH=2, initial_lr=0.001, meta_optimizer=adam)meta_opt))\n",
    "\n",
    "    # standard benchmarks\n",
    "    benchmarks = {\n",
    "        'sgd': optax.inject_hyperparams(optax.sgd)(learning_rate=0.1),\n",
    "        # 'sgd_wd': optax.chain(optax.add_decayed_weights(1e-5), optax.inject_hyperparams(optax.sgd)(learning_rate=0.1)),\n",
    "        # 'momentum': optax.inject_hyperparams(optax.sgd)(learning_rate=0.01, momentum=0.9),\n",
    "        # 'adam': optax.inject_hyperparams(optax.adam)(learning_rate=1e-3),\n",
    "        # 'adamw': optax.inject_hyperparams(optax.adamw)(learning_rate=1e-3, b1=0.9, b2=0.999, weight_decay=1e-5),\n",
    "        # 'rmsprop': optax.inject_hyperparams(optax.rmsprop)(learning_rate=1e-3),\n",
    "        # 'rsqrt': optax.inject_hyperparams(optax.adamw)(learning_rate=wmt.rsqrt_lr_schedule(0.001, 1000), b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-5),  # lr schedule for WMT transformer\n",
    "    }\n",
    "    for k, opt in benchmarks.items(): results[k].append(train_standard_opt(s, NAME, opt))\n",
    "\n",
    "    # other\n",
    "    # results['hgd'].append(train_hgd(s, NAME, initial_lr=0.1, hypergrad_lr=1e-4))\n",
    "    # results['meta_GAPS'].append(train_gaps_meta_opt(s, NAME, 'scalar', meta_lr=0.001, H=6, B=6, initial_lr=0.2, use_adam=False))  # the caltech paper\n",
    "\n",
    "    if len(results) > 0:\n",
    "        filename = f'{DIR}/data/{NAME}_raw.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pkl.dump(results, f)\n",
    "            print(f'Saved checkpoint for seed {s} to {filename}')\n",
    "\n",
    "try:  # to kill the colab after running the experiment so i can leave it overnight without going bankrupt :)\n",
    "    from google.colab import runtime\n",
    "    runtime.unassign()  \n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f1b24-5846-45f3-b20b-ddcd89bf8fe5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "335f1b24-5846-45f3-b20b-ddcd89bf8fe5",
    "outputId": "79a6e7ee-0b5a-4dad-d667-f96fbf3a689a"
   },
   "outputs": [],
   "source": [
    "# clean the stats\n",
    "to_del = []\n",
    "for k, v in results.items(): \n",
    "    if len(v) == 0: to_del.append(k)\n",
    "for k in to_del: del results[k]\n",
    "\n",
    "# gather stats\n",
    "aggregated = {}  # experiment name -> 'args' or timestamp -> stat key -> stat value\n",
    "for k, v in results.items():  # for each experiment\n",
    "    aggregated[k] = {'args': []}\n",
    "\n",
    "    for n in range(len(v)):  # for each trial\n",
    "        aggregated[k]['args'].append(v[n]['args'])\n",
    "\n",
    "        for t in range(1, v[0]['args']['num_iters'] + 1):  # for each timestamp\n",
    "            if t not in v[n]: continue\n",
    "            for stat_key, value in v[n][t].items():  # for each stat recorded at that timestamp\n",
    "                if stat_key not in aggregated[k]: aggregated[k][stat_key] = {}\n",
    "                if t not in aggregated[k][stat_key]: aggregated[k][stat_key][t] = []\n",
    "                aggregated[k][stat_key][t].append(value)\n",
    "\n",
    "# aggregate stats\n",
    "ret = defaultdict(dict)  # stat key -> experiment name -> 't' or 'avg' or 'std' ->\n",
    "args = {}\n",
    "for k, v in aggregated.items():  # for experiment\n",
    "    for stat_key in v.keys():  # for stat\n",
    "        if stat_key in ['args', 'bleu']:\n",
    "            args[k] = v[stat_key]\n",
    "            continue\n",
    "        if k not in ret[stat_key]: ret[stat_key][k] = {}\n",
    "        ret[stat_key][k]['t'] = list(v[stat_key].keys())\n",
    "        arr = np.array(list(v[stat_key].values()))\n",
    "        ret[stat_key][k]['avg'] = np.mean(arr, axis=1)\n",
    "        ret[stat_key][k]['std'] = np.std(arr, axis=1)\n",
    "\n",
    "with open(f'{DIR}/data/{NAME}_processed.pkl', 'wb') as f:\n",
    "    pkl.dump(ret, f)\n",
    "    print('Saved processed results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf",
   "metadata": {
    "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkj0hJUA5gkp",
   "metadata": {
    "id": "kkj0hJUA5gkp"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# plot a particular set of experiments\n",
    "# ----------------------------------------\n",
    "# keys_to_plot = [\n",
    "#     'sgd_wd',\n",
    "#     'momentum',\n",
    "#     # 'adam_0.001',\n",
    "#     # 'rmsprop_0.001',\n",
    "#     'scalar_adam_0.001_initial',\n",
    "#     # 'scalar_0.00004'\n",
    "#     # 'scalar_ema',\n",
    "#     # 'diagonal_short',\n",
    "#     # 'diagonal_ema',\n",
    "#     ]\n",
    "\n",
    "# ----------------------------------------\n",
    "# OR just plot em all\n",
    "# ----------------------------------------\n",
    "keys_to_plot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd",
    "outputId": "c8f7de49-af69-4b34-b75c-b02b3b260c08"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(len(ret), 1, figsize=(10, 24))\n",
    "Ms = {}\n",
    "\n",
    "for i, stat_key in enumerate(ret.keys()):\n",
    "    ax[i].set_title(stat_key)\n",
    "    for experiment_name in ret[stat_key].keys():\n",
    "        if keys_to_plot is not None and experiment_name not in keys_to_plot: continue\n",
    "        ts, avgs, stds = ret[stat_key][experiment_name]['t'], ret[stat_key][experiment_name]['avg'], ret[stat_key][experiment_name]['std']\n",
    "        if avgs.ndim == 2:  # how to handle stats that are vectors (such as the Ms for scalar meta-opt)\n",
    "            Ms[experiment_name] = avgs\n",
    "            ax[i].plot(ts, avgs.sum(axis=-1), label=experiment_name)\n",
    "            stds = ((stds ** 2).sum(axis=-1)) ** 0.5\n",
    "            ax[i].fill_between(ts, avgs.sum(axis=-1) - 1.96 * stds, avgs.sum(axis=-1) + 1.96 * stds, alpha=0.2)\n",
    "            # for j in range(avgs.shape[1]):\n",
    "            #     ax[i].plot(ts, avgs[:, j], label=f'{experiment_name} {str(j)}')\n",
    "            #     ax[i].fill_between(ts, avgs[:, j] - 1.96 * stds[:, j], avgs[:, j] + 1.96 * stds[:, j], alpha=0.2)\n",
    "        else:\n",
    "            if stat_key in ['loss', 'grad_sq_norm']:\n",
    "                n = 20\n",
    "                kernel = np.array([1 / n,] * n)\n",
    "                avgs = np.convolve(avgs, kernel)[n // 2:n // 2 + avgs.shape[0]]\n",
    "                stds = np.convolve(stds ** 2, kernel ** 2)[n // 2:n // 2 + stds.shape[0]] ** 0.5\n",
    "            ax[i].plot(ts, avgs, label=experiment_name)\n",
    "            ax[i].fill_between(ts, avgs - 1.96 * stds, avgs + 1.96 * stds, alpha=0.2)\n",
    "    ax[i].legend()\n",
    "\n",
    "\n",
    "ax[1].set_ylim(-0.1, 2.5)\n",
    "# ax[2].set_ylim(-0.1, 0.7)\n",
    "# ax[3].set_ylim(0.5, 0.9)\n",
    "# ax[4].set_ylim(-0.1, 40)\n",
    "# ax[5].set_ylim(-0.05, 0.05)\n",
    "# plt.savefig(f'{DIR}/figs/{NAME}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTMpnF5ybSr2",
   "metadata": {
    "id": "LTMpnF5ybSr2"
   },
   "source": [
    "# Animate\n",
    "Animate the values taken by the $\\{M_h\\}_{h=1}^H$ coefficients during training. Each $M_h$ multiplies a disturbance from $h$ training steps ago (i.e. 0 is most recent in this plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DA5o-cJ1MpL5",
   "metadata": {
    "id": "DA5o-cJ1MpL5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "for v in Ms.values(): assert v.shape == list(Ms.values())[0].shape\n",
    "\n",
    "downsample_factor = 100\n",
    "T, H = v.shape\n",
    "ymin, ymax = -0.012, 0.012\n",
    "\n",
    "fig = plt.figure()  # initializing a figure in which the graph will be plotted\n",
    "ax = plt.axes(xlim =(0, H), ylim=(ymin, ymax))  # marking the x-axis and y-axis\n",
    "ax.set_xlabel('number of steps in the past')\n",
    "ax.set_ylabel('M coefficient')\n",
    "\n",
    "# initializing a line variable\n",
    "ls = {}\n",
    "for k in Ms.keys():\n",
    "  ls[k], = ax.plot([], [], lw = 3, label=k)\n",
    "legend = ax.legend()\n",
    "\n",
    "# data which the line will contain (x, y)\n",
    "def init():\n",
    "  for l in ls.values(): l.set_data([], [])\n",
    "  return list(ls.values())\n",
    "\n",
    "def animate(i):\n",
    "    for k, M in Ms.items():\n",
    "      x, y = range(0, H), M[i * downsample_factor]\n",
    "      ls[k].set_data(x, y[::-1])\n",
    "      # line.set_label(i)\n",
    "    # legend.get_texts()[0].set_text(i * downsample_factor) #Update label each at frame\n",
    "    ax.set_title(f'timestep #{i * downsample_factor} of meta-opt on {NAME}')\n",
    "    return list(ls.values())\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func = init,\n",
    "                     frames = T // downsample_factor, interval = downsample_factor, blit = True)\n",
    "plt.close()\n",
    "h = HTML(anim.to_html5_video())\n",
    "display(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e31e50-2a60-46c3-bff9-05d6584136c8",
   "metadata": {
    "id": "nT5oGkiQ6qAO"
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
