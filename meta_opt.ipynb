{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
   "metadata": {
    "id": "760f4e7d-a02f-42f3-b87d-231d03392428"
   },
   "outputs": [],
   "source": [
    "# # for use in google colab!!\n",
    "# !git clone https://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@github.com/edogariu/meta-opt\n",
    "# !pip install -q ./meta-opt\n",
    "# !pip install -q tensorflow-text ml_collections clu sentencepiece  # for WMT\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DIR_PREFIX = \"drive/My Drive/meta-opt\"\n",
    "\n",
    "# # # for extra one-time setup in colab\n",
    "# # !git clone https://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@github.com/edogariu/meta-opt\n",
    "# # !mkdir meta-opt/data\n",
    "# # !mkdir meta-opt/datasets\n",
    "# # !cp -r \"meta-opt\" \"drive/My Drive/\"\n",
    "# # !pip install kora -q  # library from https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab to help get ID\n",
    "# # from kora.xattr import get_id\n",
    "# # fid = get_id(f\"{dir_prefix}meta_opt.ipynb\")\n",
    "# # print(\"https://colab.research.google.com/drive/\"+fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b898f1e-f002-435a-b1b4-65a21e05a8a7",
   "metadata": {
    "id": "7b898f1e-f002-435a-b1b4-65a21e05a8a7"
   },
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from meta_opt.nn.trainer import create_train_state, gradient_descent, reset_model, eval\n",
    "from meta_opt.problems import mnist, cifar10, wmt\n",
    "\n",
    "from meta_opt.meta_opt import MetaOpt\n",
    "from meta_opt.gaps import MetaOptGAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12",
   "metadata": {
    "id": "56168648-67e9-4ee2-91b8-38dfe7ae5f12"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint()\n",
    "        print('seed set to {}'.format(seed))\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    return rng, seed\n",
    "\n",
    "def get_opt_hyperparams(opt_state):\n",
    "  \"\"\"\n",
    "  helper fn to serialize optax optimizer hyperparameters from the opt_state\n",
    "  \"\"\"\n",
    "  if isinstance(opt_state, Tuple): h = [deepcopy(o.hyperparams) for o in opt_state if hasattr(o, 'hyperparams')]\n",
    "  else: h = deepcopy(opt_state.hyperparams)\n",
    "  return h\n",
    "\n",
    "\n",
    "def get_problem(seed, name, optimizer):\n",
    "    rng, seed = set_seed(seed)\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "\n",
    "    # get dataset and model\n",
    "    if 'MNIST' in name:\n",
    "        tokenizer = None\n",
    "        train_ds, test_ds, example_input, loss_fn, acc_fn = mnist.load_mnist(NUM_ITERS, BATCH_SIZE, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
    "        model = mnist.MLP([28 * 28, 100, 100, 10])\n",
    "    elif 'CIFAR' in name:\n",
    "        tokenizer = None\n",
    "        train_ds, test_ds, example_input, loss_fn, acc_fn = cifar10.load_cifar10(NUM_ITERS, BATCH_SIZE, dataset_dir=f'{DIR_PREFIX}/datasets')\n",
    "        # model = cifar10.VGG(stages=((32, 32), (64, 64), (128, 128)), layer_dims=[128, 10], drop_last_activation=True, dropout=0.1)\n",
    "        model = cifar10.VGG(stages=((64, 64), (128, 128), (256, 256, 256), (512, 512, 512), (512, 512, 512)), layer_dims=[512, 10], drop_last_activation=True, dropout=0.1)  # this is VGG-16\n",
    "    elif 'WMT' in name:\n",
    "        train_ds, test_ds, example_input, loss_fn, acc_fn, tokenizer = wmt.load_wmt(NUM_ITERS, BATCH_SIZE, dataset_dir=f'{DIR_PREFIX}/datasets', num_eval_iters=NUM_EVAL_ITERS if 'NUM_EVAL_ITERS' in globals() else 256)\n",
    "        train_ds.cache()\n",
    "        model = wmt.make_transformer(num_heads=8, num_layers=6, emb_dim=512, qkv_dim=512, mlp_dim=2048)\n",
    "        # model = wmt.make_transformer(num_heads=4, num_layers=3, emb_dim=64, qkv_dim=64, mlp_dim=256)\n",
    "    else:\n",
    "        raise NotImplementedError(name)\n",
    "\n",
    "    tstate = create_train_state(init_rng, model, example_input, optimizer, loss_fn, acc_fn=acc_fn, tokenizer=tokenizer)\n",
    "    del init_rng\n",
    "\n",
    "    args = {'seed': seed,\n",
    "            'model': str(model),\n",
    "            'params': sum(x.size for x in jax.tree_util.tree_leaves(tstate.params)),\n",
    "            'dataset': name,\n",
    "            'num_iters': NUM_ITERS,\n",
    "            'eval_every': EVAL_EVERY,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'reset_every': RESET_EVERY,\n",
    "            'print_every': PRINT_EVERY}\n",
    "\n",
    "    return tstate, train_ds, test_ds, rng, args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68611b25-ff6c-498a-bac1-30873eac9b16",
   "metadata": {
    "id": "68611b25-ff6c-498a-bac1-30873eac9b16"
   },
   "source": [
    "# Standard Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35553ddc-66dd-454e-9608-8820ba402b03",
   "metadata": {
    "id": "35553ddc-66dd-454e-9608-8820ba402b03"
   },
   "outputs": [],
   "source": [
    "def train_standard_opt(seed, problem_name, optimizer):\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = get_opt_hyperparams(tstate.opt_state)\n",
    "    args['optimizer_name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
    "            n = 0\n",
    "            for batch in test_ds.as_numpy_iterator():\n",
    "                loss, acc = eval(tstate, batch)\n",
    "                s['eval_loss'] += loss\n",
    "                s['eval_acc'] += acc\n",
    "                n += 1\n",
    "            s['eval_loss'] /= n\n",
    "            s['eval_acc'] /= n\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "        try:\n",
    "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
    "            s['bleu'] = wmt.bleu(tstate, test_ds)   # calculate bleu score if WMT task\n",
    "            print(s['bleu'][1])\n",
    "        except Exception as e:\n",
    "          print('uh oh', BLEU_EVERY, e)\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    stats['model_params'] = deepcopy(tstate.params)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f",
   "metadata": {
    "id": "5b4d2163-9b72-4c54-85cc-2abb68c0459f"
   },
   "source": [
    "# Meta-Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "907792cd-7420-4e61-a458-372e2dac9128",
   "metadata": {
    "id": "907792cd-7420-4e61-a458-372e2dac9128"
   },
   "outputs": [],
   "source": [
    "def train_meta_opt(seed, problem_name: str, m_method: str, meta_optimizer, H: int, HH: int, initial_lr: int, ema_keys = [], grad_clip = 1e9):\n",
    "    delta = 1e-5\n",
    "    optimizer = optax.chain(optax.add_decayed_weights(delta), optax.sgd(learning_rate=initial_lr))\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "    meta_opt = MetaOpt(tstate, H=H, HH=HH, m_method=m_method, meta_optimizer=meta_optimizer, ema_keys=ema_keys, grad_clip=grad_clip)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_name'] = 'meta'\n",
    "    args['optimizer_args'] = {'initial_lr': initial_lr,\n",
    "                              'm_method': m_method,\n",
    "                              'meta_optimizer_args': get_opt_hyperparams(meta_opt.cstate.opt_state),\n",
    "                              'H': H,\n",
    "                              'HH': HH,\n",
    "                              'ema_keys': ema_keys,\n",
    "                              'grad_clip': grad_clip,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            meta_opt = meta_opt.episode_reset()\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
    "        tstate = meta_opt.meta_step(tstate, grads, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
    "            n = 0\n",
    "            for batch in test_ds.as_numpy_iterator():\n",
    "                loss, acc = eval(tstate, batch)\n",
    "                s['eval_loss'] += loss\n",
    "                s['eval_acc'] += acc\n",
    "                n += 1\n",
    "            s['eval_loss'] /= n\n",
    "            s['eval_acc'] /= n\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "        try:\n",
    "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
    "            s['bleu'] = wmt.bleu(tstate, test_ds)   # calculate bleu score if WMT task\n",
    "            print(s['bleu'][1])\n",
    "        except Exception as e: print('uh oh', BLEU_EVERY, e)\n",
    "        if m_method == 'scalar':\n",
    "            s['M'] = meta_opt.cstate.cparams['M'].reshape(-1)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = v\n",
    "        else:\n",
    "            s['M'] = jnp.stack([m.reshape((m.shape[0], -1)).mean(axis=-1) for m in jax.tree_util.tree_leaves(meta_opt.cstate.cparams['M'])], axis=0).mean(axis=0)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = jnp.stack([m.mean() for m in jax.tree_util.tree_leaves(v)], axis=0).mean(axis=0)\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), 'M': s['M'].sum(), 'M_ema': sum([s[k] for k in s.keys() if k[:5] == 'M_ema'])})\n",
    "\n",
    "    stats['model_params'] = deepcopy(tstate.params)\n",
    "    stats['cparams'] = deepcopy(meta_opt.cstate.cparams)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10045c-53d9-41f3-b559-f7a0c2fe8bf6",
   "metadata": {
    "id": "4d10045c-53d9-41f3-b559-f7a0c2fe8bf6"
   },
   "source": [
    "# Gradient-based Adaptive Policy Selection (GAPS) Meta-Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5867db6-fe41-4dbd-9934-1a52617cc94c",
   "metadata": {
    "id": "c5867db6-fe41-4dbd-9934-1a52617cc94c"
   },
   "outputs": [],
   "source": [
    "def train_gaps_meta_opt(seed, problem_name: str, m_method: str, meta_lr: float, use_adam: bool, H: int, B: int, initial_lr: int, grad_clip: float = 1.0):\n",
    "    optimizer = optax.sgd(learning_rate=initial_lr)\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "    meta_opt = MetaOptGAPS(tstate, H=H, B=B, meta_lr=meta_lr, use_adam=use_adam, delta=1e-5, m_method=m_method)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_name'] = 'gaps_meta'\n",
    "    args['optimizer_args'] = {'initial_lr': initial_lr,\n",
    "                              'm_method': m_method,\n",
    "                              'meta_lr': meta_lr,\n",
    "                              'use_adam': use_adam,\n",
    "                              'H': H,\n",
    "                              'B': B,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            meta_opt = meta_opt.episode_reset()\n",
    "            del reset_rng\n",
    "\n",
    "        # tstate, (loss, grads) = gradient_descent(tstate, batch)  # gaps one does not use both lines\n",
    "        tstate, (loss, grads) = meta_opt.meta_step(tstate, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
    "            n = 0\n",
    "            for batch in test_ds.as_numpy_iterator():\n",
    "                loss, acc = eval(tstate, batch)\n",
    "                s['eval_loss'] += loss\n",
    "                s['eval_acc'] += acc\n",
    "                n += 1\n",
    "            s['eval_loss'] /= n\n",
    "            s['eval_acc'] /= n\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "        try:\n",
    "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
    "            s['bleu'] = wmt.bleu(tstate, test_ds)   # calculate bleu score if WMT task\n",
    "            print(s['bleu'][1])\n",
    "        except Exception as e:\n",
    "          print('uh oh', BLEU_EVERY, e)\n",
    "        if m_method == 'scalar':\n",
    "            s['M'] = meta_opt.cstate.cparams['M'].reshape(-1)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = v\n",
    "        else:\n",
    "            s['M'] = jnp.stack([m.reshape((m.shape[0], -1)).mean(axis=-1) for m in jax.tree_util.tree_leaves(meta_opt.cstate.cparams['M'])], axis=0).mean(axis=0)\n",
    "            for k, v in meta_opt.cstate.cparams['M_ema'].items(): s[f'M_ema_{k}'] = jnp.stack([m.mean() for m in jax.tree_util.tree_leaves(v)], axis=0).mean(axis=0)\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), 'M': s['M'].sum(), 'M_ema': sum([s[k] for k in s.keys() if k[:5] == 'M_ema'])})\n",
    "\n",
    "    stats['model_params'] = deepcopy(tstate.params)\n",
    "    stats['cparams'] = deepcopy(meta_opt.cstate.cparams)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada709b3-489a-44e8-bba2-cfd9221f0988",
   "metadata": {
    "id": "ada709b3-489a-44e8-bba2-cfd9221f0988"
   },
   "source": [
    "# Hypergradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3",
   "metadata": {
    "id": "bb5f5747-f1ee-43fb-8e9e-5228dd1b3bd3"
   },
   "outputs": [],
   "source": [
    "def train_hgd(seed, problem_name: str, initial_lr: float, hypergrad_lr: float):\n",
    "\n",
    "    optimizer = optax.inject_hyperparams(optax.sgd)(learning_rate=initial_lr)\n",
    "    tstate, train_ds, test_ds, rng, args = get_problem(seed, problem_name, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_name'] = 'hgd'\n",
    "    args['optimizer_args'] = {'initial_lr': initial_lr,\n",
    "                              'hypergrad_lr': hypergrad_lr,\n",
    "                              }\n",
    "    stats['args'] = args\n",
    "\n",
    "    prev_grads = None\n",
    "    t0 = perf_counter()\n",
    "    for t, batch in enumerate(pbar := tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])):\n",
    "        t += 1\n",
    "\n",
    "        if t % RESET_EVERY == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = gradient_descent(tstate, batch)\n",
    "        if prev_grads is not None:\n",
    "            hypergrad = -sum([(g1 * g2).sum() for g1, g2 in zip(jax.tree_util.tree_leaves(grads), jax.tree_util.tree_leaves(prev_grads))])\n",
    "            tstate.opt_state.hyperparams['learning_rate'] -= hypergrad_lr * hypergrad\n",
    "        prev_grads = grads\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        s['lr'] = tstate.opt_state.hyperparams['learning_rate'].item()\n",
    "        if t % EVAL_EVERY == 0:\n",
    "            s['eval_loss'], s['eval_acc'] = 0., 0.\n",
    "            n = 0\n",
    "            for batch in test_ds.as_numpy_iterator():\n",
    "                loss, acc = eval(tstate, batch)\n",
    "                s['eval_loss'] += loss\n",
    "                s['eval_acc'] += acc\n",
    "                n += 1\n",
    "            s['eval_loss'] /= n\n",
    "            s['eval_acc'] /= n\n",
    "            s['param_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda p: (p * p).sum(), tstate.params))[0])\n",
    "            s['grad_sq_norm'] = sum(jax.tree_util.tree_flatten(jax.tree_map(lambda g: (g * g).sum(), grads))[0])\n",
    "        try:\n",
    "          if ('WMT' in problem_name) and (t % BLEU_EVERY == 0):\n",
    "            s['bleu'] = wmt.bleu(tstate, test_ds)   # calculate bleu score if WMT task\n",
    "            print(s['bleu'][1])\n",
    "        except Exception as e:\n",
    "          print('uh oh', BLEU_EVERY, e)\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3)})\n",
    "\n",
    "    stats['model_params'] = deepcopy(tstate.params)\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125998da-b7ed-4d07-af09-d2020a813e0a",
   "metadata": {
    "id": "125998da-b7ed-4d07-af09-d2020a813e0a"
   },
   "source": [
    "# Run\n",
    "Select the hyperparameters and the seeds to use for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
    "outputId": "76208dee-ad85-4bd3-b54d-df63f6f04c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: MNIST-bozo\n",
      "using cpu for jax\n",
      "saving data at `./data/`\n"
     ]
    }
   ],
   "source": [
    "# hyperparams\n",
    "SEEDS = [1, 2, 3]\n",
    "NUM_ITERS = 60000\n",
    "NUM_EVAL_ITERS = 32\n",
    "EVAL_EVERY = 200\n",
    "# BLEU_EVERY = 2000  # only for WMT\n",
    "BATCH_SIZE = 128\n",
    "RESET_EVERY = 100000\n",
    "PRINT_EVERY = int(1e10)\n",
    "\n",
    "NAME = 'MNIST-bozo'\n",
    "if 'DIR_PREFIX' not in globals(): DIR_PREFIX = '.'  # use this directory if unspecified\n",
    "\n",
    "from jax.lib import xla_bridge\n",
    "print('dataset:', NAME)\n",
    "print('using', xla_bridge.get_backend().platform, 'for jax')\n",
    "print(f'saving data at `{DIR_PREFIX}/data/`')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5db68d9-62c2-4848-9d47-92c9aa56b251",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "c5db68d9-62c2-4848-9d47-92c9aa56b251",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "0c56701a-c438-44cb-d10a-d47f24b4032b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179220 params in the controller {'M': 179220, 'M_ema': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                   | 4/60000 [00:00<47:16, 21.15it/s, loss=2.27, M=0.0, M_ema=0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m meta_opt \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39minject_hyperparams(optax\u001b[38;5;241m.\u001b[39madam)(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# results['scalar'].append(train_meta_opt(s, NAME, 'scalar', H=64, HH=2, initial_lr=0.01, meta_optimizer=meta_opt))\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiagonal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_meta_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiagonal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# results['scalar_short'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# results['diagonal_short'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True))\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# results['scalar_ema'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True, ema_keys=[0.5, 0.9])),\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# results['diagonal_ema'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True, ema_keys=[0.5, 0.9]))\u001b[39;00m\n\u001b[1;32m     31\u001b[0m sgd_opt \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mchain(optax\u001b[38;5;241m.\u001b[39madd_decayed_weights(\u001b[38;5;241m1e-5\u001b[39m), optax\u001b[38;5;241m.\u001b[39minject_hyperparams(optax\u001b[38;5;241m.\u001b[39msgd)(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36mtrain_meta_opt\u001b[0;34m(seed, problem_name, m_method, meta_optimizer, H, HH, initial_lr, ema_keys, grad_clip)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m reset_rng\n\u001b[1;32m     29\u001b[0m tstate, (loss, grads) \u001b[38;5;241m=\u001b[39m gradient_descent(tstate, batch)\n\u001b[0;32m---> 30\u001b[0m tstate \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_opt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# update all the stats\u001b[39;00m\n\u001b[1;32m     33\u001b[0m s \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Desktop/meta-opt/meta_opt/meta_opt.py:162\u001b[0m, in \u001b[0;36mMetaOpt.meta_step\u001b[0;34m(self, tstate, grads, batch)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m beta, avg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memas\u001b[38;5;241m.\u001b[39mitems(): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memas[beta] \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_map(\u001b[38;5;28;01mlambda\u001b[39;00m v, g: beta \u001b[38;5;241m*\u001b[39m v \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta) \u001b[38;5;241m*\u001b[39m g, avg, grads)  \u001b[38;5;66;03m# update emas\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcstate\u001b[38;5;241m.\u001b[39mH \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcstate\u001b[38;5;241m.\u001b[39mHH:\n\u001b[0;32m--> 162\u001b[0m     control \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_control\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_pytree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memas\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# use past H disturbances\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     tstate \u001b[38;5;241m=\u001b[39m tstate\u001b[38;5;241m.\u001b[39mreplace(params\u001b[38;5;241m=\u001b[39madd_pytrees(tstate\u001b[38;5;241m.\u001b[39mparams, control))\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# compute the states of the ema buffers from `HH` steps ago by reversing the running averages\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/meta-opt/meta_opt/meta_opt.py:69\u001b[0m, in \u001b[0;36mcompute_control\u001b[0;34m(cparams, disturbances, emas)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(emas) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m         control \u001b[38;5;241m=\u001b[39m add_pytrees(control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(multiply_pytrees, M_ema\u001b[38;5;241m.\u001b[39mvalues(), emas\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontrol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m(jnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# uncomment the ones to run\n",
    "results = defaultdict(list)\n",
    "# filename = f'{DIR_PREFIX}/data/{NAME}_raw.pkl'; results = pkl.load(open(filename, 'rb')); print(f'loaded checkpoint from {filename}, containing {list(results.keys())}')\n",
    "\n",
    "for s in SEEDS:\n",
    "    # # # standard benchmarks\n",
    "    # results['rsqrt_0.001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adamw)(learning_rate=wmt.rsqrt_lr_schedule(0.001, 1000), b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-5)))\n",
    "    # # # results['adamw_0.001_1e-3'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adamw)(learning_rate=1e-3, weight_decay=1e-3)))\n",
    "    # # results['rmsprop_0.001'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.rmsprop)(learning_rate=0.001)))  # (CIFAR, 0.001)\n",
    "    # # # results['hgd_0.1'].append(train_hgd(s, NAME, initial_lr=0.1, hypergrad_lr=1e-5))\n",
    "\n",
    "    # # # scalar\n",
    "    # # results['scalar_0.0001'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.0001, H=64, HH=2, initial_lr=0.01, use_adam=False))\n",
    "    # # results['scalar_0.00004'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.00004, H=64, HH=2, initial_lr=0.01, use_adam=False))\n",
    "    # results['scalar_adam_0.0001'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.0001, H=64, HH=2, initial_lr=0.01, use_adam=True))  # (CIFAR, <=0.0001)\n",
    "    # # results['scalar_adam_0.00004'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=0.00004, H=64, HH=2, initial_lr=0.01, use_adam=True))\n",
    "\n",
    "    # # diagonal\n",
    "    # # results['diagonal_0.4'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=0.4, H=64, HH=2, initial_lr=0.01, use_adam=False))\n",
    "    # results['diagonal_adam_0.0002'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=0.0002, H=64, HH=2, initial_lr=0.01, use_adam=True))\n",
    "\n",
    "    # # some ema experiments\n",
    "    meta_opt = optax.inject_hyperparams(optax.adam)(learning_rate=1e-4)\n",
    "    # results['scalar'].append(train_meta_opt(s, NAME, 'scalar', H=64, HH=2, initial_lr=0.01, meta_optimizer=meta_opt))\n",
    "    results['diagonal'].append(train_meta_opt(s, NAME, 'diagonal', H=2, HH=2, initial_lr=0.01, meta_optimizer=meta_opt, grad_clip=2.))\n",
    "    # results['scalar_short'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True))\n",
    "    # results['diagonal_short'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True))\n",
    "    # results['scalar_ema'].append(train_meta_opt(s, NAME, 'scalar', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True, ema_keys=[0.5, 0.9])),\n",
    "    # results['diagonal_ema'].append(train_meta_opt(s, NAME, 'diagonal', meta_lr=1e-4, H=4, HH=2, initial_lr=0.01, use_adam=True, ema_keys=[0.5, 0.9]))\n",
    "\n",
    "    sgd_opt = optax.chain(optax.add_decayed_weights(1e-5), optax.inject_hyperparams(optax.sgd)(learning_rate=0.1))\n",
    "    results['sgd_wd'].append(train_standard_opt(s, NAME, sgd_opt))  # (CIFAR, 0.1)\n",
    "    results['momentum'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.sgd)(learning_rate=0.01, momentum=0.9)))  # (CIFAR, 0.01)\n",
    "    # results['adamw'].append(train_standard_opt(s, NAME, optax.inject_hyperparams(optax.adam)(learning_rate=1e-3)))  # (CIFAR, 0.001)\n",
    "\n",
    "\n",
    "    # # # the caltech paper\n",
    "    # # results['meta_GAPS'].append(train_gaps_meta_opt(s, NAME, 'scalar', meta_lr=0.001, H=6, B=6, initial_lr=0.2, use_adam=False))\n",
    "\n",
    "    if len(results) > 0:\n",
    "        filename = f'{DIR_PREFIX}/data/{NAME}_raw.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pkl.dump(results, f)\n",
    "            print(f'Saved checkpoint for seed #{s} to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f1b24-5846-45f3-b20b-ddcd89bf8fe5",
   "metadata": {
    "id": "335f1b24-5846-45f3-b20b-ddcd89bf8fe5"
   },
   "outputs": [],
   "source": [
    "# clean the stats\n",
    "to_del = []\n",
    "for k, v in results.items():\n",
    "    if len(v) == 0: to_del.append(k)\n",
    "for k in to_del: del results[k]\n",
    "\n",
    "aggregated = {}  # experiment name -> 'args' or timestamp -> stat key -> stat value\n",
    "# gather stats\n",
    "for k, v in results.items():  # for each experiment\n",
    "    aggregated[k] = {'args': []}\n",
    "\n",
    "    for n in range(len(v)):  # for each trial\n",
    "        aggregated[k]['args'].append(v[n]['args'])\n",
    "\n",
    "        for t in range(1, v[0]['args']['num_iters'] + 1):  # for each timestamp\n",
    "            if t not in v[n]: continue\n",
    "            for stat_key, value in v[n][t].items():  # for each stat recorded at that timestamp\n",
    "                if stat_key not in aggregated[k]: aggregated[k][stat_key] = {}\n",
    "                if t not in aggregated[k][stat_key]: aggregated[k][stat_key][t] = []\n",
    "                aggregated[k][stat_key][t].append(value)\n",
    "\n",
    "# aggregate stats\n",
    "ret = defaultdict(dict)  # stat key -> experiment name -> 't' or 'avg' or 'std' ->\n",
    "args = {}\n",
    "for k, v in aggregated.items():  # for experiment\n",
    "    for stat_key in v.keys():  # for stat\n",
    "        if stat_key in ['args', 'model_params', 'controller_params', 'bleu']:\n",
    "            args[k] = v[stat_key]\n",
    "            continue\n",
    "        if k not in ret[stat_key]: ret[stat_key][k] = {}\n",
    "        ret[stat_key][k]['t'] = list(v[stat_key].keys())\n",
    "        arr = np.array(list(v[stat_key].values()))\n",
    "        ret[stat_key][k]['avg'] = np.mean(arr, axis=1)\n",
    "        ret[stat_key][k]['std'] = np.std(arr, axis=1)\n",
    "\n",
    "with open(f'{DIR_PREFIX}/data/{NAME}_processed.pkl', 'wb') as f:\n",
    "    pkl.dump(ret, f)\n",
    "    print('Saved processed results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf",
   "metadata": {
    "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkj0hJUA5gkp",
   "metadata": {
    "id": "kkj0hJUA5gkp"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# plot a particular set of experiments\n",
    "# ----------------------------------------\n",
    "# keys_to_plot = [\n",
    "#     # 'sgd_0.1',\n",
    "#     'sgd_0.4',\n",
    "#     # 'momentum_0.01',\n",
    "#     'adam_0.001',\n",
    "#     # 'adamw_0.001',\n",
    "#     # 'adamw_0.001_1e-2',\n",
    "#     # 'adamw_0.001_1e-3',\n",
    "#     # 'rmsprop_0.001',\n",
    "#     # 'scalar_0.0001',\n",
    "#     # 'scalar_0.00004'\n",
    "#     'scalar_ema',\n",
    "#     'diagonal_short',\n",
    "#     'diagonal_ema',\n",
    "#     ]\n",
    "\n",
    "# ----------------------------------------\n",
    "# OR just plot em all\n",
    "# ----------------------------------------\n",
    "keys_to_plot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd",
   "metadata": {
    "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(len(ret), 1, figsize=(10, 24))\n",
    "Ms = {}\n",
    "\n",
    "for i, stat_key in enumerate(ret.keys()):\n",
    "    ax[i].set_title(stat_key)\n",
    "    for experiment_name in ret[stat_key].keys():\n",
    "        if keys_to_plot is not None and experiment_name not in keys_to_plot: continue\n",
    "        ts, avgs, stds = ret[stat_key][experiment_name]['t'], ret[stat_key][experiment_name]['avg'], ret[stat_key][experiment_name]['std']\n",
    "        if avgs.ndim == 2:\n",
    "            Ms[experiment_name] = avgs\n",
    "            ax[i].plot(ts, avgs.sum(axis=-1), label=experiment_name)\n",
    "            stds = ((stds ** 2).sum(axis=-1)) ** 0.5\n",
    "            ax[i].fill_between(ts, avgs.sum(axis=-1) - 1.96 * stds, avgs.sum(axis=-1) + 1.96 * stds, alpha=0.2)\n",
    "            # for j in range(avgs.shape[1]):\n",
    "            #     ax[i].plot(ts, avgs[:, j], label=f'{experiment_name} {str(j)}')\n",
    "            #     ax[i].fill_between(ts, avgs[:, j] - 1.96 * stds[:, j], avgs[:, j] + 1.96 * stds[:, j], alpha=0.2)\n",
    "        else:\n",
    "            if stat_key in ['loss', 'grad_sq_norm']:\n",
    "                n = 20\n",
    "                kernel = np.array([1 / n,] * n)\n",
    "                avgs = np.convolve(avgs, kernel)[n // 2:n // 2 + avgs.shape[0]]\n",
    "                stds = np.convolve(stds ** 2, kernel ** 2)[n // 2:n // 2 + stds.shape[0]] ** 0.5\n",
    "            ax[i].plot(ts, avgs, label=experiment_name)\n",
    "            ax[i].fill_between(ts, avgs - 1.96 * stds, avgs + 1.96 * stds, alpha=0.2)\n",
    "    ax[i].legend()\n",
    "\n",
    "\n",
    "# ax[1].set_ylim(-0.1, 2.5)\n",
    "# ax[2].set_ylim(-0.1, 0.7)\n",
    "# ax[3].set_ylim(0.5, 0.9)\n",
    "# ax[4].set_ylim(-0.1, 40)\n",
    "# ax[5].set_ylim(-0.05, 0.05)\n",
    "# plt.savefig(f'{DIR_PREFIX}/figs/{NAME}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTMpnF5ybSr2",
   "metadata": {
    "id": "LTMpnF5ybSr2"
   },
   "source": [
    "# Animate\n",
    "Animate the values taken by the $\\{M_h\\}_{h=1}^H$ coefficients during training. Each $M_h$ multiplies a disturbance from $h$ training steps ago (i.e. 0 is most recent in this plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DA5o-cJ1MpL5",
   "metadata": {
    "id": "DA5o-cJ1MpL5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "for v in Ms.values(): assert v.shape == list(Ms.values())[0].shape\n",
    "\n",
    "downsample_factor = 100\n",
    "T, H = v.shape\n",
    "ymin, ymax = -0.012, 0.012\n",
    "\n",
    "fig = plt.figure()  # initializing a figure in which the graph will be plotted\n",
    "ax = plt.axes(xlim =(0, H), ylim=(ymin, ymax))  # marking the x-axis and y-axis\n",
    "ax.set_xlabel('number of steps in the past')\n",
    "ax.set_ylabel('M coefficient')\n",
    "\n",
    "# initializing a line variable\n",
    "ls = {}\n",
    "for k in Ms.keys():\n",
    "  ls[k], = ax.plot([], [], lw = 3, label=k)\n",
    "legend = ax.legend()\n",
    "\n",
    "# data which the line will contain (x, y)\n",
    "def init():\n",
    "  for l in ls.values(): l.set_data([], [])\n",
    "  return list(ls.values())\n",
    "\n",
    "def animate(i):\n",
    "    for k, M in Ms.items():\n",
    "      x, y = range(0, H), M[i * downsample_factor]\n",
    "      ls[k].set_data(x, y[::-1])\n",
    "      # line.set_label(i)\n",
    "    # legend.get_texts()[0].set_text(i * downsample_factor) #Update label each at frame\n",
    "    ax.set_title(f'timestep #{i * downsample_factor} of meta-opt on {NAME}')\n",
    "    return list(ls.values())\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func = init,\n",
    "                     frames = T // downsample_factor, interval = downsample_factor, blit = True)\n",
    "plt.close()\n",
    "h = HTML(anim.to_html5_video())\n",
    "display(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nT5oGkiQ6qAO",
   "metadata": {
    "id": "nT5oGkiQ6qAO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
