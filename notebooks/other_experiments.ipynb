{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4fc9de-e02f-4ca4-a807-3b42951c6cc7",
   "metadata": {},
   "source": [
    "This notebook is for experiments probing things other than performance, such as checking conditions and assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f07326-a164-4e44-8a3f-13aa6770c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 00:18:00.001824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-29 00:18:01.709800: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    from google import colab  # for use in google colab!!    os.system('git clone https://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@github.com/edogariu/meta-opt')\n",
    "    os.system('pip install -q ./meta-opt')\n",
    "    os.system('pip install -q dill')\n",
    "    # !pip install -q jax[cuda12_pip]==0.4.20 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html  # for disabling prealloc, see https://github.com/google/jax/discussions/19014\n",
    "    os.system('pip install -q tensorflow-text ml_collections clu sentencepiece')  # for WMT\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except: pass\n",
    "\n",
    "from meta_opt.train_loops import train_standard_opt, train_hgd, train_meta_opt\n",
    "from meta_opt.utils.experiment_utils import make, save_checkpoint, process_results, bcolors, plot, get_final_cparams\n",
    "from meta_opt.nn import reset_model, train_step, eval\n",
    "from meta_opt import DIR\n",
    "from meta_opt.workloads import get_workload\n",
    "from meta_opt.workloads.wmt import rsqrt\n",
    "from meta_opt.utils.pytree_utils import pytree_sq_norm, pytree_proj, append\n",
    "from meta_opt.utils.experiment_utils import get_opt_hyperparams\n",
    "\n",
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import re\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pkl\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1a678-5b6a-4ee6-a2d9-b57fc2a3c2b8",
   "metadata": {},
   "source": [
    "# Sequential Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9433b-0947-41d5-81c0-99b690e19681",
   "metadata": {},
   "source": [
    "A time-varying linear dynamical system with dynamics $A_1, \\ldots, A_T$ is $(\\kappa, \\gamma)$-sequentially stable if  for all intervals $I = [r, s]\\subseteq [T]$,\n",
    "$$\n",
    "\\left \\|\\prod_{t=s}^{r} A_t\\right \\| \\le \\kappa^2 (1-\\gamma)^{|I|}$$\n",
    "We check if the LTV in meta-opt is indeed sequentially stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ac2716-6f83-44e9-aa26-963fc53bc7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "def forward_and_backward_with_hessian(tstate, batch):\n",
    "    if tstate.rng is not None:\n",
    "        next_key, dropout_key = jax.random.split(tstate.rng)\n",
    "        tstate = tstate.replace(rng=next_key)\n",
    "    else: dropout_key = None\n",
    "    def loss_fn(params):\n",
    "        yhat = tstate.apply_fn({'params': params}, batch['x'], train=True, rngs={'dropout': dropout_key})\n",
    "        loss = tstate.loss_fn(yhat, batch['y'])\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(tstate.params)\n",
    "\n",
    "    p, td = jax.tree_util.tree_flatten(tstate.params)\n",
    "    def loss_fn_from_flat(params_flat):  # for hessian computation\n",
    "        q = []\n",
    "        n = 0\n",
    "        for v in p:\n",
    "            d = np.prod(v.shape)\n",
    "            q.append(params_flat[n: n + d].reshape(v.shape))\n",
    "            n += d\n",
    "        params = jax.tree_util.tree_unflatten(td, q)\n",
    "        return loss_fn(params)\n",
    "        \n",
    "    hessians = jax.hessian(loss_fn_from_flat)(jnp.concatenate([_p.reshape(-1) for _p in p], axis=0))\n",
    "    return tstate, (loss, grads, hessians)\n",
    "\n",
    "# @jax.jit\n",
    "def sequential_stability(tstate, batch, carry, delta):\n",
    "    # the vanilla stuff, but also computing hessian\n",
    "    stats = {}\n",
    "    tstate, (loss, grads, hessians) = forward_and_backward_with_hessian(tstate, batch)\n",
    "    tstate = tstate.apply_gradients(grads=grads)\n",
    "\n",
    "    # use hessian to compute transition matrix and append to the buffer. note that this is using batch averages\n",
    "    def f(H, eta, d, carry):\n",
    "        I = jnp.eye(H.shape[0])\n",
    "        A = jnp.block([[(1 - d) * I, 0 * I, -eta * I], [I, 0 * I, 0 * I], [H, -H, 0 * I]])  # transition matrix for this step\n",
    "        carry = A @ append(carry, jnp.eye(A.shape[0]))  # append an entry of 1 to the right, then left multiply each entry by A. this dynamically handles the cumprod\n",
    "        spectral_norms = jnp.linalg.norm(carry, axis=(1, 2), ord=2)\n",
    "        return carry, spectral_norms\n",
    "    \n",
    "    H = hessians # + 2 * beta * jnp.eye(hessians.shape[0])  # TODO CHECK THIS!!!\n",
    "    carry, spectral_norms = f(H, tstate.opt_state.hyperparams['learning_rate'], delta, carry)\n",
    "    # print(carry.shape, spectral_norms.shape, spectral_norms)\n",
    "    stats['sequential_stability'] = spectral_norms\n",
    "\n",
    "    return tstate, (loss, grads, stats, carry)\n",
    "\n",
    "def run_experiment(seed, name, opt, exp_fn, max_len, model_size):\n",
    "    cfg = {\n",
    "        # training options\n",
    "        'workload': 'MNIST',\n",
    "        'num_iters': 1000,\n",
    "        'eval_every': int(1e9),\n",
    "        'num_eval_iters': -1,\n",
    "        'batch_size': 32,\n",
    "        'full_batch': True,\n",
    "        'reset_every': int(1e9),\n",
    "        'model_size': model_size,\n",
    "    \n",
    "        # experiment options\n",
    "        'seed': seed,\n",
    "        'experiment_name': name,\n",
    "        'load_checkpoint': False,\n",
    "        'overwrite': True,  # whether to allow us to overwrite existing checkpoints or throw errors\n",
    "        'directory': f'{DIR}/..',\n",
    "    } \n",
    "    tstate, train_ds, test_ds, rng, args = get_workload(cfg, opt)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = get_opt_hyperparams(tstate.opt_state)\n",
    "    args['optimizer_name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    param_count = sum(x.size for x in jax.tree_util.tree_leaves(tstate.params))    \n",
    "    carry = jnp.zeros((max_len, param_count * 3, param_count * 3))\n",
    "    t0 = perf_counter()\n",
    "    last_eval_step = None\n",
    "    pbar = tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])\n",
    "    for t, batch in enumerate(pbar):\n",
    "\n",
    "        tstate, (loss, grads, s, carry) = exp_fn(tstate, batch, carry)\n",
    "        \n",
    "        # update all the stats\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % args['eval_every'] == 0 and t != 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = pytree_sq_norm(tstate.params)\n",
    "            s['grad_sq_norm'] = pytree_sq_norm(grads)\n",
    "            if hasattr(tstate.model, 'radius'):\n",
    "                proj_grads = pytree_proj(grads, tstate.params)\n",
    "                s['proj_grad_sq_norm'] = pytree_sq_norm(proj_grads)\n",
    "            last_eval_step = t\n",
    "        if 'bleu_every' in args and t % args['bleu_every'] == 0 and t != 0:\n",
    "            s['bleu'], s['bleu_exemplars'] = tstate.model.bleu(tstate, test_ds.as_numpy_iterator())\n",
    "            print(s['bleu'], s['bleu_exemplars'])\n",
    "        if hasattr(tstate.opt_state, 'hyperparams'): s['lr'] = float(tstate.opt_state.hyperparams['learning_rate'])\n",
    "        else: s['lr'] = 0.\n",
    "        \n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), \n",
    "                          'eval_loss': round(stats[last_eval_step]['eval_loss'].item(), 3) if last_eval_step is not None else 'N/A',\n",
    "                          'lr': round(s['lr'], 5)\n",
    "                          })\n",
    "        if t % args['reset_every'] == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa377ad-8191-4a87-bde2-19562ffd8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOING THE CUSTOM MODEL WITH SIZE [784, 2, 10]\n",
      "1600 params in the model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]2024-03-29 00:18:04.006958: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 30%|███       | 304/1000 [38:40<1:27:53,  7.58s/it, loss=0.657, eval_loss=N/A, lr=0.1]"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 3  # for computational reasons, we will only compute with lengths up to this value\n",
    "DELTA = 0.001  # (1-delta) decay factor for state\n",
    "NAME = 'seq_stab_2'\n",
    "MODEL_SIZE = [28 * 28, 2, 10]\n",
    "SEED = 0\n",
    "\n",
    "results = run_experiment(SEED, \n",
    "                         NAME, \n",
    "                         optax.inject_hyperparams(optax.sgd)(0.1), \n",
    "                         functools.partial(sequential_stability, delta=DELTA), \n",
    "                         MAX_LEN, MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65966847-6875-40bb-875e-a07b6636139f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
