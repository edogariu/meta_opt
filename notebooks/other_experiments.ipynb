{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4fc9de-e02f-4ca4-a807-3b42951c6cc7",
   "metadata": {},
   "source": [
    "This notebook is for experiments probing things other than performance, such as checking conditions and assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f07326-a164-4e44-8a3f-13aa6770c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    from google import colab  # for use in google colab!!    os.system('git clone https://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@github.com/edogariu/meta-opt')\n",
    "    os.system('pip install -q ./meta-opt')\n",
    "    os.system('pip install -q dill')\n",
    "    # !pip install -q jax[cuda12_pip]==0.4.20 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html  # for disabling prealloc, see https://github.com/google/jax/discussions/19014\n",
    "    os.system('pip install -q tensorflow-text ml_collections clu sentencepiece')  # for WMT\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except: pass\n",
    "\n",
    "from meta_opt.train_loops import train_standard_opt, train_hgd, train_meta_opt\n",
    "from meta_opt.utils.experiment_utils import make, save_checkpoint, process_results, bcolors, plot, get_final_cparams\n",
    "from meta_opt.nn import reset_model, train_step, eval\n",
    "from meta_opt import DIR\n",
    "from meta_opt.workloads import get_workload\n",
    "from meta_opt.workloads.wmt import rsqrt\n",
    "from meta_opt.utils.pytree_utils import pytree_sq_norm, pytree_proj\n",
    "from meta_opt.utils.experiment_utils import get_opt_hyperparams\n",
    "\n",
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import re\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pkl\n",
    "import jax\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1a678-5b6a-4ee6-a2d9-b57fc2a3c2b8",
   "metadata": {},
   "source": [
    "# Sequential Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9433b-0947-41d5-81c0-99b690e19681",
   "metadata": {},
   "source": [
    "A time-varying linear dynamical system with dynamics $A_1, \\ldots, A_T$ is $(\\kappa, \\gamma)$-sequentially stable if  for all intervals $I = [r, s]\\subseteq [T]$,\n",
    "$$\n",
    "\\left \\|\\prod_{t=s}^{r} A_t\\right \\| \\le \\kappa^2 (1-\\gamma)^{|I|}$$\n",
    "We check if the LTV in meta-opt is indeed sequentially stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ac2716-6f83-44e9-aa26-963fc53bc7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def forward_and_backward_with_hessian(tstate, batch):\n",
    "    if tstate.rng is not None:\n",
    "        next_key, dropout_key = jax.random.split(tstate.rng)\n",
    "        tstate = tstate.replace(rng=next_key)\n",
    "    else: dropout_key = None\n",
    "    def loss_fn(params):\n",
    "        yhat = tstate.apply_fn({'params': params}, batch['x'], train=True, rngs={'dropout': dropout_key})\n",
    "        loss = tstate.loss_fn(yhat, batch['y'])\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(tstate.params)\n",
    "\n",
    "    p, td = jax.tree_util.tree_flatten(tstate.params)\n",
    "    def loss_fn_from_flat(params_flat):  # for hessian computation\n",
    "        q = []\n",
    "        n = 0\n",
    "        for v in p:\n",
    "            d = np.prod(v.shape)\n",
    "            q.append(params_flat[n: n + d].reshape(v.shape))\n",
    "            n += d\n",
    "        params = jax.tree_util.tree_unflatten(td, q)\n",
    "        return loss_fn(params)\n",
    "        \n",
    "    hessians = jax.hessian(loss_fn_from_flat)(jnp.concatenate([_p.reshape(-1) for _p in p], axis=0))\n",
    "    return tstate, (loss, grads, hessians)\n",
    "\n",
    "@jax.jit\n",
    "def sequential_stability(tstate, batch, carry, max_len, delta):\n",
    "    # the vanilla stuff, but also computing hessian\n",
    "    stats = {}\n",
    "    tstate, (loss, grads, hessians) = forward_and_backward_with_hessian(tstate, batch)\n",
    "    tstate = tstate.apply_gradients(grads=grads)\n",
    "\n",
    "    # use hessian to compute transition matrix and append to the buffer. note that this is using batch averages\n",
    "    def f(H, eta, d, carry):\n",
    "        I = jnp.eye(H.shape[0])\n",
    "        A = jnp.block([[(1 - d) * I, 0 * I, -eta * I], [I, 0 * I, 0 * I], [H, -H, 0 * I]])  # transition matrix for this step\n",
    "        carry = A @ append(carry, jnp.eye(A.shape[0]))  # append an entry of 1 to the right, then left multiply each entry by A. this dynamically handles the cumprod\n",
    "        spectral_norms = jnp.linalg.norm(carry, axis=(1, 2), ord=2)\n",
    "        return carry, spectral_norms\n",
    "    \n",
    "    H = hessians # + 2 * beta * jnp.eye(hessians.shape[0])  # TODO CHECK THIS!!!\n",
    "    carry, spectral_norms = f(H, tstate.opt_state.hyperparams['learning_rate'], delta, carry)\n",
    "    print(carry.shape, spectral_norms.shape, spectral_norms)\n",
    "    stats['sequential_stability'] = spectral_norms\n",
    "\n",
    "    return tstate, (loss, grads, stats, carry)\n",
    "\n",
    "def run_experiment(seed, name, opt, exp_fn):\n",
    "    cfg = {\n",
    "        # training options\n",
    "        'workload': 'CIFAR',\n",
    "        'num_iters': 1000,\n",
    "        'eval_every': int(1e9),\n",
    "        'num_eval_iters': -1,\n",
    "        'batch_size': -1,\n",
    "        'full_batch': False,\n",
    "        'reset_every': int(1e9),\n",
    "        'model': 'tiny',\n",
    "    \n",
    "        # experiment options\n",
    "        'seed': seed,\n",
    "        'experiment_name': name,\n",
    "        'load_checkpoint': False,\n",
    "        'overwrite': True,  # whether to allow us to overwrite existing checkpoints or throw errors\n",
    "        'directory': f'{DIR}/..',\n",
    "    } \n",
    "    tstate, train_ds, test_ds, rng, args = get_workload(cfg, opt)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = get_opt_hyperparams(tstate.opt_state)\n",
    "    args['optimizer_name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    carry = None\n",
    "    t0 = perf_counter()\n",
    "    last_eval_step = None\n",
    "    pbar = tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])\n",
    "    for t, batch in enumerate(pbar):\n",
    "\n",
    "        tstate, (loss, grads, s, carry) = exp_fn(tstate, batch, carry)\n",
    "        \n",
    "        # update all the stats\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % args['eval_every'] == 0 and t != 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = pytree_sq_norm(tstate.params)\n",
    "            s['grad_sq_norm'] = pytree_sq_norm(grads)\n",
    "            if hasattr(tstate.model, 'radius'):\n",
    "                proj_grads = pytree_proj(grads, tstate.params)\n",
    "                s['proj_grad_sq_norm'] = pytree_sq_norm(proj_grads)\n",
    "            last_eval_step = t\n",
    "        if 'bleu_every' in args and t % args['bleu_every'] == 0 and t != 0:\n",
    "            s['bleu'], s['bleu_exemplars'] = tstate.model.bleu(tstate, test_ds.as_numpy_iterator())\n",
    "            print(s['bleu'], s['bleu_exemplars'])\n",
    "        if hasattr(tstate.opt_state, 'hyperparams'): s['lr'] = float(tstate.opt_state.hyperparams['learning_rate'])\n",
    "        else: s['lr'] = 0.\n",
    "        \n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), \n",
    "                          'eval_loss': round(stats[last_eval_step]['eval_loss'].item(), 3) if last_eval_step is not None else 'N/A',\n",
    "                          'lr': round(s['lr'], 5)\n",
    "                          })\n",
    "        if t % args['reset_every'] == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "    return dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa377ad-8191-4a87-bde2-19562ffd8e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension -1 must be >= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_stab_2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_hyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequential_stability\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDELTA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(seed, name, opt, exp_fn)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_experiment\u001b[39m(seed, name, opt, exp_fn):\n\u001b[1;32m     51\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m# training options\u001b[39;00m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectory\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/..\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     68\u001b[0m     } \n\u001b[0;32m---> 69\u001b[0m     tstate, train_ds, test_ds, rng, args \u001b[38;5;241m=\u001b[39m \u001b[43mget_workload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     stats \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m     72\u001b[0m     args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_args\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_opt_hyperparams(tstate\u001b[38;5;241m.\u001b[39mopt_state)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/meta_opt/workloads/__init__.py:33\u001b[0m, in \u001b[0;36mget_workload\u001b[0;34m(cfg, optimizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: model \u001b[38;5;241m=\u001b[39m MLP([\u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     train_ds, test_ds, example_input, loss_fn, metric_fns \u001b[38;5;241m=\u001b[39m \u001b[43mload_cifar10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m VGG16()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mworkload\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWMT\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/meta_opt/workloads/cifar10.py:39\u001b[0m, in \u001b[0;36mload_cifar10\u001b[0;34m(cfg, dataset_dir)\u001b[0m\n\u001b[1;32m     34\u001b[0m     test_ds \u001b[38;5;241m=\u001b[39m test_ds\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m sample: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: tf\u001b[38;5;241m.\u001b[39mcast(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     35\u001b[0m                                                         tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m,\n\u001b[1;32m     36\u001b[0m                                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]}) \u001b[38;5;66;03m# normalize test set\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (num_iters \u001b[38;5;241m*\u001b[39m batch_size) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m50000\u001b[39m)  \u001b[38;5;66;03m# hardcoded train dataset length because `tfds` on della is mean :(\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_remainder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtake(num_iters)\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     40\u001b[0m     test_ds \u001b[38;5;241m=\u001b[39m test_ds\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(batch_size, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_ds, test_ds, jnp\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m)), cross_entropy, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: cross_entropy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy}\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:1877\u001b[0m, in \u001b[0;36mDatasetV2.batch\u001b[0;34m(self, batch_size, drop_remainder, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> batch_op ->\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[39;00m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_op\n\u001b[0;32m-> 1877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbatch_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_remainder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1878\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/data/ops/batch_op.py:39\u001b[0m, in \u001b[0;36m_batch\u001b[0;34m(input_dataset, batch_size, drop_remainder, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m debug_mode\u001b[38;5;241m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     37\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_BatchDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_remainder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _ParallelBatchDataset(\n\u001b[1;32m     42\u001b[0m       input_dataset,\n\u001b[1;32m     43\u001b[0m       batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m       deterministic,\n\u001b[1;32m     47\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/data/ops/batch_op.py:68\u001b[0m, in \u001b[0;36m_BatchDataset.__init__\u001b[0;34m(self, input_dataset, batch_size, drop_remainder, name)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m constant_drop_remainder:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;66;03m# NOTE(mrry): `constant_drop_remainder` may be `None` (unknown statically)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m   \u001b[38;5;66;03m# or `False` (explicitly retaining the remainder).\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;66;03m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[1;32m     67\u001b[0m   constant_batch_size \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconstant_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size)\n\u001b[0;32m---> 68\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstant_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     73\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m component_spec: component_spec\u001b[38;5;241m.\u001b[39m_batch(\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     74\u001b[0m       input_dataset\u001b[38;5;241m.\u001b[39melement_spec)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/data/util/nest.py:122\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_types_dict):\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies `func` to each entry in `structure` and returns a new structure.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m  Applies `func(x[0], x[1], ...)` where x[i] is an entry in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_types_dict\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1068\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[0;32m-> 1068\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_data_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown modality used \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(modality)\n\u001b[1;32m   1072\u001b[0m   )\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1135\u001b[0m, in \u001b[0;36m_tf_data_map_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m   1132\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_data_flatten(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1133\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m-> 1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_pack_sequence_as(structure[\u001b[38;5;241m0\u001b[39m], \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1135\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1132\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_data_flatten(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1133\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m-> 1135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_pack_sequence_as(structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries])\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/data/ops/batch_op.py:69\u001b[0m, in \u001b[0;36m_BatchDataset.__init__.<locals>.<lambda>\u001b[0;34m(component_spec)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m constant_drop_remainder:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;66;03m# NOTE(mrry): `constant_drop_remainder` may be `None` (unknown statically)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m   \u001b[38;5;66;03m# or `False` (explicitly retaining the remainder).\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;66;03m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[1;32m     67\u001b[0m   constant_batch_size \u001b[38;5;241m=\u001b[39m tensor_util\u001b[38;5;241m.\u001b[39mconstant_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_size)\n\u001b[1;32m     68\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m---> 69\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m component_spec: \u001b[43mcomponent_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconstant_batch_size\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     70\u001b[0m       input_dataset\u001b[38;5;241m.\u001b[39melement_spec)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m     73\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m component_spec: component_spec\u001b[38;5;241m.\u001b[39m_batch(\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     74\u001b[0m       input_dataset\u001b[38;5;241m.\u001b[39melement_spec)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/framework/tensor.py:1194\u001b[0m, in \u001b[0;36mTensorSpec._batch\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m   1193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m TensorSpec(\n\u001b[0;32m-> 1194\u001b[0m       \u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorShape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape),\n\u001b[1;32m   1195\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:830\u001b[0m, in \u001b[0;36mTensorShape.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new TensorShape with the given dimensions.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m  TypeError: If dims cannot be converted to a list of dimensions.\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dims, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):  \u001b[38;5;66;03m# Most common case.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(as_dimension(d)\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:830\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new TensorShape with the given dimensions.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m  TypeError: If dims cannot be converted to a list of dimensions.\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dims, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):  \u001b[38;5;66;03m# Most common case.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mas_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:744\u001b[0m, in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    742\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/meta-opt/env/lib/python3.11/site-packages/tensorflow/python/framework/tensor_shape.py:225\u001b[0m, in \u001b[0;36mDimension.__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    221\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension value must be integer or None or have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man __index__ method, got value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{1!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    223\u001b[0m           value, \u001b[38;5;28mtype\u001b[39m(value))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 225\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimension \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m must be >= 0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value)\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension -1 must be >= 0"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 3  # for computational reasons, we will only compute with lengths up to this value\n",
    "DELTA = 0.001  # (1-delta) decay factor for state\n",
    "NAME = 'seq_stab_2'\n",
    "SEED = 0\n",
    "\n",
    "results = run_experiment(SEED, NAME, optax.inject_hyperparams(optax.sgd)(0.1), functools.partial(sequential_stability, max_len=MAX_LEN, delta=DELTA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65966847-6875-40bb-875e-a07b6636139f",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
