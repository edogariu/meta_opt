{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33082369-577e-4cf7-9dac-29ca172c9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sgd\n"
     ]
    }
   ],
   "source": [
    "# from absl import logging\n",
    "# logging.set_verbosity(logging.INFO)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from meta_opt.algoperf import runner\n",
    "from meta_opt.experiment import ExperimentConfig\n",
    "from meta_opt.optimizers import *\n",
    "from meta_opt.optimizers.schedules import ConstantScheduleConfig\n",
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    seed = 0,\n",
    "    full_batch = False,\n",
    "    num_episodes = 6,\n",
    "    num_iters = 200,\n",
    "    batch_size = 256,\n",
    "    eval_every = 20,\n",
    "\n",
    "    workload_name = 'mnist',\n",
    "    experiment_name = 'colab_test',\n",
    "    experimental_setup = 'algoperf',\n",
    "    data_dir = '../datasets',\n",
    "    num_batch_devices=8, num_opt_devices=1,\n",
    ")\n",
    "\n",
    "cfgs = {\n",
    "    'sgd': SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=0.2)),\n",
    "    'nesterov': SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=0.01), nesterov=True, momentum=0.9),\n",
    "    'adamw': AdamWConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=0.001), b1=0.9, b2=0.999),\n",
    "    'dadapt': DAdaptationConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1), b1=0.9, b2=0.999),\n",
    "    # 'cocob': COCOBConfig(),\n",
    "    # 'dog': DoGConfig(),\n",
    "    'dowg': DoWGConfig(),\n",
    "    'polyak': PolyakConfig(f_min=0),\n",
    "    # 'scalar': MetaOptConfig(base_learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=0.00), \n",
    "    #                          # scale_by_adam_betas=None, \n",
    "    #                          scale_by_adam_betas=(0.9, 0.999), \n",
    "    #                          H=1, HH=2, m_method='scalar', \n",
    "    #                          # meta_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-3), grad_clip=5),\n",
    "    #                          meta_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=CosineScheduleConfig(init_value=0, warmup_steps=100, peak_value=1e-3, decay_steps=2000), grad_clip=None),\n",
    "    #                          # meta_optimizer_cfg=COCOBConfig(),\n",
    "    #                          # meta_optimizer_cfg=DAdaptationConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1), b1=0.9, b2=0.9),\n",
    "    #                          # meta_optimizer_cfg=MechanicConfig(base_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-4))),\n",
    "    #                          # meta_optimizer_cfg=AdamWConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-3), b1=0.9, b2=0.9),\n",
    "    #                          counterfactual=True,\n",
    "    #                          fake_the_dynamics=False, freeze_cost_fn_during_rollouts=False,\n",
    "    #                          weight_decay=1e-4, grad_clip=None, freeze_gpc_params=None, use_bfloat16=False),\n",
    "    'diagonal': MetaOptConfig(base_learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=0.00), \n",
    "                         # scale_by_adam_betas=None, \n",
    "                         scale_by_adam_betas=(0.9, 0.999), \n",
    "                         H=1, HH=2, m_method='diagonal', \n",
    "                         meta_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-3), grad_clip=None),\n",
    "                         # meta_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=CosineScheduleConfig(init_value=0, warmup_steps=300, peak_value=1e-3, decay_steps=200000), grad_clip=None),\n",
    "                         # meta_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-3), momentum=0.9, grad_clip=None),\n",
    "                         # meta_optimizer_cfg=COCOBConfig(),\n",
    "                         # meta_optimizer_cfg=DAdaptationConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=0.5), b1=0.9, b2=0.999),\n",
    "                         # meta_optimizer_cfg=MechanicConfig(base_optimizer_cfg=SGDConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-1))),\n",
    "                         # meta_optimizer_cfg=AdamWConfig(learning_rate_schedule_cfg=ConstantScheduleConfig(learning_rate=1e-3), b1=0.9, b2=0.9),\n",
    "                         counterfactual=True,\n",
    "                         fake_the_dynamics=False, freeze_cost_fn_during_rollouts=False,\n",
    "                         weight_decay=1e-4, grad_clip=None, freeze_gpc_params=None, use_bfloat16=False),\n",
    "}\n",
    "\n",
    "from absl import flags\n",
    "flags.FLAGS(['yeet.py', '--config_path', 'hi'])\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 9))\n",
    "for cfg_key, cfg in cfgs.items():\n",
    "    print('running', cfg_key)\n",
    "    losses, evals, tstate = runner.run(experiment_cfg, cfg)\n",
    "    ax[0].plot(losses.T[0], losses.T[1], label=cfg_key)\n",
    "    if not experiment_cfg.full_batch: ax[1].plot(evals.T[0], evals.T[1], label=cfg_key)\n",
    "    if cfg.optimizer_name == 'MetaOpt': print(tstate.opt_state[0].gpc_params.reshape(cfg.H, -1).mean(axis=-1))\n",
    "ax[0].set_ylim(0, 3)\n",
    "ax[1].set_ylim(0.5, 1)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "# plt.savefig('../figs/mnist_synthetic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea340e5-0781-4800-a4c6-d08d6270f230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaopt",
   "language": "python",
   "name": "metaopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
