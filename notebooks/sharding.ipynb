{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765805c0-b299-41e9-9d19-7a225cac12ef",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "- [X] shard!\n",
    "- [X] make sure we instantiate the big arrays in sharded form from the init fn!\n",
    "- [X] make sure we pad flattened params/grads to divide `n_opt_devices` \n",
    "- [X] add `bfloat16` support cause memory matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b16246-84dd-4104-88ba-c527d260e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert 'batch' in MESH.axis_names\n",
    "\n",
    "from meta_opt.optimizers import OptimizerConfig, SGDConfig, AdamWConfig, MetaOptConfig\n",
    "\n",
    "import tqdm\n",
    "import functools\n",
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import NamedSharding, Mesh, PartitionSpec as P\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from flax import jax_utils, struct\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406e6348-f3c3-4707-b481-824463634b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip3 install memory-profiler\n",
    "%load_ext memory_profiler\n",
    "!pip3 install tensorflow tensorboard-plugin-profile\n",
    "!pip3 install ml-dtypes==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba45a14-ac63-4524-9289-aff5f9950ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if os.path.isdir('/tmp/trace'):\n",
    "#     import shutil\n",
    "#     shutil.rmtree(\"/tmp/trace\")\n",
    "\n",
    "# @functools.partial(jax.pmap, axis_name='batch', in_axes=(0, 0, 0, None), out_axes=(0, 0), static_broadcasted_argnums=(3,)) \n",
    "# def _pmapped_train_step(batch, params, opt_state, opt):\n",
    "#     loss_fn = LossFn(batch)\n",
    "#     grads = jax.grad(loss_fn)(params)\n",
    "#     grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "\n",
    "#     # dh = jax.lax.with_sharding_constraint(opt_state[0].disturbance_history, NamedSharding(MESH, P(None, 'opt')))\n",
    "#     # ph = jax.lax.with_sharding_constraint(opt_state[0].param_history, NamedSharding(MESH, P(None, 'opt')))\n",
    "#     # opt_state = (opt_state[0].replace(disturbance_history=dh, param_history=ph), opt_state[1])\n",
    "    \n",
    "#     updates, opt_state = opt[1](grads, opt_state, params, cost_fn=loss_fn)\n",
    "\n",
    "#     # dh = jax.lax.with_sharding_constraint(opt_state[0].disturbance_history, NamedSharding(MESH, P(None, 'opt')))\n",
    "#     # ph = jax.lax.with_sharding_constraint(opt_state[0].param_history, NamedSharding(MESH, P(None, 'opt')))\n",
    "#     # opt_state = (opt_state[0].replace(disturbance_history=dh, param_history=ph), opt_state[1])\n",
    "    \n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     return params, opt_state\n",
    "\n",
    "# def pmapped_train_step(batch, replicated_params, replicated_opt_state, opt):\n",
    "#     batch = batch.reshape(BATCH_NUM_DEVICES, -1, NUM_DATA)\n",
    "#     sharded_batch = jax.device_put(batch, NamedSharding(MESH, P('batch', None, None)))\n",
    "    \n",
    "#     # print(replicated_opt_state[0].disturbance_history.sharding, replicated_opt_state[0].disturbance_history.shape)\n",
    "#     replicated_params, replicated_opt_state = _pmapped_train_step(sharded_batch, replicated_params, replicated_opt_state, opt)\n",
    "#     # print(replicated_opt_state[0].disturbance_history.sharding, replicated_opt_state[0].disturbance_history.shape)\n",
    "    \n",
    "#     return replicated_params, replicated_opt_state\n",
    "\n",
    "# # with jax.profiler.trace('/tmp/trace'):\n",
    "\n",
    "# opt = cfg.make_jax()\n",
    "# opt = (jax.tree_util.Partial(opt.init), jax.tree_util.Partial(opt.update))\n",
    "# params = make_params(0)  # make some pretend parameters\n",
    "# opt_state = make_opt_state(params, opt)\n",
    "\n",
    "# # replicate\n",
    "# replicate_fn = lambda v: jnp.tile(v[None], (BATCH_NUM_DEVICES,) + ((1,) * v.ndim))\n",
    "# unreplicate_fn = lambda v: v[0]\n",
    "\n",
    "# replicated_params = replicate_fn(params)\n",
    "# replicated_params = jax.device_put(replicated_params, NamedSharding(MESH, P('batch')))\n",
    "\n",
    "# # replicated_opt_state = opt_state\n",
    "# replicated_opt_state = jax.tree_map(replicate_fn, opt_state)\n",
    "\n",
    "# replicated_opt_state = shard_opt_state(replicated_opt_state)  # uncomment this line to shard the optimizer state\n",
    "# jax.debug.visualize_array_sharding(unreplicate_fn(replicated_opt_state[0].disturbance_history))\n",
    "\n",
    "# # do a few steps of pmapped GD\n",
    "# for seed in tqdm.trange(90):\n",
    "#     batch = make_batch(seed)\n",
    "#     replicated_params, replicated_opt_state = pmapped_train_step(batch, replicated_params, replicated_opt_state, opt)\n",
    "\n",
    "# jax.debug.visualize_array_sharding(unreplicate_fn(replicated_opt_state[0].disturbance_history))\n",
    "# %timeit pmapped_train_step(batch, replicated_params, replicated_opt_state, opt)[0].block_until_ready()\n",
    "\n",
    "# # %load_ext tensorboard\n",
    "# # %tensorboard --logdir=/tmp/trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64b7a27-99ae-466a-ac7a-b00c498beb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if os.path.isdir('/tmp/trace'):\n",
    "#     import shutil\n",
    "#     shutil.rmtree(\"/tmp/trace\")\n",
    "\n",
    "# from jax.experimental.shard_map import shard_map\n",
    "\n",
    "# replicate_fn = lambda v: jnp.tile(v, (BATCH_NUM_DEVICES,) + ((1,) * (v.ndim - 1))) if v.ndim > 0 else v\n",
    "# unreplicate_fn = lambda v: v[:(v.shape[0] // BATCH_NUM_DEVICES)] if v.ndim > 0 else v\n",
    "\n",
    "# def make_shardings(opt, spec):\n",
    "#     shardings = {\n",
    "#         # to replicate\n",
    "#         'gpc_params': P(),\n",
    "#         # 'gpc_tx': P(),\n",
    "#         'gpc_opt_state': P(),\n",
    "#         # 'H': P(),\n",
    "#         # 'HH': P(),\n",
    "#         't': P(),\n",
    "#         # 'num_params': P(),\n",
    "#         # 'base_lr': P(),\n",
    "#         # 'disturbance_transform': P(),\n",
    "#         'disturbance_transform_state': P(),\n",
    "#         'recent_gpc_grads': P(),\n",
    "#         'recent_gpc_cost': P(),\n",
    "#         'cost_fn_history': P(),\n",
    "    \n",
    "#         # to shard along num_params axis\n",
    "#         'disturbance_history': spec,\n",
    "#         'param_history': spec,\n",
    "#     }\n",
    "#     os = make_opt_state(make_params(0), opt)\n",
    "#     shardings = (os[0].replace(**shardings), optax.EmptyState())\n",
    "#     del os\n",
    "#     return shardings\n",
    "\n",
    "# opt = cfg.make_jax()\n",
    "# opt = (jax.tree_util.Partial(opt.init), jax.tree_util.Partial(opt.update))\n",
    "# shardings = make_shardings(opt, P('batch', None))\n",
    "# s2 = jax.tree_map(lambda v: NamedSharding(MESH, v), make_shardings(opt, P('batch', 'opt')))\n",
    "\n",
    "# @functools.partial(shard_map, mesh=MESH, in_specs=(P('batch'), P('batch'), shardings, P()), out_specs=(P('batch'), shardings), check_rep=False)\n",
    "# def _shmapped_train_step(batch, params, opt_state, opt):\n",
    "#     loss_fn = LossFn(batch)\n",
    "#     grads = jax.grad(loss_fn)(params)\n",
    "#     grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "#     updates, opt_state = opt[1](grads, opt_state, params, cost_fn=loss_fn)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     return params, opt_state\n",
    "\n",
    "# @jax.jit\n",
    "# def shmapped_train_step(batch, params, opt_state, opt):\n",
    "#     # assert params.ndim == 2 and batch.ndim == 2 and opt_state[0].disturbance_history.ndim == 3, (params.shape, batch.shape, opt_state[0].disturbance_history.shape)\n",
    "#     # print(opt_state[0].disturbance_history.shape, opt_state[0].disturbance_history.sharding)\n",
    "#     batch = jax.device_put(batch, NamedSharding(MESH, P('batch', None)))\n",
    "#     opt_state = jax.lax.with_sharding_constraint(opt_state, s2)\n",
    "#     params, opt_state = _shmapped_train_step(batch, params, opt_state, opt)\n",
    "#     opt_state = jax.lax.with_sharding_constraint(opt_state, s2)\n",
    "#     # print(opt_state[0].disturbance_history.shape, opt_state[0].disturbance_history.sharding)\n",
    "#     return params, opt_state\n",
    "\n",
    "# with jax.profiler.trace('/tmp/trace'):\n",
    "\n",
    "#     # opt = cfg.make_jax()\n",
    "#     # opt = (jax.tree_util.Partial(opt.init), jax.tree_util.Partial(opt.update))\n",
    "#     params = make_params(0)  # make some pretend parameters\n",
    "#     opt_state = make_opt_state(params, opt)\n",
    "    \n",
    "#     # replicate\n",
    "#     replicated_params = jax.device_put(replicate_fn(params), NamedSharding(MESH, P('batch')))\n",
    "#     replicated_opt_state = (opt_state[0].replace(disturbance_history=replicate_fn(opt_state[0].disturbance_history), \n",
    "#                                                  param_history=replicate_fn(opt_state[0].param_history)), opt_state[1])\n",
    "#     replicated_opt_state = shard_opt_state(replicated_opt_state)  # uncomment this line to shard the optimizer state\n",
    "#     print(replicated_opt_state[0].disturbance_history.shape, replicated_opt_state[0].disturbance_history.sharding)\n",
    "    \n",
    "#     # do a few steps of pmapped GD\n",
    "#     for seed in tqdm.trange(74):\n",
    "#         batch = make_batch(seed)\n",
    "#         replicated_params, replicated_opt_state = shmapped_train_step(batch, replicated_params, replicated_opt_state, opt)\n",
    "    \n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=/tmp/trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547c386-5cb7-4c24-9a45-feaedbc1a1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c577206-877f-4b6f-a864-81706b3e6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if os.path.isdir('/tmp/trace'):\n",
    "#     import shutil\n",
    "#     shutil.rmtree(\"/tmp/trace\")\n",
    "\n",
    "# @jax.jit\n",
    "# def sharded_train_step(batch, params, opt_state, opt):\n",
    "#     loss_fn = LossFn(batch)\n",
    "#     grads = jax.grad(loss_fn)(params)\n",
    "#     updates, opt_state = opt[1](grads, opt_state, params, cost_fn=loss_fn)\n",
    "#     params = optax.apply_updates(params, updates)\n",
    "#     return params, opt_state\n",
    "\n",
    "# with jax.profiler.trace('/tmp/trace'):\n",
    "\n",
    "#     opt = cfg.make_jax()\n",
    "#     opt = (jax.tree_util.Partial(opt.init), jax.tree_util.Partial(opt.update))\n",
    "#     params = make_params(0)  # make some pretend parameters\n",
    "#     opt_state = make_opt_state(params, opt)\n",
    "    \n",
    "#     opt_state = shard_opt_state(opt_state)  # uncomment this line to shard the optimizer state\n",
    "    \n",
    "#     # do a few steps of pmapped GD\n",
    "#     for seed in tqdm.trange(90):\n",
    "#         batch = make_batch(seed)\n",
    "#         batch = jax.device_put(batch, NamedSharding(MESH, P('batch', None)))\n",
    "#         params, opt_state = sharded_train_step(batch, params, opt_state, opt)\n",
    "#     %timeit sharded_train_step(batch, params, opt_state, opt)[0].block_until_ready()\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=/tmp/trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4dc907-fade-401e-8592-0a71f35e650d",
   "metadata": {},
   "source": [
    "# Profile a full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a6dc10-b69e-46e4-b139-5200b81c533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \u001b[93m\u001b[1m8 devices in a mesh Mesh(device_ids=array([[0, 1, 2, 3, 4, 5, 6, 7]]), axis_names=('batch', 'opt')) of shape OrderedDict([('batch', 1), ('opt', 8)])\u001b[0m\n",
      "INFO:absl:Creating directory at /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax for experiments to be saved to.\n",
      "INFO:absl:\u001b[96m\u001b[1mno `batch_size` provided. using default of 512 for the workload mnist!\u001b[0m\n",
      "INFO:absl: \u001b[93m\u001b[1m8 devices in a mesh Mesh(device_ids=array([[0, 1, 2, 3, 4, 5, 6, 7]]), axis_names=('batch', 'opt')) of shape OrderedDict([('batch', 1), ('opt', 8)])\u001b[0m\n",
      "INFO:absl:Using \u001b[93m\u001b[1mcpu\u001b[0m for jax\n",
      "INFO:absl:\u001b[1m==========================================================================\u001b[0m\n",
      "INFO:absl:\u001b[1mEXPERIMENT CONFIG\u001b[0m\n",
      "INFO:absl:\u001b[1m==========================================================================\u001b[0m\n",
      "INFO:absl:{ 'experiment_name': 'test',\n",
      "  'seed': 0,\n",
      "  'workload_name': 'mnist',\n",
      "  'full_batch': False,\n",
      "  'num_episodes': 1,\n",
      "  'num_iters': 200,\n",
      "  'batch_size': 512,\n",
      "  'framework': 'jax',\n",
      "  'num_batch_devices': 1,\n",
      "  'num_opt_devices': 8,\n",
      "  'eval_every': -1,\n",
      "  'checkpoint_every': -1,\n",
      "  'log_every': 20,\n",
      "  'print_with_colors': True,\n",
      "  'use_wandb': False}\n",
      "INFO:absl:\n",
      "\n",
      "INFO:absl:\u001b[1m==========================================================================\u001b[0m\n",
      "INFO:absl:\u001b[1mOPTIMIZER CONFIG\u001b[0m\n",
      "INFO:absl:\u001b[1m==========================================================================\u001b[0m\n",
      "INFO:absl:{ 'initial_learning_rate': 0.1,\n",
      "  'weight_decay': 0.0001,\n",
      "  'grad_clip': None,\n",
      "  'scale_by_adam_betas': None,\n",
      "  'H': 64,\n",
      "  'HH': 2,\n",
      "  'm_method': 'scalar',\n",
      "  'meta_optimizer_cfg': { 'learning_rate': 1e-05,\n",
      "                          'momentum': 0,\n",
      "                          'nesterov': False,\n",
      "                          'weight_decay': None,\n",
      "                          'grad_clip': None,\n",
      "                          'optimizer_name': 'SGD',\n",
      "                          'self_tuning': False,\n",
      "                          'reset_opt_state': True},\n",
      "  'fake_the_dynamics': False,\n",
      "  'freeze_gpc_params': True,\n",
      "  'freeze_cost_fn_during_rollouts': False,\n",
      "  'use_bfloat16': True,\n",
      "  'optimizer_name': 'MetaOpt',\n",
      "  'self_tuning': True,\n",
      "  'reset_opt_state': True}\n",
      "INFO:absl:\n",
      "\n",
      "\n",
      "INFO:absl:\n",
      "INFO:absl:\u001b[92m\u001b[1mexperiment_dir=/Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax\u001b[0m\n",
      "INFO:absl:\u001b[92m\u001b[1mseed=0\u001b[0m\n",
      "INFO:absl:Initializing dataset for workload: \u001b[96m\u001b[1mmnist\u001b[0m.\n",
      "INFO:absl:Load dataset info from /Users/evandogariu/Desktop/meta-opt/datasets/mnist/3.0.1\n",
      "INFO:absl:For 'mnist/3.0.1': fields info.[citation, splits, supervised_keys, module_name] differ on disk and in the code. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/Users/evandogariu/Desktop/meta-opt/datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split train[:50000], from /Users/evandogariu/Desktop/meta-opt/datasets/mnist/3.0.1\n",
      "INFO:absl:Initializing the model (for \u001b[96m\u001b[1mmnist\u001b[0m) and also optimizer: \u001b[92m\u001b[1mMetaOpt\u001b[0m\n",
      "WARNING:absl:\u001b[93m\u001b[1mthe meta-opt controller is frozen! optimizer behavior wont change over time\u001b[0m\n",
      "INFO:absl:\u001b[96msharding opt state across 8 devices\u001b[0m\n",
      "INFO:absl:\u001b[96mneeded to pad iterates from 101770 to 101776 to evenly divide over 8 devices\u001b[0m\n",
      "INFO:absl:Model has \u001b[1m101770\u001b[0m parameters and the optimizer state takes \u001b[1m0.01MB\u001b[0m\n",
      "INFO:absl:Initializing checkpoint and metrics.\n",
      "INFO:absl:Found no checkpoint files in /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax with prefix checkpoint_\n",
      "INFO:absl:Saving meta data to /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax/meta_data_0.json.\n",
      "INFO:absl:Saving config to /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax/config_0.json.\n",
      "WARNING:absl:\u001b[93m\u001b[1mI found no checkpoint, so we proceed without loading :)\u001b[0m\n",
      "INFO:absl:\u001b[91m\u001b[1mStarting training loop.\u001b[0m\n",
      "INFO:absl:\u001b[94m\u001b[1mResetting model!\u001b[0m\n",
      "INFO:absl:\u001b[94m\u001b[1mAlso resetting optimizer state!\u001b[0m\n",
      "INFO:absl:[1] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=35.100000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=1, grad_sq_norm=44.306313, loss=2.645919, mem.available=12697731072, mem.percent_used=50.700000, mem.read_bytes_since_boot=252404592640, mem.total=25769803776, mem.used=9543499776, mem.write_bytes_since_boot=289327357952, model_state_memory=16, net.bytes_recv_since_boot=3723100160, net.bytes_sent_since_boot=1650926592, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=137.56842041015625, step_time=0.384817, total_metaopt_memory=7714\n",
      "INFO:absl:[20] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=71.000000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=20, grad_sq_norm=13.048641, loss=1.313312, mem.available=12653314048, mem.percent_used=50.900000, mem.read_bytes_since_boot=252404604928, mem.total=25769803776, mem.used=9588391936, mem.write_bytes_since_boot=289327362048, model_state_memory=16, net.bytes_recv_since_boot=3723116544, net.bytes_sent_since_boot=1650947072, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=141.3916778564453, step_time=0.039300, total_metaopt_memory=7714\n",
      "INFO:absl:[40] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=64.400000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=40, grad_sq_norm=7.933668, loss=0.946238, mem.available=12632178688, mem.percent_used=51.000000, mem.read_bytes_since_boot=252404617216, mem.total=25769803776, mem.used=9609248768, mem.write_bytes_since_boot=289327366144, model_state_memory=16, net.bytes_recv_since_boot=3723129856, net.bytes_sent_since_boot=1650960384, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=145.70022583007812, step_time=0.041880, total_metaopt_memory=7714\n",
      "INFO:absl:[60] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=73.000000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=60, grad_sq_norm=7.623615, loss=0.784008, mem.available=12638076928, mem.percent_used=51.000000, mem.read_bytes_since_boot=252404617216, mem.total=25769803776, mem.used=9604399104, mem.write_bytes_since_boot=289327370240, model_state_memory=16, net.bytes_recv_since_boot=3723149312, net.bytes_sent_since_boot=1650978816, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=149.8242645263672, step_time=0.037272, total_metaopt_memory=7714\n",
      "INFO:absl:[80] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=71.200000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=80, grad_sq_norm=10.009375, loss=0.649458, mem.available=12602998784, mem.percent_used=51.100000, mem.read_bytes_since_boot=252404617216, mem.total=25769803776, mem.used=9639477248, mem.write_bytes_since_boot=289327587328, model_state_memory=16, net.bytes_recv_since_boot=3723217920, net.bytes_sent_since_boot=1651047424, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=153.4681854248047, step_time=0.037561, total_metaopt_memory=7714\n",
      "INFO:absl:[100] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=59.700000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=100, grad_sq_norm=13.168853, loss=0.615329, mem.available=12604375040, mem.percent_used=51.100000, mem.read_bytes_since_boot=252404617216, mem.total=25769803776, mem.used=9637167104, mem.write_bytes_since_boot=289327591424, model_state_memory=16, net.bytes_recv_since_boot=3723230208, net.bytes_sent_since_boot=1651059712, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=156.97354125976562, step_time=0.036672, total_metaopt_memory=7714\n",
      "INFO:absl:[120] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=59.700000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=120, grad_sq_norm=6.295961, loss=0.562937, mem.available=12569804800, mem.percent_used=51.200000, mem.read_bytes_since_boot=252404617216, mem.total=25769803776, mem.used=9670557696, mem.write_bytes_since_boot=289327595520, model_state_memory=16, net.bytes_recv_since_boot=3723244544, net.bytes_sent_since_boot=1651074048, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=160.188720703125, step_time=0.035506, total_metaopt_memory=7714\n",
      "INFO:absl:[140] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=62.200000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=140, grad_sq_norm=8.335656, loss=0.522688, mem.available=12584550400, mem.percent_used=51.200000, mem.read_bytes_since_boot=252404617216, mem.total=25769803776, mem.used=9657319424, mem.write_bytes_since_boot=289327599616, model_state_memory=16, net.bytes_recv_since_boot=3723256832, net.bytes_sent_since_boot=1651086336, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=163.34170532226562, step_time=0.035787, total_metaopt_memory=7714\n",
      "INFO:absl:[160] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=58.500000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=160, grad_sq_norm=4.610901, loss=0.487970, mem.available=12584550400, mem.percent_used=51.200000, mem.read_bytes_since_boot=252404629504, mem.total=25769803776, mem.used=9657319424, mem.write_bytes_since_boot=289327603712, model_state_memory=16, net.bytes_recv_since_boot=3723272192, net.bytes_sent_since_boot=1651101696, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=166.1789093017578, step_time=0.035715, total_metaopt_memory=7714\n",
      "INFO:absl:[180] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=60.400000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=180, grad_sq_norm=6.751245, loss=0.519986, mem.available=12583763968, mem.percent_used=51.200000, mem.read_bytes_since_boot=252404629504, mem.total=25769803776, mem.used=9660317696, mem.write_bytes_since_boot=289327607808, model_state_memory=16, net.bytes_recv_since_boot=3723284480, net.bytes_sent_since_boot=1651113984, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=169.0510711669922, step_time=0.035719, total_metaopt_memory=7714\n",
      "INFO:absl:[199] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=57.300000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=199, grad_sq_norm=6.720049, loss=0.483078, mem.available=12570738688, mem.percent_used=51.200000, mem.read_bytes_since_boot=252404629504, mem.total=25769803776, mem.used=9671786496, mem.write_bytes_since_boot=289327611904, model_state_memory=16, net.bytes_recv_since_boot=3723296768, net.bytes_sent_since_boot=1651126272, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=171.73854064941406, step_time=0.035910, total_metaopt_memory=7714\n",
      "INFO:absl:[200] M_0=-0.100098, M_1=0, M_10=0, M_11=0, M_12=0, M_13=0, M_14=0, M_15=0, M_16=0, M_17=0, M_18=0, M_19=0, M_2=0, M_20=0, M_21=0, M_22=0, M_23=0, M_24=0, M_25=0, M_26=0, M_27=0, M_28=0, M_29=0, M_3=0, M_30=0, M_31=0, M_32=0, M_33=0, M_34=0, M_35=0, M_36=0, M_37=0, M_38=0, M_39=0, M_4=0, M_40=0, M_41=0, M_42=0, M_43=0, M_44=0, M_45=0, M_46=0, M_47=0, M_48=0, M_49=0, M_5=0, M_50=0, M_51=0, M_52=0, M_53=0, M_54=0, M_55=0, M_56=0, M_57=0, M_58=0, M_59=0, M_6=0, M_60=0, M_61=0, M_62=0, M_63=0, M_7=0, M_8=0, M_9=0, cost_fn_history_memory=424, cpu.freq.current=3504, cpu.util.avg_percent_since_last=28.600000, disturbance_history_memory=640, disturbance_transform_state_memory=40, global_step=200, grad_sq_norm=7.183496, loss=0.441752, mem.available=12579094528, mem.percent_used=51.200000, mem.read_bytes_since_boot=252404629504, mem.total=25769803776, mem.used=9663971328, mem.write_bytes_since_boot=289327620096, model_state_memory=16, net.bytes_recv_since_boot=3723303936, net.bytes_sent_since_boot=1651133440, opt_state_memory=7810, param_history_memory=640, param_memory=160, param_sq_norm=171.87484741210938, step_time=0.022379, total_metaopt_memory=7714\n",
      "INFO:absl:Profiler Report\n",
      ":\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Action                                                                                  \t|  Mean Duration (s)\t|  Std Duration (s)\t|  Num Calls      \t|  Total Time (s) \t|  Percentage %   \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Total                                                                                   \t|  -----          \t|  -----          \t|  215            \t|  5.6246         \t|  100 %          \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Train                                                                                   \t|  5.6211         \t|  0.0            \t|  1              \t|  5.6211         \t|  99.938         \t|\n",
      "|  Update parameters                                                                       \t|  0.020666       \t|  0.024839       \t|  200            \t|  4.1333         \t|  73.486         \t|\n",
      "|  Collecting step metrics                                                                 \t|  0.092434       \t|  0.0064704      \t|  12             \t|  1.1092         \t|  19.721         \t|\n",
      "|  Initializing dataset for workload: \u001b[96m\u001b[1mmnist\u001b[0m.                                  \t|  0.049711       \t|  0.0            \t|  1              \t|  0.049711       \t|  0.88383        \t|\n",
      "|  Initializing the model (for \u001b[96m\u001b[1mmnist\u001b[0m) and also optimizer: \u001b[92m\u001b[1mMetaOpt\u001b[0m\t|  0.025988       \t|  0.0            \t|  1              \t|  0.025988       \t|  0.46205        \t|\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 84508), started 0:02:32 ago. (Use '!kill 84508' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b68daee0eefef659\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b68daee0eefef659\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if os.path.isdir('/tmp/trace'):\n",
    "    import shutil\n",
    "    shutil.rmtree(\"/tmp/trace\")\n",
    "\n",
    "import sys\n",
    "from absl import logging, flags\n",
    "from meta_opt.algoperf.runner import run\n",
    "from meta_opt.experiment import ExperimentConfig\n",
    "from meta_opt.utils import make_mesh, GLOBAL_MESH\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "flags.FLAGS(['runner.py', '--config_path=hi'])\n",
    "\n",
    "experiment_cfg = ExperimentConfig(\n",
    "    experiment_name='test',\n",
    "    \n",
    "    # workload details\n",
    "    seed=0,\n",
    "    workload_name='mnist', \n",
    "    full_batch=False,  # whether to do full gradient descent on one batch (fixed during the whole training) or regular minibatch SGD\n",
    "    num_episodes=1,\n",
    "    num_iters=200,  # if None, uses default for the workload\n",
    "\n",
    "    framework='jax',\n",
    "    num_batch_devices=1,\n",
    "    num_opt_devices=8,\n",
    "\n",
    "    # how often to do things\n",
    "    eval_every=-1,\n",
    "    checkpoint_every=-1,\n",
    "    log_every=20,\n",
    "\n",
    "    # other details\n",
    "    use_wandb=False,\n",
    "    print_with_colors=True)\n",
    "\n",
    "# optimizer_cfg = AdamWConfig(learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8, weight_decay=None, grad_clip=None)\n",
    "optimizer_cfg = MetaOptConfig(initial_learning_rate=0.1, weight_decay=1e-4, grad_clip=None,\n",
    "                            H=64, HH=2, m_method='scalar', scale_by_adam_betas=None,\n",
    "                            use_bfloat16=True,\n",
    "                            fake_the_dynamics=False, freeze_gpc_params=True, freeze_cost_fn_during_rollouts=False,\n",
    "                            meta_optimizer_cfg=SGDConfig(learning_rate=1e-5, momentum=0, nesterov=False, weight_decay=None, grad_clip=None))\n",
    "\n",
    "with jax.profiler.trace('/tmp/trace'):\n",
    "    make_mesh(experiment_cfg.num_batch_devices, experiment_cfg.num_opt_devices)\n",
    "    run(experiment_cfg, optimizer_cfg)\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=/tmp/trace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0525147-b487-4d39-b104-e39a6759ebe7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# random tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee97ebda-b549-4279-9699-83a7acef1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chex\n",
    "# import numpy as np\n",
    "\n",
    "# def compute(gpc_params: chex.Array, \n",
    "#                         disturbance_history: chex.Array) -> chex.Array:\n",
    "#     ret = jnp.dot(disturbance_history.T, gpc_params)\n",
    "#     return ret\n",
    "\n",
    "# @jax.custom_jvp\n",
    "# def compute2(gpc_params: chex.Array, \n",
    "#                 disturbance_history: chex.Array) -> chex.Array:\n",
    "#     ret = disturbance_history.T @ gpc_params\n",
    "#     return ret\n",
    "\n",
    "# @compute2.defjvp\n",
    "# def compute2_jvp(primals, tangents):\n",
    "#     g, d = primals\n",
    "#     g_dot, d_dot = tangents\n",
    "#     primal_out = compute2(g, d)\n",
    "#     tangent_out = d.T @ g_dot\n",
    "#     return primal_out, tangent_out\n",
    "\n",
    "\n",
    "# # @custom_jvp\n",
    "# # def f(x, y):\n",
    "# #   return x ** 2 * y\n",
    "\n",
    "# # @f.defjvp\n",
    "# # def f_jvp(primals, tangents):\n",
    "# #   x, y = primals\n",
    "# #   x_dot, y_dot = tangents\n",
    "# #   primal_out = f(x, y)\n",
    "# #   tangent_out = 2 * x * y * x_dot + x ** 2 * y_dot\n",
    "# #   return primal_out, tangent_out\n",
    "\n",
    "# def loss(p, d):\n",
    "#     return jnp.linalg.norm(compute(p, d) + jax.random.normal(jax.random.PRNGKey(0), d.shape[1:]))\n",
    "# def loss2(p, d):\n",
    "#     return jnp.linalg.norm(compute2(p, d) + jax.random.normal(jax.random.PRNGKey(0), d.shape[1:]))\n",
    "\n",
    "# gpc_params = jnp.array(np.random.randn(32,))\n",
    "# disturbance_history = jnp.array(np.random.randn(32, 2048))\n",
    "\n",
    "# print('losses')\n",
    "# print(loss(gpc_params, disturbance_history))\n",
    "# print(loss2(gpc_params, disturbance_history))\n",
    "\n",
    "# print('grads')\n",
    "# print(jax.grad(loss)(gpc_params, disturbance_history))\n",
    "# print(jax.grad(loss2)(gpc_params, disturbance_history))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaopt",
   "language": "python",
   "name": "metaopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
