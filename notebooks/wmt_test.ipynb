{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f937c1bf-e61d-4d67-bf11-1f2c0caa03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 00:57:55.436389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-13 00:57:56.472378: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# handle the system stuff, colab stuff, etc\n",
    "import os\n",
    "DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# make sure we have the necessary folders\n",
    "for subdir in ['data', 'figs', 'datasets']: \n",
    "    temp = os.path.join(DIR, subdir)\n",
    "    if not os.path.isdir(temp): os.mkdir(temp)\n",
    "\n",
    "from meta_opt.train_loops import train_standard_opt, train_hgd, train_meta_opt\n",
    "from meta_opt.utils.experiment_utils import make, save_checkpoint, process_results, bcolors, plot, get_final_cparams\n",
    "import meta_opt.configs as configs\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pkl\n",
    "import optax\n",
    "\n",
    "# ==================================================\n",
    "# configuration and seeds for each trial\n",
    "SEEDS = [0,]\n",
    "\n",
    "NAME = 'wmt_yeet'\n",
    "CFG = {\n",
    "    # training options\n",
    "    'workload': 'WMT',\n",
    "    'num_iters': 5000,\n",
    "    'eval_every': 200,\n",
    "    'num_eval_iters': -1,\n",
    "    'batch_size': 32,\n",
    "    'full_batch': False,\n",
    "    'reset_every': int(1e9),\n",
    "\n",
    "    # experiment options\n",
    "    'experiment_name': NAME,\n",
    "    'load_checkpoint': False,\n",
    "    'overwrite': True,  # whether to allow us to overwrite existing checkpoints or throw errors\n",
    "    'directory': DIR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5515f45-3adc-4851-8616-a6828dcaeeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:21<?, ?it/s]E0313 01:01:32.054113 4162103 pjrt_stream_executor_client.cc:2804] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6590442048 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  797.73MiB\n",
      "              constant allocation:    1.00MiB\n",
      "        maybe_live_out allocation:    1.56GiB\n",
      "     preallocated temp allocation:    6.14GiB\n",
      "  preallocated temp fragmentation:   65.00MiB (1.03%)\n",
      "                 total allocation:    8.47GiB\n",
      "              total fragmentation:  915.75MiB (10.55%)\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 1000.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/transpose(jvp(Transformer))/Transformer.decode/decoder/div\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[32,256,32000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 500.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.decode/decoder/shared_embedding.attend/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f16[8192,32000]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 125.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.decode/decoder/shared_embedding.attend/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tEntry Parameter Subshape: f32[32000,1024]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 125.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/add\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[32000,1024]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 125.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/transpose(jvp(Transformer))/Transformer.decode/decoder/shared_embedding.attend/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: f32[32000,1024]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_5/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: f16[8192,4096]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_5/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f16[32,16,256,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_4/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: f16[8192,4096]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_4/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f16[32,16,256,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_3/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: f16[8192,4096]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 11:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_3/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f16[32,16,256,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 12:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_2/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: f16[8192,4096]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 13:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_2/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f16[32,16,256,256]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 14:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_1/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n",
      "\t\tXLA Label: custom-call\n",
      "\t\tShape: f16[8192,4096]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 15:\n",
      "\t\tSize: 64.00MiB\n",
      "\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_1/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f16[32,16,256,256]\n",
      "\t\t==========================\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6590442048 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  797.73MiB\n              constant allocation:    1.00MiB\n        maybe_live_out allocation:    1.56GiB\n     preallocated temp allocation:    6.14GiB\n  preallocated temp fragmentation:   65.00MiB (1.03%)\n                 total allocation:    8.47GiB\n              total fragmentation:  915.75MiB (10.55%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/transpose(jvp(Transformer))/Transformer.decode/decoder/div\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: fusion\n\t\tShape: f32[32,256,32000]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 500.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.decode/decoder/shared_embedding.attend/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: fusion\n\t\tShape: f16[8192,32000]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 125.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.decode/decoder/shared_embedding.attend/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tEntry Parameter Subshape: f32[32000,1024]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 125.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/add\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: fusion\n\t\tShape: f32[32000,1024]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 125.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/transpose(jvp(Transformer))/Transformer.decode/decoder/shared_embedding.attend/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f32[32000,1024]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_5/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_5/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_4/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_4/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_3/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_3/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_2/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_2/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_1/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_1/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(stats)\n\u001b[1;32m     52\u001b[0m CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m SEEDS[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrain_standard_opt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_hyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mtrain_standard_opt\u001b[0;34m(cfg, optimizer)\u001b[0m\n\u001b[1;32m     30\u001b[0m     tstate \u001b[38;5;241m=\u001b[39m reset_model(reset_rng, tstate)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m reset_rng\n\u001b[0;32m---> 33\u001b[0m tstate, (loss, grads) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# update all the stats\u001b[39;00m\n\u001b[1;32m     36\u001b[0m s \u001b[38;5;241m=\u001b[39m {}\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/meta-opt/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1209\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1211\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 6590442048 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  797.73MiB\n              constant allocation:    1.00MiB\n        maybe_live_out allocation:    1.56GiB\n     preallocated temp allocation:    6.14GiB\n  preallocated temp fragmentation:   65.00MiB (1.03%)\n                 total allocation:    8.47GiB\n              total fragmentation:  915.75MiB (10.55%)\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 1000.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/transpose(jvp(Transformer))/Transformer.decode/decoder/div\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: fusion\n\t\tShape: f32[32,256,32000]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 500.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.decode/decoder/shared_embedding.attend/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: fusion\n\t\tShape: f16[8192,32000]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 125.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.decode/decoder/shared_embedding.attend/transpose[permutation=(1, 0)]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tEntry Parameter Subshape: f32[32000,1024]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 125.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/add\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: fusion\n\t\tShape: f32[32000,1024]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 125.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/transpose(jvp(Transformer))/Transformer.decode/decoder/shared_embedding.attend/dot_general[dimension_numbers=(((0, 1), (0, 1)), ((), ())) precision=None preferred_element_type=float32]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f32[32000,1024]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_5/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_5/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_4/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_4/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_3/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 11:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_3/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 12:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_2/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 13:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_2/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n\tBuffer 14:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_1/MlpBlock_0/Dense_0/dot_general[dimension_numbers=(((2,), (0,)), ((), ())) precision=None preferred_element_type=None]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33\n\t\tXLA Label: custom-call\n\t\tShape: f16[8192,4096]\n\t\t==========================\n\n\tBuffer 15:\n\t\tSize: 64.00MiB\n\t\tOperator: op_name=\"jit(train_step)/jit(main)/jvp(Transformer)/Transformer.encode/encoder/encoderblock_1/MultiHeadDotProductAttention_0/convert_element_type[new_dtype=float16 weak_type=False]\" source_file=\"/tmp/ipykernel_4162103/4186806824.py\" source_line=33 deduplicated_name=\"input_exponential_reduce_fusion\"\n\t\tXLA Label: fusion\n\t\tShape: f16[32,16,256,256]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import jax\n",
    "\n",
    "from meta_opt.nn import reset_model, train_step, eval\n",
    "from meta_opt.workloads import get_workload\n",
    "from meta_opt.utils.pytree_utils import pytree_sq_norm\n",
    "from meta_opt.utils.experiment_utils import get_opt_hyperparams\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# ------------------------------ Standard Optax Optimizers ----------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def train_standard_opt(cfg, optimizer):\n",
    "    tstate, train_ds, test_ds, rng, args = get_workload(cfg, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = get_opt_hyperparams(tstate.opt_state)\n",
    "    args['optimizer_name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    last_eval_step = None\n",
    "    pbar = tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])\n",
    "    for t, batch in enumerate(pbar):\n",
    "\n",
    "        if t % args['reset_every'] == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = train_step(tstate, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % args['eval_every'] == 0 and t != 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = pytree_sq_norm(tstate.params)\n",
    "            s['grad_sq_norm'] = pytree_sq_norm(grads)\n",
    "            last_eval_step = t\n",
    "\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), \n",
    "                          'eval_loss': round(stats[last_eval_step]['eval_loss'].item(), 3) if last_eval_step is not None else 'N/A',\n",
    "                          })\n",
    "\n",
    "    return dict(stats)\n",
    "\n",
    "CFG['seed'] = SEEDS[0]\n",
    "train_standard_opt(CFG, optax.inject_hyperparams(optax.sgd)(0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
