{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f937c1bf-e61d-4d67-bf11-1f2c0caa03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle the system stuff, colab stuff, etc\n",
    "import os\n",
    "DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# make sure we have the necessary folders\n",
    "for subdir in ['data', 'figs', 'datasets']: \n",
    "    temp = os.path.join(DIR, subdir)\n",
    "    if not os.path.isdir(temp): os.mkdir(temp)\n",
    "\n",
    "from meta_opt.train_loops import train_standard_opt, train_hgd, train_meta_opt\n",
    "from meta_opt.utils.experiment_utils import make, save_checkpoint, process_results, bcolors, plot, get_final_cparams\n",
    "import meta_opt.configs as configs\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pkl\n",
    "import optax\n",
    "\n",
    "# ==================================================\n",
    "# configuration and seeds for each trial\n",
    "SEEDS = [0,]\n",
    "\n",
    "NAME = 'wmt_yeet'\n",
    "CFG = {\n",
    "    # training options\n",
    "    'workload': 'WMT',\n",
    "    'num_iters': 5000,\n",
    "    'eval_every': 200,\n",
    "    'num_eval_iters': -1,\n",
    "    'batch_size': 32,\n",
    "    'full_batch': False,\n",
    "    'reset_every': int(1e9),\n",
    "\n",
    "    # experiment options\n",
    "    'experiment_name': NAME,\n",
    "    'load_checkpoint': False,\n",
    "    'overwrite': True,  # whether to allow us to overwrite existing checkpoints or throw errors\n",
    "    'directory': DIR,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5515f45-3adc-4851-8616-a6828dcaeeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                       | 0/5000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "from collections import defaultdict\n",
    "import tqdm\n",
    "import jax\n",
    "\n",
    "from meta_opt.nn import reset_model, train_step, eval\n",
    "from meta_opt.workloads import get_workload\n",
    "from meta_opt.utils.pytree_utils import pytree_sq_norm\n",
    "from meta_opt.utils.experiment_utils import get_opt_hyperparams\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# ------------------------------ Standard Optax Optimizers ----------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "def train_standard_opt(cfg, optimizer):\n",
    "    tstate, train_ds, test_ds, rng, args = get_workload(cfg, optimizer)\n",
    "\n",
    "    stats = defaultdict(dict)\n",
    "    args['optimizer_args'] = get_opt_hyperparams(tstate.opt_state)\n",
    "    args['optimizer_name'] = 'standard'\n",
    "    stats['args'] = args\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    last_eval_step = None\n",
    "    pbar = tqdm.tqdm(train_ds.as_numpy_iterator(), total=args['num_iters'])\n",
    "    for t, batch in enumerate(pbar):\n",
    "\n",
    "        if t % args['reset_every'] == 0:\n",
    "            reset_rng, rng = jax.random.split(rng)\n",
    "            tstate = reset_model(reset_rng, tstate)\n",
    "            del reset_rng\n",
    "\n",
    "        tstate, (loss, grads) = train_step(tstate, batch)\n",
    "\n",
    "        # update all the stats\n",
    "        s = {}\n",
    "        s['timestamp'] = perf_counter() - t0\n",
    "        s['loss'] = loss\n",
    "        if t % args['eval_every'] == 0 and t != 0:\n",
    "            for k, v in eval(tstate, test_ds.as_numpy_iterator()).items(): s[f'eval_{k}'] = v\n",
    "            s['param_sq_norm'] = pytree_sq_norm(tstate.params)\n",
    "            s['grad_sq_norm'] = pytree_sq_norm(grads)\n",
    "            last_eval_step = t\n",
    "\n",
    "        stats[t] = s\n",
    "        pbar.set_postfix({'loss': round(s['loss'].item(), 3), \n",
    "                          'eval_loss': round(stats[last_eval_step]['eval_loss'].item(), 3) if last_eval_step is not None else 'N/A',\n",
    "                          })\n",
    "\n",
    "    return dict(stats)\n",
    "\n",
    "CFG['seed'] = SEEDS[0]\n",
    "train_standard_opt(CFG, optax.inject_hyperparams(optax.sgd)(0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
