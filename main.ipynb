{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "760f4e7d-a02f-42f3-b87d-231d03392428",
    "outputId": "5af8d008-1034-4d1d-bc78-58a0d6ffc6e6"
   },
   "outputs": [],
   "source": [
    "# handle the system stuff, colab stuff, etc\n",
    "import os\n",
    "try:\n",
    "    from google import colab  # for use in google colab!!\n",
    "    !git clone https://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@github.com/edogariu/meta-opt\n",
    "    !pip install -q ./meta-opt\n",
    "    !pip install -q dill\n",
    "    # !pip install -q jax[cuda12_pip]==0.4.20 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html  # for disabling prealloc, see https://github.com/google/jax/discussions/19014\n",
    "    # !pip install -q tensorflow-text ml_collections clu sentencepiece  # for WMT\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DIR = os.path.abspath(\"./drive/My Drive/meta-opt\")\n",
    "except: \n",
    "    DIR = os.path.abspath(\".\")\n",
    "assert os.path.isdir(DIR)\n",
    "\n",
    "# make sure we have the necessary folders\n",
    "for subdir in ['data', 'figs', 'datasets']: \n",
    "    temp = os.path.join(DIR, subdir)\n",
    "    if not os.path.isdir(temp): os.mkdir(temp)\n",
    "\n",
    "# # for the one-time colab setup\n",
    "# !git clone https://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@github.com/edogariu/meta-opt\n",
    "# !cp -r \"meta-opt\" \"drive/My Drive/\"\n",
    "# !pip install kora -q  # library from https://stackoverflow.com/questions/62596466/how-can-i-run-notebooks-of-a-github-project-in-google-colab to help get ID\n",
    "# from kora.xattr import get_id\n",
    "# fid = get_id(f\"{dir_prefix}meta_opt.ipynb\")\n",
    "# print(\"https://colab.research.google.com/drive/\"+fid)\n",
    "\n",
    "from meta_opt.train_loops import train_standard_opt, train_hgd, train_meta_opt\n",
    "from meta_opt.utils.experiment_utils import make, save_checkpoint, process_results, bcolors, plot\n",
    "import meta_opt.configs as configs\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pkl\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125998da-b7ed-4d07-af09-d2020a813e0a",
   "metadata": {
    "id": "125998da-b7ed-4d07-af09-d2020a813e0a"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8211e988-2b8f-43a1-9ba5-23045d3057d3",
    "outputId": "ef881563-f7b8-483c-b35a-523921e95e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using \u001b[93m\u001b[1mcpu\u001b[0m for jax\n",
      "results will be stored at: \u001b[96m\u001b[1m/Users/evandigiorno/Desktop/meta-opt/data/mnist_fullbatch_*.pkl\u001b[0m\n",
      "we will \u001b[91m\u001b[1mNOT\u001b[0m try to load experiment checkpoint first\n",
      "\u001b[91m\u001b[1mWARNING: there already exists a checkpoint with this name! make sure you want to overwrite\u001b[0m\n",
      "starting the experiment from scratch :)\n",
      "\u001b[91m\u001b[1mnote: using full_batch means we will never eval\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# configuration and seeds for each trial\n",
    "SEEDS = [0, 1, 2, 3,]  # the length of this list is the number of trials we will run :)\n",
    "# CFG = {\n",
    "#     # training options\n",
    "#     'workload': 'NONCONVEX_QUADRATIC',\n",
    "#     'num_iters': 400,\n",
    "#     'eval_every': -1,\n",
    "#     'num_eval_iters': -1,\n",
    "#     'batch_size': -1,\n",
    "#     'full_batch': True,\n",
    "#     'reset_every': 900,\n",
    "\n",
    "#     # experiment options\n",
    "#     'experiment_name': 'yeet',\n",
    "#     'load_checkpoint': False,\n",
    "#     'overwrite': True,  # whether to allow us to overwrite existing checkpoints or throw errors\n",
    "#     'directory': DIR,\n",
    "# }\n",
    "CFG = configs.MNIST_FULLBATCH\n",
    "\n",
    "results = make(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fa482-aba2-4e9e-9e3b-caf40c5bceb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/12000 [00:00<?, ?it/s]2024-03-05 09:50:21.091749: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      " 78%|██████████████████████████████████████████████             | 9372/12000 [03:12<01:05, 40.00it/s, loss=0.038, eval_loss=N/A]"
     ]
    }
   ],
   "source": [
    "# uncomment the ones to run, with correctly chosen hyperparameters\n",
    "for s in SEEDS:\n",
    "    CFG['seed'] = s\n",
    "    \n",
    "    # ours\n",
    "    opt = optax.inject_hyperparams(optax.adam)(learning_rate=4e-4, b1=0.9, b2=0.999)\n",
    "    results['cf'].append(train_meta_opt(CFG, counterfactual=True, H=32, HH=2, meta_optimizer=opt))\n",
    "    results['ncf'].append(train_meta_opt(CFG, counterfactual=False, H=32, HH=2, meta_optimizer=opt))\n",
    "\n",
    "    no_adam = optax.inject_hyperparams(optax.sgd)(learning_rate=2e-4)\n",
    "    results['cf_noadam'].append(train_meta_opt(CFG, counterfactual=True, H=32, HH=2, meta_optimizer=no_adam))\n",
    "    results['ncf_noadam'].append(train_meta_opt(CFG, counterfactual=True, H=32, HH=2, meta_optimizer=no_adam))\n",
    "\n",
    "    # standard benchmarks\n",
    "    benchmarks = {\n",
    "        'sgd': optax.inject_hyperparams(optax.sgd)(learning_rate=0.4),\n",
    "        'momentum': optax.chain(optax.add_decayed_weights(1e-4), optax.inject_hyperparams(optax.sgd)(learning_rate=0.1, momentum=0.9)),\n",
    "        'adamw': optax.inject_hyperparams(optax.adamw)(learning_rate=1e-3, b1=0.9, b2=0.999, weight_decay=1e-4),\n",
    "        # 'rmsprop': optax.inject_hyperparams(optax.rmsprop)(learning_rate=1e-3),\n",
    "    }\n",
    "    for k, opt in benchmarks.items(): results[k].append(train_standard_opt(CFG, opt))\n",
    "\n",
    "    # other\n",
    "    results['hgd'].append(train_hgd(CFG, initial_lr=0.4, hypergrad_lr=1e-3))\n",
    "\n",
    "    save_checkpoint(CFG, results, checkpoint_name=f'seed {s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf",
   "metadata": {
    "id": "c3f13ef9-3e01-4171-b516-a50e90c4c8cf"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a523230-566c-4007-b3fa-6f255de46d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_results = process_results(CFG, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkj0hJUA5gkp",
   "metadata": {
    "id": "kkj0hJUA5gkp"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# plot a particular set of experiments\n",
    "# ----------------------------------------\n",
    "keys_to_plot = [\n",
    "    'metaopt_cf_long',\n",
    "    'metaopt_ncf',\n",
    "    'metaopt_cf_long_noadam',\n",
    "    'metaopt_ncf_noadam',\n",
    "    'sgd',\n",
    "    'momentum',\n",
    "    'adamw',\n",
    "    'hgd',\n",
    "    # 'adam_0.001',\n",
    "    # 'rmsprop_0.001',\n",
    "    ]\n",
    "\n",
    "# ----------------------------------------\n",
    "# OR just plot em all\n",
    "# ----------------------------------------\n",
    "# keys_to_plot = ''  # specific regex\n",
    "keys_to_plot = '.*'  # anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "93d6db68-9bdc-41cc-95eb-ebdfe55807fd",
    "outputId": "c8f7de49-af69-4b34-b75c-b02b3b260c08"
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(len(processed_results), 1, figsize=(10, 32))\n",
    "Ms = {}\n",
    "\n",
    "for i, stat_key in enumerate(processed_results.keys()):\n",
    "    ax[i].set_title(stat_key)\n",
    "    for experiment_name in processed_results[stat_key].keys():\n",
    "        if (isinstance(keys_to_plot, list) and experiment_name not in keys_to_plot) or (isinstance(keys_to_plot, str) and not re.match(keys_to_plot, experiment_name)): \n",
    "            # print(f'skipped {experiment_name}')\n",
    "            continue\n",
    "        ts, avgs, stds = processed_results[stat_key][experiment_name]['t'], processed_results[stat_key][experiment_name]['avg'], processed_results[stat_key][experiment_name]['std']\n",
    "        if avgs.ndim == 2:  # how to handle stats that are vectors (such as the Ms for scalar meta-opt)\n",
    "            Ms[experiment_name] = (ts, avgs)\n",
    "            ax[i].plot(ts, avgs.sum(axis=-1), label=experiment_name)\n",
    "            stds = ((stds ** 2).sum(axis=-1)) ** 0.5\n",
    "            ax[i].fill_between(ts, avgs.sum(axis=-1) - 1.96 * stds, avgs.sum(axis=-1) + 1.96 * stds, alpha=0.2)\n",
    "            # for j in range(avgs.shape[1]):\n",
    "            #     ax[i].plot(ts, avgs[:, j], label=f'{experiment_name} {str(j)}')\n",
    "            #     ax[i].fill_between(ts, avgs[:, j] - 1.96 * stds[:, j], avgs[:, j] + 1.96 * stds[:, j], alpha=0.2)\n",
    "        else:\n",
    "            # if stat_key in ['loss', 'grad_sq_norm', 'eval_acc', 'eval_loss']:\n",
    "            #     n = 20\n",
    "            #     kernel = np.array([1 / n,] * n)\n",
    "            #     avgs = np.convolve(avgs, kernel)[n // 2:n // 2 + avgs.shape[0]]\n",
    "            #     stds = np.convolve(stds ** 2, kernel ** 2)[n // 2:n // 2 + stds.shape[0]] ** 0.5\n",
    "            ax[i].plot(ts, avgs, label=experiment_name)\n",
    "            ax[i].fill_between(ts, avgs - 1.96 * stds, avgs + 1.96 * stds, alpha=0.2)\n",
    "    ax[i].legend()\n",
    "\n",
    "\n",
    "# ax[1].set_ylim(0.0, 0.03)\n",
    "# ax[2].set_ylim(0.96, 0.98)\n",
    "# ax[3].set_ylim(0.0, 0.3)\n",
    "# ax[5].set_ylim(-0.1, 1)\n",
    "# ax[5].set_ylim(-0.05, 0.05)\n",
    "# name = CFG['experiment_name']; plt.savefig(f'{DIR}/figs/{name}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LTMpnF5ybSr2",
   "metadata": {
    "id": "LTMpnF5ybSr2"
   },
   "source": [
    "## Animate\n",
    "Animate the values taken by the $\\{M_h\\}_{h=1}^H$ coefficients during training. Each $M_h$ multiplies a disturbance from $h$ training steps ago (i.e. 0 is most recent in this plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DA5o-cJ1MpL5",
   "metadata": {
    "id": "DA5o-cJ1MpL5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from copy import deepcopy\n",
    "\n",
    "downsample_factor = 200  # how many timesteps to move forward every animation step\n",
    "ymin, ymax = -0.4, 0.1\n",
    "\n",
    "anim_data = []  # each entry is a dictionary containing the M values for that animation step\n",
    "_Ms = {k: (np.array(v[0]), v[1]) for k, v in Ms.items()}\n",
    "H_max = max([v[1].shape[1] for v in _Ms.values()])\n",
    "T = CFG['num_iters']\n",
    "name = CFG['workload']\n",
    "for t in range(0, T, downsample_factor):\n",
    "    temp = {}\n",
    "    for k, (ts, vals) in _Ms.items(): temp[k] = vals[max(0, np.argmax(ts > t) - 1)]\n",
    "    anim_data.append(temp)\n",
    "\n",
    "fig = plt.figure()  # initializing a figure in which the graph will be plotted\n",
    "ax = plt.axes(xlim =(0, H_max), ylim=(ymin, ymax))  # marking the x-axis and y-axis\n",
    "ax.set_xlabel('number of steps in the past')\n",
    "ax.set_ylabel('M coefficient')\n",
    "\n",
    "# initializing a line variable\n",
    "ls = {}\n",
    "for k in _Ms.keys():\n",
    "    ls[k], = ax.plot([], [], lw = 3, label=k)\n",
    "legend = ax.legend()\n",
    "\n",
    "# data which the line will contain (x, y)\n",
    "def init():\n",
    "    for l in ls.values(): l.set_data([], [])\n",
    "    return list(ls.values())\n",
    "\n",
    "def animate(i):\n",
    "    for k, M in anim_data[i].items():\n",
    "        x, y = range(0, len(M)), M\n",
    "        ls[k].set_data(x, y[::-1])\n",
    "        # line.set_label(i)\n",
    "    # legend.get_texts()[0].set_text(i * downsample_factor) #Update label each at frame\n",
    "    ax.set_title(f'timestep #{i * downsample_factor} of meta-opt on {name}')\n",
    "    return list(ls.values())\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func = init,\n",
    "                     frames = T // downsample_factor, interval = downsample_factor, blit = True)\n",
    "plt.close()\n",
    "h = HTML(anim.to_html5_video())\n",
    "display(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e31e50-2a60-46c3-bff9-05d6584136c8",
   "metadata": {
    "id": "nT5oGkiQ6qAO"
   },
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b63910b-6f36-4cfe-a509-017152701d72",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "meta-opt",
   "language": "python",
   "name": "meta-opt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
