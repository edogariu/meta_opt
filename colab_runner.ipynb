{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d027f2-f9d6-4c7a-84d5-1b1222ffd966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging, flags\n",
    "\n",
    "from configs.experiment import ExperimentConfig\n",
    "from configs.optimizers import OptimizerConfig, SGDConfig, AdamWConfig, MetaOptConfig\n",
    "from runner import run\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d808976-76c6-4341-82ca-0e8a7b1679b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_cfg = ExperimentConfig(\n",
    "    \n",
    "    # name of the experiment\n",
    "    experiment_name='test',\n",
    "    \n",
    "    # workload details\n",
    "    workload_name='mnist', \n",
    "    full_batch=True,  # whether to do full gradient descent on one batch (fixed during the whole training) or regular minibatch SGD\n",
    "    num_episodes=3,\n",
    "\n",
    "    framework='jax',\n",
    "    num_iters=100,  # if None, uses default for the workload\n",
    "    batch_size=None,  # if None, uses default for the workload\n",
    "\n",
    "    # how often to do things\n",
    "    eval_every=-1,\n",
    "    checkpoint_every=-1,\n",
    "    print_every=-1,\n",
    "    log_every=20,\n",
    "\n",
    "    # other details\n",
    "    resume_last_run=False,\n",
    "    overwrite=True)\n",
    "\n",
    "# optimizer_cfg = SGDConfig(learning_rate=0.01, momentum=0.9, nesterov=False, weight_decay=None, grad_clip=None)\n",
    "# optimizer_cfg = AdamWConfig(learning_rate=0.001, b1=0.9, b2=0.999, eps=1e-8, weight_decay=None, grad_clip=None)\n",
    "\n",
    "# meta_optimizer_cfg = SGDConfig(learning_rate=1e-5, momentum=0, nesterov=False, weight_decay=None, grad_clip=None)\n",
    "meta_optimizer_cfg = AdamWConfig(learning_rate=1e-3, b1=0.9, b2=0.999, eps=1e-8, weight_decay=0, grad_clip=None)\n",
    "optimizer_cfg = MetaOptConfig(initial_learning_rate=0.1, weight_decay=1e-4, grad_clip=None,\n",
    "                            H=16, HH=2, m_method='scalar', scale_by_adam_betas=None, fake_the_dynamics=False, freeze_meta_params=False,\n",
    "                            meta_optimizer_cfg=meta_optimizer_cfg, meta_grad_clip=10.0)\n",
    "\n",
    "# meta_optimizer_cfg = SGDConfig(learning_rate=1e-5, momentum=0, nesterov=False, weight_decay=None, grad_clip=None)\n",
    "# optimizer_cfg = MetaOptConfig(initial_learning_rate=0.001, weight_decay=1e-4, grad_clip=None,\n",
    "#                               H=16, HH=2, m_method='diagonal', scale_by_adam_betas=(0.9, 0.999),\n",
    "#                               meta_optimizer_cfg=meta_optimizer_cfg, meta_grad_clip=10.0)\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9abf7-e7a1-4b42-b128-ba4613c6d21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Creating directory at /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax for experiments to be saved to.\n",
      "INFO:absl:\u001b[96m\u001b[1mno `batch_size` provided. using default of 512 for the workload mnist!\u001b[0m\n",
      "INFO:absl:Using \u001b[93m\u001b[1mcpu\u001b[0m for jax\n",
      "INFO:absl: \u001b[93m\u001b[1m8 devices\u001b[0m\n",
      "INFO:absl:Initializing dataset for workload: \u001b[96m\u001b[1mmnist\u001b[0m.\n",
      "INFO:absl:Load dataset info from /Users/evandogariu/Desktop/meta-opt/datasets/mnist/3.0.1\n",
      "INFO:absl:Fields info.[citation, splits, supervised_keys, module_name] from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/Users/evandogariu/Desktop/meta-opt/datasets/mnist/3.0.1)\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split train[:50000], from /Users/evandogariu/Desktop/meta-opt/datasets/mnist/3.0.1\n",
      "INFO:absl:Initializing the model (for \u001b[96m\u001b[1mmnist\u001b[0m) and also optimizer: \u001b[92m\u001b[1mMetaOpt\u001b[0m\n",
      "INFO:absl:Model has \u001b[1m814160\u001b[0m parameters and the optimizer state takes \u001b[1m-1.00MB\u001b[0m\n",
      "INFO:absl:Initializing checkpoint and logger.\n",
      "INFO:absl:Found no checkpoint files in /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax with prefix checkpoint_\n",
      "INFO:absl:Saving meta data to /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax/meta_data_0.json.\n",
      "INFO:absl:Saving config to /Users/evandogariu/Desktop/meta-opt/experiments/test/mnist_jax/config_0.json.\n",
      "WARNING:absl:\u001b[93m\u001b[1mwont load checkpoints for episodic learning, its weird\u001b[0m\n",
      "INFO:absl:\u001b[91m\u001b[1mStarting training loop.\u001b[0m\n",
      "INFO:absl:\u001b[94m\u001b[1mResetting model!\u001b[0m\n",
      "INFO:absl:\u001b[94m\u001b[1mAlso resetting optimizer state!\u001b[0m\n",
      "INFO:absl:\u001b[91m\u001b[1mStarting training episode 1.\u001b[0m\n",
      "INFO:absl:[1] M_0=-0.10000000149011612, M_1=0.0, M_10=0.0, M_11=0.0, M_12=0.0, M_13=0.0, M_14=0.0, M_15=0.0, M_2=0.0, M_3=0.0, M_4=0.0, M_5=0.0, M_6=0.0, M_7=0.0, M_8=0.0, M_9=0.0, cpu.freq.current=3504, cpu.util.avg_percent_since_last=27.800000, global_step=1, grad_sq_norm=44.306305, loss=2.645919, mem.available=11230625792, mem.percent_used=56.400000, mem.read_bytes_since_boot=212856365056, mem.total=25769803776, mem.used=12811190272, mem.write_bytes_since_boot=136837369856, net.bytes_recv_since_boot=3524817920, net.bytes_sent_since_boot=2188017664, opt_state_memory=-1048576, param_sq_norm=1100.71533203125, step_time=0.411275\n",
      "INFO:absl:[20] M_0=-0.09958425164222717, M_1=-0.0002537346153985709, M_10=-0.00030444623553194106, M_11=6.790237966924906e-05, M_12=3.2515134080313146e-05, M_13=0.0003879543801303953, M_14=0.00036841820110566914, M_15=-6.112968549132347e-07, M_2=-0.00023684058396611363, M_3=-0.0005001655081287026, M_4=-0.0008632856770418584, M_5=0.0004365220374893397, M_6=-0.000260093278484419, M_7=-0.0007022185600362718, M_8=0.0006453483365476131, M_9=0.0006633474258705974, cpu.freq.current=3504, cpu.util.avg_percent_since_last=23.100000, global_step=20, grad_sq_norm=8.701473, loss=1.333040, mem.available=11140366336, mem.percent_used=56.800000, mem.read_bytes_since_boot=212856397824, mem.total=25769803776, mem.used=12904071168, mem.write_bytes_since_boot=136837373952, net.bytes_recv_since_boot=3524833280, net.bytes_sent_since_boot=2188062720, opt_state_memory=-1048576, param_sq_norm=1130.4195556640625, step_time=0.610432\n",
      "INFO:absl:[40] M_0=-0.09900190681219101, M_1=0.00018006475875154138, M_10=-0.0013213009806349874, M_11=0.0007784810150042176, M_12=-0.0018362002447247505, M_13=0.0015077583957463503, M_14=0.0018773404881358147, M_15=-8.929998148232698e-05, M_2=-0.0010583086404949427, M_3=-0.0021947524510324, M_4=-0.00249156984500587, M_5=0.001078725908882916, M_6=0.0009108299855142832, M_7=-0.0018465934554114938, M_8=0.0003091132966801524, M_9=0.001301610842347145, cpu.freq.current=3504, cpu.util.avg_percent_since_last=22.400000, global_step=40, grad_sq_norm=5.011710, loss=0.909572, mem.available=17836032000, mem.percent_used=30.800000, mem.read_bytes_since_boot=217460764672, mem.total=25769803776, mem.used=4916953088, mem.write_bytes_since_boot=142381342720, net.bytes_recv_since_boot=3526128640, net.bytes_sent_since_boot=2188419072, opt_state_memory=-1048576, param_sq_norm=1165.4248046875, step_time=0.624883\n",
      "INFO:absl:[60] M_0=-0.09921833127737045, M_1=-0.0009871916845440865, M_10=-0.003366285702213645, M_11=0.0017314571887254715, M_12=-0.0030252868309617043, M_13=0.00044156546937301755, M_14=0.0016981536755338311, M_15=-0.000540080654900521, M_2=-0.0030486686155200005, M_3=-0.0023119505494832993, M_4=-0.0030248400289565325, M_5=0.0020054499618709087, M_6=0.0004964940599165857, M_7=-0.001666432712227106, M_8=-0.0015378394164144993, M_9=0.0009890035726130009, cpu.freq.current=3504, cpu.util.avg_percent_since_last=32.100000, global_step=60, grad_sq_norm=3.113812, loss=0.695798, mem.available=17260167168, mem.percent_used=33.000000, mem.read_bytes_since_boot=217698455552, mem.total=25769803776, mem.used=5705187328, mem.write_bytes_since_boot=142432514048, net.bytes_recv_since_boot=3542119424, net.bytes_sent_since_boot=2189128704, opt_state_memory=-1048576, param_sq_norm=1201.2425537109375, step_time=0.615362\n"
     ]
    }
   ],
   "source": [
    "flags.FLAGS(['runner.py', f'--seed={seed}', '--config_path=\\\"\\\"'])\n",
    "run(seed, experiment_cfg, optimizer_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaopt",
   "language": "python",
   "name": "metaopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
